{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8rUN7eCZBkXU","sX-obQ9KBkXZ","4FhHiV9ewezJ","X7Li0568BkXc","7teb07y3BkXj","znolnMDHBkXk","fLQIEANsBkXl","Ik4aBvEeBkXp","iZetRXUWBkXq","GyPLNCoTBkXq","faVNT2NCBkXu","xVsiYu9eBkXz","lxKAjCS6BkX0","_av7NrelBkX0"],"machine_shape":"hm","authorship_tag":"ABX9TyNyfMDJwSi90uHYIbv7RiAz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Optimized code for translation (*test_code_Paraphrase Datasets Translations - GaMS.ipynb*) with OpenAI model. This script uses cache patterns and batch processing inside of pipeline programming, enabling parallel processing of translation batches. As a result, the API calls are optimized, and if an error occurs during the translation of a single sentence, the entire batch is not lost. We are using ChatGPT-3.5 via the OpenAI API key."],"metadata":{"id":"B7cJnBTxBa3h"}},{"cell_type":"markdown","source":["## Mount Google Drive"],"metadata":{"id":"j9NLS82cGq5F"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"msAJWILyGttj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739278310621,"user_tz":-60,"elapsed":19337,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"e5fc0528-d628-4185-d7f3-c21fba510e8a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"markdown","source":["## Git add and commit"],"metadata":{"id":"8rUN7eCZBkXU"}},{"cell_type":"code","source":["!git clone https://alikova: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@github.com/alikova/paraphrase-categorization.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189158415,"user_tz":-60,"elapsed":840,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"e0481015-96d6-4184-f573-ee0d627b5673","id":"9s71E83_BkXU"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'paraphrase-categorization'...\n","remote: Enumerating objects: 29, done.\u001b[K\n","remote: Counting objects: 100% (29/29), done.\u001b[K\n","remote: Compressing objects: 100% (22/22), done.\u001b[K\n","remote: Total 29 (delta 10), reused 18 (delta 5), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (29/29), 518.56 KiB | 16.20 MiB/s, done.\n","Resolving deltas: 100% (10/10), done.\n"]}]},{"cell_type":"code","source":["!ls /content\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189161207,"user_tz":-60,"elapsed":127,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"9a7b9d8c-35e0-4c49-e459-b27a5782e492","id":"tAfkbrc3BkXV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["drive  paraphrase-categorization  sample_data\n"]}]},{"cell_type":"code","source":["%cd /content/paraphrase-categorization\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189163569,"user_tz":-60,"elapsed":113,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"820dddf2-a4f5-4b28-a17c-195e0d73427f","id":"-tcdAMSyBkXV"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/paraphrase-categorization\n","/content/paraphrase-categorization\n"]}]},{"cell_type":"code","source":["!find /content -name \"Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n","\n","!ls /content/paraphrase-categorization/\n","\n","!ls /content/drive/MyDrive/\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1739189165188,"user_tz":-60,"elapsed":441,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"id":"uVLQ8AlpBkXW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1af77602-16e4-4c9c-d7bd-7a08c8258bd5"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/paraphrase-categorization/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," kaggle_prevajajanje_zbirk.html\n","'Load and translate Paraphrases with GaMS _ Kaggle.html'\n"," paraphrase_datasets_translation_GaMS_gpu\n"," Paraphrase_Datasets_Translations_GaMS.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," paraphrase_read-and-translate.py\n"," README.md\n","'Colab Notebooks'   Translated_Datasets\n"]}]},{"cell_type":"code","source":["!find /content/drive/ -name \"Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLpfCqYYHay-","executionInfo":{"status":"ok","timestamp":1739189167282,"user_tz":-60,"elapsed":133,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"4fd87263-4f2e-4a08-ea5c-a0f39d25d8e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"]}]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\" /content/paraphrase-categorization/\n","\n","!ls /content/paraphrase-categorization/\n","%cd /content/paraphrase-categorization\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189171401,"user_tz":-60,"elapsed":656,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"7c516d1a-bfe9-475d-f6fd-d74d511f6f44","id":"HlEcO_pxBkXW"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" kaggle_prevajajanje_zbirk.html\n","'Load and translate Paraphrases with GaMS _ Kaggle.html'\n"," paraphrase_datasets_translation_GaMS_gpu\n"," Paraphrase_Datasets_Translations_GaMS.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," paraphrase_read-and-translate.py\n"," README.md\n","/content/paraphrase-categorization\n"]}]},{"cell_type":"code","source":["!ls -a\n","\n","!git add Paraphrase_Datasets_Translations_GPT3.5.ipynb\n","!git status\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189188860,"user_tz":-60,"elapsed":401,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"55870e7f-b2c1-44bb-9ef3-4619e7f8f82a","id":"sZYez0CLBkXX"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":[" .\n"," ..\n"," .git\n"," kaggle_prevajajanje_zbirk.html\n","'Load and translate Paraphrases with GaMS _ Kaggle.html'\n"," paraphrase_datasets_translation_GaMS_gpu\n"," Paraphrase_Datasets_Translations_GaMS.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," paraphrase_read-and-translate.py\n"," README.md\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes to be committed:\n","  (use \"git restore --staged <file>...\" to unstage)\n","\t\u001b[32mmodified:   Paraphrase_Datasets_Translations_GPT3.5.ipynb\u001b[m\n","\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"z.alenka7@gmail.com\"\n","!git config --global user.name \"alikova\"\n"],"metadata":{"id":"95QXRLcUBkXX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"Update Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189194414,"user_tz":-60,"elapsed":118,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"9a82e724-a7b7-434b-f672-8357c0811f12","id":"GcR-ZHLjBkXX"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 8ac2b37] Update Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite Paraphrase_Datasets_Translations_GPT3.5.ipynb (95%)\n"]}]},{"cell_type":"code","source":["!git push origin main  # or the appropriate branch name if it's not 'main'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739189199780,"user_tz":-60,"elapsed":602,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"be5cee3c-b549-4e1a-9471-c0a874c9b117","id":"sj0U_6hoBkXY"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 5, done.\n","Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n","Delta compression using up to 8 threads\n","Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n","Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 1.17 KiB | 599.00 KiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas:   0% (0/2)\u001b[K\rremote: Resolving deltas:  50% (1/2)\u001b[K\rremote: Resolving deltas: 100% (2/2)\u001b[K\rremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/alikova/paraphrase-categorization.git\n","   4f90302..8ac2b37  main -> main\n"]}]},{"cell_type":"markdown","source":["## Installations"],"metadata":{"id":"-BPcvwxZBkXY"}},{"cell_type":"code","source":["pip uninstall openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cftOsVoceHhc","executionInfo":{"status":"ok","timestamp":1739278323309,"user_tz":-60,"elapsed":6983,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"3d166a23-57c1-4b2b-9b30-15fed7ed3e54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: openai 1.61.1\n","Uninstalling openai-1.61.1:\n","  Would remove:\n","    /usr/local/bin/openai\n","    /usr/local/lib/python3.11/dist-packages/openai-1.61.1.dist-info/*\n","    /usr/local/lib/python3.11/dist-packages/openai/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled openai-1.61.1\n"]}]},{"cell_type":"code","source":["!pip uninstall -y openai\n","!pip install openai>=1.0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umygU4wve8r0","executionInfo":{"status":"ok","timestamp":1739278331049,"user_tz":-60,"elapsed":5024,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"6586fc7e-063b-4c58-c279-3732e292e2cb"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping openai as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0m"]}]},{"cell_type":"code","source":["!pip uninstall -y openai\n","!pip install openai>=1.0.0\n","!python -c \"import openai; print(openai.__version__)\"  # Should print 1.x.x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lqjl4jP7_R7t","executionInfo":{"status":"ok","timestamp":1739277025472,"user_tz":-60,"elapsed":6286,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"f2b617bf-8fda-4aca-f3f0-b9065eb28a6a"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: openai 1.61.1\n","Uninstalling openai-1.61.1:\n","  Successfully uninstalled openai-1.61.1\n","1.61.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"sX-obQ9KBkXZ"},"source":["## Connect to OpenAI with an API key"]},{"cell_type":"code","source":["import os\n","import openai"],"metadata":{"id":"rZdz89fScAR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","#userdata.get('OPENAI_API_KEY')\n"],"metadata":{"id":"gaQuw2A9BSvb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Retrieve the API key\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","\n","# Verify if the key exists (good practice)\n","if api_key is None:\n","    raise ValueError(\"API key not found in environment variables\")\n","\n","print(\"API key successfully loaded\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wGD1XIodUgT","executionInfo":{"status":"ok","timestamp":1738916298717,"user_tz":-60,"elapsed":698,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"ddcedc92-07b3-4c44-8f4c-6e4e5cc59af9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["API key successfully loaded\n"]}]},{"cell_type":"code","source":["# This will retrieve the key you configured in Colab's secrets\n","api_key = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"evqZmL5YhEIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an OpenAI client (New API format)\n","from openai import OpenAI  # We'll use synchronous version instead\n","\n","# Instead of AsyncOpenAI, we'll modify our code to use the synchronous version:\n","client = OpenAI(api_key=api_key)"],"metadata":{"id":"o3fsR0bCmXa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example OpenAI API request\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}]\n",")\n","\n","print(response.choices[0].message.content)\n","print(response.usage.total_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBzGPkvFgr2w","executionInfo":{"status":"ok","timestamp":1738919275914,"user_tz":-60,"elapsed":738,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"fe35311e-4649-4b4f-9b4d-36d39bad57db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Why was the math book sad?\n","\n","Because it had too many problems.\n","27\n"]}]},{"cell_type":"markdown","metadata":{"id":"9_PrDfecBkXb"},"source":["# Translate with Pipeline Programming"]},{"cell_type":"markdown","source":["#####           Lower Temperature (closer to 0): The model will be more deterministic and predictable, choosing the words with the highest probability. It will tend to produce more repetitive and safe outputs. This is often preferred for tasks where accuracy and consistency are paramount, such as translation or factual question answering. Higher Temperature (closer to 1 or above): The model becomes more creative and unpredictable. It's more likely to sample from less probable words, leading to more diverse and unexpected outputs. This is suitable for tasks where creativity and novelty are desired, such as creative writing or brainstorming.\n"],"metadata":{"id":"lq-c_WA97e72"}},{"cell_type":"code","source":["from typing import Iterator, List, Dict, Any, Tuple, Optional\n","from openai.types.chat import ChatCompletion\n","from openai import OpenAI\n","import json\n","from pathlib import Path\n","from datetime import datetime\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n","        self.client = OpenAI(api_key=api_key)\n","        self.cache_dir = Path(cache_dir)\n","        self.checkpoint_dir = Path(checkpoint_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.checkpoint_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = []\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if not text:  # Skip empty strings\n","                    translations.append(\"\")\n","                    continue\n","                if text in cache:\n","                    translations.append(cache[text])\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                try:\n","                    response = self.client.chat.completions.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                            {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n","                        ],\n","                        temperature=0.3\n","                    )\n","\n","                    # Get translations from response\n","                    new_translations = [choice.message.content for choice in response.choices]\n","\n","                    # Ensure we have the same number of translations as input texts\n","                    if len(new_translations) != len(uncached_texts):\n","                        new_translations = new_translations[:len(uncached_texts)]\n","                        if len(new_translations) < len(uncached_texts):\n","                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                    # Update cache\n","                    for text, trans in zip(uncached_texts, new_translations):\n","                        self.batch_cache[text] = trans\n","                        self.current_cache_size += 1\n","\n","                    # Insert translations at correct positions\n","                    for idx, trans in zip(uncached_indices, new_translations):\n","                        translations.insert(idx, trans)\n","\n","                    if self.current_cache_size >= self.max_cache_size:\n","                        self._save_batch_cache(batch_id)\n","\n","                    return translations, response\n","\n","                except Exception as e:\n","                    print(f\"Error in API call: {str(e)}\")\n","                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def get_checkpoint_file(self, dataset_name: str) -> Path:\n","        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n","        checkpoint_data = {\n","            \"last_batch\": batch_num,\n","            \"translations_count\": translations_count,\n","            \"timestamp\": datetime.now().isoformat()\n","        }\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","\n","    def load_checkpoint(self, dataset_name: str) -> Dict:\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        if checkpoint_file.exists():\n","            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {\"last_batch\": -1, \"translations_count\": 0}\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = []\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if not text:  # Skip empty strings\n","                    translations.append(\"\")\n","                    continue\n","                if text in cache:\n","                    translations.append(cache[text])\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                try:\n","                    response = self.client.chat.completions.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                            {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n","                        ],\n","                        temperature=0.3\n","                    )\n","\n","                    # new_translations = response.choices[0].message.content.split(\"\\n---\\n\") # for version 0.28\n","                    new_translations = [choice.message.content for choice in response.choices]\n","\n","                    # Ensure we have the same number of translations as input texts\n","                    if len(new_translations) != len(uncached_texts):\n","                        new_translations = new_translations[:len(uncached_texts)]\n","                        if len(new_translations) < len(uncached_texts):\n","                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                    for text, trans in zip(uncached_texts, new_translations):\n","                        self.batch_cache[text] = trans\n","                        self.current_cache_size += 1\n","\n","                    for idx, trans in zip(uncached_indices, new_translations):\n","                        translations.insert(idx, trans)\n","\n","                    if self.current_cache_size >= self.max_cache_size:\n","                        self._save_batch_cache(batch_id)\n","\n","                    return translations, response\n","\n","                except Exception as e:\n","                    print(f\"Error in API call: {str(e)}\")\n","                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.text_column_index = text_column_index  # Default to second column (index 1)\n","        self.total_lines = self._count_lines()\n","        # Add 1 to account for header\n","        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n","\n","    def _count_lines(self) -> int:\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                return sum(1 for _ in f)\n","        except Exception as e:\n","            print(f\"Error counting lines: {str(e)}\")\n","            return 0\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        current_line = 0\n","\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                # Skip header\n","                header = next(f, None)\n","                if not header:\n","                    raise ValueError(\"Empty file or no header found\")\n","\n","                # Skip to start line (accounting for already skipped header)\n","                for _ in range(self.start_line - 1):\n","                    next(f, None)\n","                    current_line += 1\n","\n","                for line in f:\n","                    try:\n","                        columns = line.strip().split('\\t')\n","                        if len(columns) > self.text_column_index:\n","                            text = columns[self.text_column_index].strip()\n","                            if text:  # Only add non-empty texts\n","                                current_batch.append(text)\n","                                if len(current_batch) == self.batch_size:\n","                                    yield current_batch\n","                                    current_batch = []\n","                    except Exception as e:\n","                        print(f\"Error processing line: {line.strip()}\")\n","                        print(f\"Error details: {str(e)}\")\n","                        continue\n","\n","                if current_batch:  # Don't forget last partial batch\n","                    yield current_batch\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {str(e)}\")\n","            if current_batch:  # Yield any remaining batch on error\n","                yield current_batch\n","\n","class CostTracker:\n","    def __init__(self):\n","        self.requests = 0\n","        self.total_tokens = 0\n","        self.price_per_1k_tokens = 0.002\n","\n","    def update(self, response: ChatCompletion):\n","        self.requests += 1\n","        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n","\n","    def get_cost(self) -> float:\n","        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n","\n","    def report(self) -> str:\n","        return f\"\"\"\n","        API Calls: {self.requests}\n","        Total Tokens: {self.total_tokens}\n","        Estimated Cost: ${self.get_cost():.2f}\n","        \"\"\"\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    for filename, data in [\n","        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n","        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n","    ]:\n","        try:\n","            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n","                for item in data:\n","                    f.write(f\"{item}\\n\")\n","        except Exception as e:\n","            print(f\"Error saving to {filename}: {str(e)}\")\n","\n","    try:\n","        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n","            for orig, trans in zip(originals, translations):\n","                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n","    except Exception as e:\n","        print(f\"Error saving aligned pairs: {str(e)}\")\n","\n","def process_dataset(\n","    input_file: str,\n","    dataset_name: str,\n","    api_key: str,\n","    batch_size: int = 32,\n","    checkpoint_interval: int = 5\n","):\n","    translator = TranslationManager(api_key=api_key)\n","\n","    # Load checkpoint if exists\n","    checkpoint = translator.load_checkpoint(dataset_name)\n","    start_batch = checkpoint[\"last_batch\"] + 1\n","    translations_count = checkpoint[\"translations_count\"]\n","\n","    # Calculate starting line\n","    start_line = start_batch * batch_size\n","\n","    dataset_iterator = DatasetIterator(input_file, batch_size, start_line)\n","    cost_tracker = CostTracker()\n","\n","    total_batches = max(1, dataset_iterator.total_lines // batch_size)\n","\n","    print(f\"Starting from batch {start_batch}, line {start_line}\")\n","    print(f\"Total lines in file: {dataset_iterator.total_lines}\")\n","\n","    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n","        try:\n","            batch_id = f\"{dataset_name}_{batch_num}\"\n","            translations, response = translator.translate_batch(batch, batch_id)\n","\n","            if response is not None:\n","                cost_tracker.update(response)\n","\n","            # Only save if we got valid translations\n","            if translations:\n","                save_pairs(batch, translations, dataset_name, mode=\"a\")\n","                translations_count += len(translations)\n","\n","            if batch_num % checkpoint_interval == 0:\n","                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n","                print(f\"Checkpoint saved at batch {batch_num}\")\n","\n","            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n","            if batch_num % 10 == 0:\n","                print(cost_tracker.report())\n","\n","            # Add a small delay to avoid rate limiting\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Error processing batch {batch_num}: {str(e)}\")\n","            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n","            time.sleep(5)  # Longer delay on error\n","            continue\n","\n","    # Final checkpoint and report\n","    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n","    print(\"\\nFinal Statistics:\")\n","    print(cost_tracker.report())\n","    print(f\"Total translations: {translations_count}\")"],"metadata":{"id":"k5De-X5UnN-8","executionInfo":{"status":"ok","timestamp":1739272791974,"user_tz":-60,"elapsed":663,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#### Testing the translation on the first 100 or 10 sentences"],"metadata":{"id":"4FhHiV9ewezJ"}},{"cell_type":"code","source":["# try to first fecth first 100 sentences\n","\n","from typing import Iterator, List, Dict, Any, Tuple, Optional\n","import numpy as np\n","import json\n","import os\n","from pathlib import Path\n","from datetime import datetime\n","from openai import OpenAI\n","from openai.types.chat import ChatCompletion\n","import time\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n","        self.client = OpenAI(api_key=api_key)\n","        self.cache_dir = Path(cache_dir)\n","        self.checkpoint_dir = Path(checkpoint_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.checkpoint_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def get_checkpoint_file(self, dataset_name: str) -> Path:\n","        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n","        checkpoint_data = {\n","            \"last_batch\": batch_num,\n","            \"translations_count\": translations_count,\n","            \"timestamp\": datetime.now().isoformat()\n","        }\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","\n","    def load_checkpoint(self, dataset_name: str) -> Dict:\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        if checkpoint_file.exists():\n","            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {\"last_batch\": -1, \"translations_count\": 0}\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = []\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if not text:  # Skip empty strings\n","                    translations.append(\"\")\n","                    continue\n","                if text in cache:\n","                    translations.append(cache[text])\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                try:\n","                    response = self.client.chat.completions.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                            {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n","                        ],\n","                        temperature=0.3\n","                    )\n","\n","                    new_translations = [choice.message.content for choice in response.choices]\n","\n","                    # Ensure we have the same number of translations as input texts\n","                    if len(new_translations) != len(uncached_texts):\n","                        new_translations = new_translations[:len(uncached_texts)]\n","                        if len(new_translations) < len(uncached_texts):\n","                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                    for text, trans in zip(uncached_texts, new_translations):\n","                        self.batch_cache[text] = trans\n","                        self.current_cache_size += 1\n","\n","                    for idx, trans in zip(uncached_indices, new_translations):\n","                        translations.insert(idx, trans)\n","\n","                    if self.current_cache_size >= self.max_cache_size:\n","                        self._save_batch_cache(batch_id)\n","\n","                    return translations, response\n","\n","                except Exception as e:\n","                    print(f\"Error in API call: {str(e)}\")\n","                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 100):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.max_sentences = max_sentences\n","        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n","        self.start_line = min(start_line, max(0, self.total_lines - 1))\n","\n","    def _count_lines(self) -> int:\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                return sum(1 for _ in f)\n","        except Exception as e:\n","            print(f\"Error counting lines: {str(e)}\")\n","            return 0\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        processed_lines = 0\n","\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                # Skip to start line\n","                for _ in range(self.start_line):\n","                    next(f, None)\n","\n","                for line in f:\n","                    if processed_lines >= self.max_sentences:\n","                        break\n","\n","                    line = line.strip()\n","                    if line:  # Only add non-empty lines\n","                        current_batch.append(line)\n","                        processed_lines += 1\n","\n","                        if len(current_batch) == self.batch_size:\n","                            yield current_batch\n","                            current_batch = []\n","\n","                    if processed_lines >= self.max_sentences:\n","                        break\n","\n","                if current_batch:  # Don't forget last partial batch\n","                    yield current_batch\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {str(e)}\")\n","            if current_batch:  # Yield any remaining batch on error\n","                yield current_batch\n","\n","class CostTracker:\n","    def __init__(self):\n","        self.requests = 0\n","        self.total_tokens = 0\n","        self.price_per_1k_tokens = 0.002\n","\n","    def update(self, response: ChatCompletion):\n","        self.requests += 1\n","        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n","\n","    def get_cost(self) -> float:\n","        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n","\n","    def report(self) -> str:\n","        return f\"\"\"\n","        API Calls: {self.requests}\n","        Total Tokens: {self.total_tokens}\n","        Estimated Cost: ${self.get_cost():.2f}\n","        \"\"\"\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    for filename, data in [\n","        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n","        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n","    ]:\n","        try:\n","            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n","                for item in data:\n","                    f.write(f\"{item}\\n\")\n","        except Exception as e:\n","            print(f\"Error saving to {filename}: {str(e)}\")\n","\n","    try:\n","        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n","            for orig, trans in zip(originals, translations):\n","                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n","    except Exception as e:\n","        print(f\"Error saving aligned pairs: {str(e)}\")\n","\n","def process_dataset(\n","    input_file: str,\n","    dataset_name: str,\n","    api_key: str,\n","    batch_size: int = 32,\n","    checkpoint_interval: int = 5,\n","    max_sentences: int = 10\n","):\n","    translator = TranslationManager(api_key=api_key)\n","\n","    # Load checkpoint if exists\n","    checkpoint = translator.load_checkpoint(dataset_name)\n","    start_batch = checkpoint[\"last_batch\"] + 1\n","    translations_count = checkpoint[\"translations_count\"]\n","\n","    # Calculate starting line\n","    start_line = start_batch * batch_size\n","\n","    # Create iterator with sentence limit\n","    dataset_iterator = DatasetIterator(\n","        file_path=input_file,\n","        batch_size=batch_size,\n","        start_line=start_line,\n","        max_sentences=max_sentences\n","    )\n","\n","    cost_tracker = CostTracker()\n","    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n","\n","    print(f\"Starting from batch {start_batch}, line {start_line}\")\n","    print(f\"Will process up to {max_sentences} sentences\")\n","    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n","\n","    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n","        try:\n","            batch_id = f\"{dataset_name}_{batch_num}\"\n","            translations, response = translator.translate_batch(batch, batch_id)\n","\n","            if response is not None:\n","                cost_tracker.update(response)\n","\n","            save_pairs(batch, translations, dataset_name, mode=\"a\")\n","            translations_count += len(translations)\n","\n","            if batch_num % checkpoint_interval == 0:\n","                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n","                print(f\"Checkpoint saved at batch {batch_num}\")\n","\n","            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n","            if batch_num % 10 == 0:\n","                print(cost_tracker.report())\n","\n","            # Add a small delay to avoid rate limiting\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Error processing batch {batch_num}: {str(e)}\")\n","            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n","            time.sleep(1)  # Longer delay on error\n","            continue\n","\n","    # Final checkpoint and report\n","    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n","    print(\"\\nFinal Statistics:\")\n","    print(cost_tracker.report())\n","    print(f\"Total translations: {translations_count}\")"],"metadata":{"id":"Pj2w8X4eDJnm","executionInfo":{"status":"ok","timestamp":1739271886906,"user_tz":-60,"elapsed":90,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":22,"outputs":[]},{"cell_type":"code","source":["# try\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase_test\"  # Using test suffix to distinguish from full runs\n","    max_sentences = 10  # Limit to first 100 sentences\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=32,  # Keeping original batch size\n","        checkpoint_interval=2,  # Frequent checkpoints for testing\n","        max_sentences=10  # Parameter to limit number of sentences\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5tHrPhDDvMM","executionInfo":{"status":"ok","timestamp":1739205034997,"user_tz":-60,"elapsed":9052,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"3ee22661-6f04-4072-b6c4-a9764d35ccca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting from batch 1, line 32\n","Will process up to 10 sentences\n","Total lines to process: 10\n","Processed batch 1/1 (10 items)\n","\n","Final Statistics:\n","\n","        API Calls: 1\n","        Total Tokens: 1276\n","        Estimated Cost: $0.00\n","        \n","Total translations: 20\n"]}]},{"cell_type":"markdown","metadata":{"id":"BCcetVRwBkXc"},"source":["## msr_paraphrase_data.txt"]},{"cell_type":"markdown","metadata":{"id":"X7Li0568BkXc"},"source":["#### Checking the size of the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737978986468,"user_tz":-60,"elapsed":4870,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"0e863ca1-6db2-418a-a059-f16b50fe1efc","id":"HVE4UDXYBkXc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File found!\n","The file has 10949 rows.\n"]}],"source":["# Check the size of the file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","\n","# Check if file exists\n","import os\n","if os.path.exists(file_path):\n","    print(\"File found!\")\n","else:\n","    print(\"File not found. Check the path!\")\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","\n","# Count lines in the file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n","\n","print(f\"The file has {line_count} rows.\")\n"]},{"cell_type":"markdown","metadata":{"id":"tnJIJ9oNBkXf"},"source":["#### Translation of msr paraphrase dataset - Executing the code for 1h\n","\n","Final Statistics:\n","\n","        API Calls: 172\n","        Total Tokens: 1094704\n","        Estimated Cost: $2.19\n"]},{"cell_type":"code","source":["# new, more robust and for non-asynchronous function\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase\"\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=64,\n","        checkpoint_interval=5\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNao8vk4CdtN","outputId":"6eb3411a-3845-4df1-f163-8c103fed2776"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting from batch 0, line 0\n","Total lines in file: 10949\n","Checkpoint saved at batch 0\n","Processed batch 0/171 (64 items)\n","\n","        API Calls: 1\n","        Total Tokens: 4420\n","        Estimated Cost: $0.01\n","        \n","Processed batch 1/171 (64 items)\n","Processed batch 2/171 (64 items)\n","Processed batch 3/171 (64 items)\n","Processed batch 4/171 (64 items)\n","Checkpoint saved at batch 5\n","Processed batch 5/171 (64 items)\n","Processed batch 6/171 (64 items)\n","Processed batch 7/171 (64 items)\n","Processed batch 8/171 (64 items)\n","Processed batch 9/171 (64 items)\n","Checkpoint saved at batch 10\n","Processed batch 10/171 (64 items)\n","\n","        API Calls: 11\n","        Total Tokens: 71280\n","        Estimated Cost: $0.14\n","        \n","Processed batch 11/171 (64 items)\n","Processed batch 12/171 (64 items)\n","Processed batch 13/171 (64 items)\n","Processed batch 14/171 (64 items)\n","Checkpoint saved at batch 15\n","Processed batch 15/171 (64 items)\n","Processed batch 16/171 (64 items)\n","Processed batch 17/171 (64 items)\n","Processed batch 18/171 (64 items)\n","Processed batch 19/171 (64 items)\n","Checkpoint saved at batch 20\n","Processed batch 20/171 (64 items)\n","\n","        API Calls: 21\n","        Total Tokens: 133185\n","        Estimated Cost: $0.27\n","        \n","Processed batch 21/171 (64 items)\n","Processed batch 22/171 (64 items)\n","Processed batch 23/171 (64 items)\n","Processed batch 24/171 (64 items)\n","Checkpoint saved at batch 25\n","Processed batch 25/171 (64 items)\n","Processed batch 26/171 (64 items)\n","Processed batch 27/171 (64 items)\n","Processed batch 28/171 (64 items)\n","Processed batch 29/171 (64 items)\n","Checkpoint saved at batch 30\n","Processed batch 30/171 (64 items)\n","\n","        API Calls: 31\n","        Total Tokens: 192489\n","        Estimated Cost: $0.38\n","        \n","Processed batch 31/171 (64 items)\n","Processed batch 32/171 (64 items)\n","Processed batch 33/171 (64 items)\n","Processed batch 34/171 (64 items)\n","Checkpoint saved at batch 35\n","Processed batch 35/171 (64 items)\n","Processed batch 36/171 (64 items)\n","Processed batch 37/171 (64 items)\n","Processed batch 38/171 (64 items)\n","Processed batch 39/171 (64 items)\n","Checkpoint saved at batch 40\n","Processed batch 40/171 (64 items)\n","\n","        API Calls: 41\n","        Total Tokens: 259937\n","        Estimated Cost: $0.52\n","        \n","Processed batch 41/171 (64 items)\n","Processed batch 42/171 (64 items)\n","Processed batch 43/171 (64 items)\n","Processed batch 44/171 (64 items)\n","Checkpoint saved at batch 45\n","Processed batch 45/171 (64 items)\n","Processed batch 46/171 (64 items)\n","Processed batch 47/171 (64 items)\n","Processed batch 48/171 (64 items)\n","Processed batch 49/171 (64 items)\n","Checkpoint saved at batch 50\n","Processed batch 50/171 (64 items)\n","\n","        API Calls: 51\n","        Total Tokens: 328845\n","        Estimated Cost: $0.66\n","        \n","Processed batch 51/171 (64 items)\n","Processed batch 52/171 (64 items)\n","Processed batch 53/171 (64 items)\n","Processed batch 54/171 (64 items)\n","Checkpoint saved at batch 55\n","Processed batch 55/171 (64 items)\n","Processed batch 56/171 (64 items)\n","Processed batch 57/171 (64 items)\n","Processed batch 58/171 (64 items)\n","Processed batch 59/171 (64 items)\n","Checkpoint saved at batch 60\n","Processed batch 60/171 (64 items)\n","\n","        API Calls: 61\n","        Total Tokens: 396591\n","        Estimated Cost: $0.79\n","        \n","Processed batch 61/171 (64 items)\n","Processed batch 62/171 (64 items)\n","Processed batch 63/171 (64 items)\n","Processed batch 64/171 (64 items)\n","Checkpoint saved at batch 65\n","Processed batch 65/171 (64 items)\n","Processed batch 66/171 (64 items)\n","Processed batch 67/171 (64 items)\n","Processed batch 68/171 (64 items)\n","Processed batch 69/171 (64 items)\n","Checkpoint saved at batch 70\n","Processed batch 70/171 (64 items)\n","\n","        API Calls: 71\n","        Total Tokens: 456596\n","        Estimated Cost: $0.91\n","        \n","Processed batch 71/171 (64 items)\n","Processed batch 72/171 (64 items)\n","Processed batch 73/171 (64 items)\n","Processed batch 74/171 (64 items)\n","Checkpoint saved at batch 75\n","Processed batch 75/171 (64 items)\n","Processed batch 76/171 (64 items)\n","Processed batch 77/171 (64 items)\n","Processed batch 78/171 (64 items)\n","Processed batch 79/171 (64 items)\n","Checkpoint saved at batch 80\n","Processed batch 80/171 (64 items)\n","\n","        API Calls: 81\n","        Total Tokens: 515085\n","        Estimated Cost: $1.03\n","        \n","Processed batch 81/171 (64 items)\n","Processed batch 82/171 (64 items)\n","Processed batch 83/171 (64 items)\n","Processed batch 84/171 (64 items)\n","Checkpoint saved at batch 85\n","Processed batch 85/171 (64 items)\n","Processed batch 86/171 (64 items)\n","Processed batch 87/171 (64 items)\n","Processed batch 88/171 (64 items)\n","Processed batch 89/171 (64 items)\n","Checkpoint saved at batch 90\n","Processed batch 90/171 (64 items)\n","\n","        API Calls: 91\n","        Total Tokens: 573380\n","        Estimated Cost: $1.15\n","        \n","Processed batch 91/171 (64 items)\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwQOdDo6BkXf","executionInfo":{"status":"ok","timestamp":1738865559047,"user_tz":-60,"elapsed":5157663,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"cfbf12d0-a925-456c-d94f-f62cdeddad9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processed batch 5/343\n","Processed batch 6/343\n","Processed batch 0/172\n","\n","        API Calls: 1\n","        Total Tokens: 7487\n","        Estimated Cost: $0.01\n","        \n","Processed batch 7/343\n","Processed batch 1/172\n","Processed batch 2/172\n","Processed batch 8/343\n","Processed batch 9/343\n","Processed batch 3/172\n","Processed batch 10/343\n","\n","        API Calls: 11\n","        Total Tokens: 39969\n","        Estimated Cost: $0.08\n","        \n","Processed batch 11/343\n","Processed batch 4/172\n","Processed batch 12/343\n","Processed batch 5/172\n","Processed batch 6/172\n","Processed batch 13/343\n","Processed batch 14/343\n","Processed batch 7/172\n","Processed batch 15/343\n","Processed batch 16/343\n","Processed batch 8/172\n","Processed batch 9/172\n","Processed batch 10/172\n","\n","        API Calls: 11\n","        Total Tokens: 70814\n","        Estimated Cost: $0.14\n","        \n","Processed batch 11/172\n","Processed batch 17/343\n","Processed batch 18/343\n","Processed batch 19/343\n","Processed batch 12/172\n","Processed batch 20/343\n","\n","        API Calls: 21\n","        Total Tokens: 78213\n","        Estimated Cost: $0.16\n","        \n","Processed batch 21/343\n","Processed batch 13/172\n","Processed batch 22/343\n","Processed batch 14/172\n","Processed batch 23/343\n","Processed batch 24/343\n","Processed batch 25/343\n","Processed batch 15/172\n","Processed batch 16/172\n","Processed batch 26/343\n","Processed batch 27/343\n","Processed batch 17/172\n","Processed batch 28/343\n","Processed batch 18/172\n","Processed batch 19/172\n","Processed batch 20/172\n","\n","        API Calls: 21\n","        Total Tokens: 130578\n","        Estimated Cost: $0.26\n","        \n","Processed batch 29/343\n","Processed batch 30/343\n","\n","        API Calls: 31\n","        Total Tokens: 115868\n","        Estimated Cost: $0.23\n","        \n","Processed batch 31/343\n","Processed batch 21/172\n","Processed batch 32/343\n","Processed batch 33/343\n","Processed batch 22/172\n","Processed batch 34/343\n","Processed batch 35/343\n","Processed batch 36/343\n","Processed batch 23/172\n","Processed batch 37/343\n","Processed batch 38/343\n","Processed batch 24/172\n","Processed batch 39/343\n","Processed batch 25/172\n","Processed batch 40/343\n","\n","        API Calls: 41\n","        Total Tokens: 150629\n","        Estimated Cost: $0.30\n","        \n","Processed batch 41/343\n","Processed batch 26/172\n","Processed batch 42/343\n","Processed batch 43/343\n","Processed batch 44/343\n","Processed batch 27/172\n","Processed batch 45/343\n","Processed batch 28/172\n","Processed batch 46/343\n","Processed batch 29/172\n","Processed batch 47/343\n","Processed batch 30/172\n","\n","        API Calls: 31\n","        Total Tokens: 199562\n","        Estimated Cost: $0.40\n","        \n","Processed batch 31/172\n","Processed batch 48/343\n","Processed batch 49/343\n","Processed batch 50/343\n","\n","        API Calls: 51\n","        Total Tokens: 185868\n","        Estimated Cost: $0.37\n","        \n","Processed batch 32/172\n","Processed batch 33/172\n","Processed batch 51/343\n","Processed batch 34/172\n","Processed batch 52/343\n","Processed batch 53/343\n","Processed batch 35/172\n","Processed batch 54/343\n","Processed batch 36/172\n","Processed batch 55/343\n","Processed batch 56/343\n","Processed batch 57/343\n","Processed batch 58/343\n","Processed batch 37/172\n","Processed batch 59/343\n","Processed batch 60/343\n","\n","        API Calls: 61\n","        Total Tokens: 219779\n","        Estimated Cost: $0.44\n","        \n","Processed batch 38/172\n","Processed batch 61/343\n","Processed batch 62/343\n","Processed batch 63/343\n","Processed batch 64/343\n","Processed batch 39/172\n","Processed batch 65/343\n","Processed batch 66/343\n","Processed batch 40/172\n","\n","        API Calls: 41\n","        Total Tokens: 265480\n","        Estimated Cost: $0.53\n","        \n","Processed batch 67/343\n","Processed batch 41/172\n","Processed batch 68/343\n","Processed batch 69/343\n","Processed batch 70/343\n","\n","        API Calls: 71\n","        Total Tokens: 253708\n","        Estimated Cost: $0.51\n","        \n","Processed batch 42/172\n","Processed batch 71/343\n","Processed batch 72/343\n","Processed batch 43/172\n","Processed batch 73/343\n","Processed batch 74/343\n","Processed batch 44/172\n","Processed batch 75/343\n","Processed batch 45/172\n","Processed batch 76/343\n","Processed batch 77/343\n","Processed batch 78/343\n","Processed batch 46/172\n","Processed batch 79/343\n","Processed batch 47/172\n","Processed batch 80/343\n","\n","        API Calls: 81\n","        Total Tokens: 292033\n","        Estimated Cost: $0.58\n","        \n","Processed batch 81/343\n","Processed batch 48/172\n","Processed batch 82/343\n","Processed batch 83/343\n","Processed batch 84/343\n","Processed batch 49/172\n","Processed batch 50/172\n","\n","        API Calls: 51\n","        Total Tokens: 337627\n","        Estimated Cost: $0.68\n","        \n","Processed batch 85/343\n","Processed batch 86/343\n","Processed batch 51/172\n","Processed batch 87/343\n","Processed batch 52/172\n","Processed batch 88/343\n","Processed batch 89/343\n","Processed batch 90/343\n","\n","        API Calls: 90\n","        Total Tokens: 324480\n","        Estimated Cost: $0.65\n","        \n","Processed batch 53/172\n","Processed batch 91/343\n","Processed batch 92/343\n","Processed batch 54/172\n","Processed batch 55/172\n","Processed batch 93/343\n","Processed batch 94/343\n","Processed batch 56/172\n","Processed batch 95/343\n","Processed batch 96/343\n","Processed batch 97/343\n","Processed batch 98/343\n","Processed batch 57/172\n","Processed batch 99/343\n","Processed batch 100/343\n","\n","        API Calls: 100\n","        Total Tokens: 361588\n","        Estimated Cost: $0.72\n","        \n","Processed batch 58/172\n","Processed batch 101/343\n","Processed batch 102/343\n","Processed batch 59/172\n","Processed batch 103/343\n","Processed batch 104/343\n","Processed batch 60/172\n","\n","        API Calls: 61\n","        Total Tokens: 405808\n","        Estimated Cost: $0.81\n","        \n","Processed batch 61/172\n","Processed batch 62/172\n","Processed batch 105/343\n","Processed batch 106/343\n","Processed batch 107/343\n","Processed batch 108/343\n","Processed batch 63/172\n","Processed batch 109/343\n","Processed batch 110/343\n","\n","        API Calls: 110\n","        Total Tokens: 399138\n","        Estimated Cost: $0.80\n","        \n","Processed batch 64/172\n","Processed batch 111/343\n","Processed batch 112/343\n","Processed batch 65/172\n","Processed batch 66/172\n","Processed batch 113/343\n","Processed batch 114/343\n","Processed batch 67/172\n","Processed batch 115/343\n","Processed batch 116/343\n","Processed batch 68/172\n","Processed batch 117/343\n","Processed batch 118/343\n","Processed batch 119/343\n","Processed batch 69/172\n","Processed batch 70/172\n","\n","        API Calls: 71\n","        Total Tokens: 468025\n","        Estimated Cost: $0.94\n","        \n","Processed batch 120/343\n","\n","        API Calls: 120\n","        Total Tokens: 435532\n","        Estimated Cost: $0.87\n","        \n","Processed batch 71/172\n","Processed batch 121/343\n","Processed batch 122/343\n","Processed batch 72/172\n","Processed batch 123/343\n","Processed batch 124/343\n","Processed batch 125/343\n","Processed batch 73/172\n","Processed batch 126/343\n","Processed batch 127/343\n","Processed batch 128/343\n","Processed batch 74/172\n","Processed batch 129/343\n","Processed batch 130/343\n","\n","        API Calls: 130\n","        Total Tokens: 471201\n","        Estimated Cost: $0.94\n","        \n","Processed batch 75/172\n","Processed batch 131/343\n","Processed batch 76/172\n","Processed batch 132/343\n","Processed batch 77/172\n","Processed batch 133/343\n","Processed batch 134/343\n","Processed batch 78/172\n","Processed batch 79/172\n","Processed batch 80/172\n","\n","        API Calls: 81\n","        Total Tokens: 532175\n","        Estimated Cost: $1.06\n","        \n","Processed batch 135/343\n","Processed batch 136/343\n","Processed batch 137/343\n","Processed batch 81/172\n","Processed batch 138/343\n","Processed batch 139/343\n","Processed batch 140/343\n","\n","        API Calls: 140\n","        Total Tokens: 507152\n","        Estimated Cost: $1.01\n","        \n","Processed batch 82/172\n","Processed batch 141/343\n","Processed batch 142/343\n","Processed batch 143/343\n","Processed batch 144/343\n","Processed batch 145/343\n","Processed batch 83/172\n","Processed batch 84/172\n","Processed batch 85/172\n","Processed batch 146/343\n","Processed batch 86/172\n","Processed batch 147/343\n","Processed batch 148/343\n","Processed batch 87/172\n","Processed batch 149/343\n","Processed batch 150/343\n","\n","        API Calls: 150\n","        Total Tokens: 539586\n","        Estimated Cost: $1.08\n","        \n","Processed batch 151/343\n","Processed batch 88/172\n","Processed batch 152/343\n","Processed batch 89/172\n","Processed batch 153/343\n","Processed batch 154/343\n","Processed batch 155/343\n","Processed batch 156/343\n","Processed batch 90/172\n","\n","        API Calls: 91\n","        Total Tokens: 596889\n","        Estimated Cost: $1.19\n","        \n","Processed batch 91/172\n","Processed batch 157/343\n","Processed batch 92/172\n","Processed batch 158/343\n","Processed batch 93/172\n","Processed batch 159/343\n","Processed batch 160/343\n","\n","        API Calls: 159\n","        Total Tokens: 571100\n","        Estimated Cost: $1.14\n","        \n","Processed batch 94/172\n","Processed batch 161/343\n","Processed batch 162/343\n","Processed batch 95/172\n","Processed batch 163/343\n","Processed batch 96/172\n","Processed batch 164/343\n","Processed batch 165/343\n","Processed batch 97/172\n","Processed batch 166/343\n","Processed batch 98/172\n","Processed batch 167/343\n","Processed batch 168/343\n","Processed batch 99/172\n","Processed batch 169/343\n","Processed batch 170/343\n","\n","        API Calls: 169\n","        Total Tokens: 608469\n","        Estimated Cost: $1.22\n","        \n","Processed batch 100/172\n","\n","        API Calls: 101\n","        Total Tokens: 657743\n","        Estimated Cost: $1.32\n","        \n","Processed batch 171/343\n","Processed batch 172/343\n","Processed batch 173/343\n","Processed batch 101/172\n","Processed batch 174/343\n","Processed batch 175/343\n","Processed batch 102/172\n","Processed batch 176/343\n","Processed batch 177/343\n","Processed batch 103/172\n","Processed batch 178/343\n","Processed batch 179/343\n","Processed batch 180/343\n","\n","        API Calls: 179\n","        Total Tokens: 640217\n","        Estimated Cost: $1.28\n","        \n","Processed batch 104/172\n","Processed batch 181/343\n","Processed batch 182/343\n","Processed batch 183/343\n","Processed batch 105/172\n","Processed batch 106/172\n","Processed batch 184/343\n","Processed batch 185/343\n","Processed batch 107/172\n","Processed batch 108/172\n","Processed batch 109/172\n","Processed batch 186/343\n","Processed batch 187/343\n","Processed batch 110/172\n","\n","        API Calls: 111\n","        Total Tokens: 722939\n","        Estimated Cost: $1.45\n","        \n","Processed batch 188/343\n","Processed batch 111/172\n","Processed batch 112/172\n","Processed batch 113/172\n","Processed batch 189/343\n","Processed batch 190/343\n","\n","        API Calls: 189\n","        Total Tokens: 677766\n","        Estimated Cost: $1.36\n","        \n","Processed batch 114/172\n","Processed batch 115/172\n","Processed batch 191/343\n","Processed batch 192/343\n","Processed batch 116/172\n","Processed batch 193/343\n","Processed batch 194/343\n","Processed batch 117/172\n","Processed batch 195/343\n","Processed batch 196/343\n","Processed batch 118/172\n","Processed batch 197/343\n","Processed batch 119/172\n","Processed batch 198/343\n","Processed batch 199/343\n","Processed batch 120/172\n","\n","        API Calls: 121\n","        Total Tokens: 781706\n","        Estimated Cost: $1.56\n","        \n","Processed batch 200/343\n","\n","        API Calls: 199\n","        Total Tokens: 712923\n","        Estimated Cost: $1.43\n","        \n","Processed batch 201/343\n","Processed batch 202/343\n","Processed batch 121/172\n","Processed batch 122/172\n","Processed batch 203/343\n","Processed batch 204/343\n","Processed batch 123/172\n","Processed batch 205/343\n","Processed batch 206/343\n","Processed batch 207/343\n","Processed batch 208/343\n","Processed batch 124/172\n","Processed batch 209/343\n","Processed batch 210/343\n","\n","        API Calls: 209\n","        Total Tokens: 745741\n","        Estimated Cost: $1.49\n","        \n","Processed batch 211/343\n","Processed batch 125/172\n","Processed batch 126/172\n","Processed batch 212/343\n","Processed batch 213/343\n","Processed batch 127/172\n","Processed batch 214/343\n","Processed batch 215/343\n","Processed batch 216/343\n","Processed batch 217/343\n","Processed batch 128/172\n","Processed batch 218/343\n","Processed batch 129/172\n","Processed batch 219/343\n","Processed batch 220/343\n","\n","        API Calls: 219\n","        Total Tokens: 779082\n","        Estimated Cost: $1.56\n","        \n","Processed batch 221/343\n","Processed batch 222/343\n","Processed batch 130/172\n","\n","        API Calls: 131\n","        Total Tokens: 848475\n","        Estimated Cost: $1.70\n","        \n","Processed batch 223/343\n","Processed batch 224/343\n","Processed batch 131/172\n","Processed batch 225/343\n","Processed batch 226/343\n","Processed batch 227/343\n","Processed batch 132/172\n","Processed batch 228/343\n","Processed batch 133/172\n","Processed batch 229/343\n","Processed batch 230/343\n","\n","        API Calls: 229\n","        Total Tokens: 812829\n","        Estimated Cost: $1.63\n","        \n","Processed batch 231/343\n","Processed batch 134/172\n","Processed batch 232/343\n","Processed batch 135/172\n","Processed batch 233/343\n","Processed batch 136/172\n","Processed batch 234/343\n","Processed batch 235/343\n","Processed batch 137/172\n","Processed batch 138/172\n","Processed batch 236/343\n","Processed batch 237/343\n","Processed batch 139/172\n","Processed batch 238/343\n","Processed batch 239/343\n","Processed batch 140/172\n","\n","        API Calls: 141\n","        Total Tokens: 912265\n","        Estimated Cost: $1.82\n","        \n","Processed batch 240/343\n","\n","        API Calls: 238\n","        Total Tokens: 848298\n","        Estimated Cost: $1.70\n","        \n","Processed batch 241/343\n","Processed batch 141/172\n","Processed batch 142/172\n","Processed batch 242/343\n","Processed batch 143/172\n","Processed batch 243/343\n","Processed batch 144/172\n","Processed batch 244/343\n","Processed batch 145/172\n","Processed batch 245/343\n","Processed batch 246/343\n","Processed batch 146/172\n","Processed batch 247/343\n","Processed batch 248/343\n","Processed batch 249/343\n","Processed batch 147/172\n","Processed batch 250/343\n","\n","        API Calls: 248\n","        Total Tokens: 885864\n","        Estimated Cost: $1.77\n","        \n","Processed batch 148/172\n","Processed batch 251/343\n","Processed batch 252/343\n","Processed batch 149/172\n","Processed batch 253/343\n","Processed batch 150/172\n","\n","        API Calls: 151\n","        Total Tokens: 972907\n","        Estimated Cost: $1.95\n","        \n","Processed batch 254/343\n","Processed batch 255/343\n","Processed batch 256/343\n","Processed batch 151/172\n","Processed batch 257/343\n","Processed batch 152/172\n","Processed batch 258/343\n","Processed batch 153/172\n","Processed batch 259/343\n","Processed batch 260/343\n","\n","        API Calls: 258\n","        Total Tokens: 921875\n","        Estimated Cost: $1.84\n","        \n","Processed batch 154/172\n","Processed batch 261/343\n","Processed batch 155/172\n","Processed batch 262/343\n","Processed batch 263/343\n","Processed batch 156/172\n","Processed batch 157/172\n","Processed batch 264/343\n","Processed batch 158/172\n","Processed batch 265/343\n","Processed batch 266/343\n","Processed batch 267/343\n","Processed batch 159/172\n","Processed batch 268/343\n","Processed batch 160/172\n","\n","        API Calls: 161\n","        Total Tokens: 1034691\n","        Estimated Cost: $2.07\n","        \n","Processed batch 269/343\n","Processed batch 270/343\n","\n","        API Calls: 268\n","        Total Tokens: 956962\n","        Estimated Cost: $1.91\n","        \n","Processed batch 271/343\n","Processed batch 161/172\n","Processed batch 272/343\n","Processed batch 273/343\n","Processed batch 162/172\n","Processed batch 274/343\n","Processed batch 275/343\n","Processed batch 163/172\n","Processed batch 276/343\n","Processed batch 164/172\n","Processed batch 165/172\n","Processed batch 277/343\n","Processed batch 278/343\n","Processed batch 279/343\n","Processed batch 166/172\n","Processed batch 167/172\n","Processed batch 280/343\n","\n","        API Calls: 278\n","        Total Tokens: 989053\n","        Estimated Cost: $1.98\n","        \n","Processed batch 281/343\n","Processed batch 168/172\n","Processed batch 169/172\n","Processed batch 282/343\n","Processed batch 283/343\n","Processed batch 170/172\n","\n","        API Calls: 171\n","        Total Tokens: 1094214\n","        Estimated Cost: $2.19\n","        \n","Processed batch 171/172\n","\n","Final Statistics:\n","\n","        API Calls: 172\n","        Total Tokens: 1094704\n","        Estimated Cost: $2.19\n","        \n"]}],"source":["# running for asyncronous function, which is not in use at the moment\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase\"\n","\n","    # Get the OpenAI API key using userdata\n","    from google.colab import userdata\n","    api_key = userdata.get('OPENAI_API_KEY')\n","\n","    if api_key is None:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets.\")\n","\n","    # Use nest_asyncio to integrate with the existing loop\n","    import nest_asyncio\n","    nest_asyncio.apply()\n","\n","    # Get the current event loop\n","    loop = asyncio.get_event_loop()\n","\n","    # Run the process_dataset function using the existing loop\n","    loop.run_until_complete(process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=64  # Adjust batch size as needed\n","    ))"]},{"cell_type":"markdown","metadata":{"id":"7teb07y3BkXj"},"source":["## para-nmt-50m-small.txt"]},{"cell_type":"markdown","metadata":{"id":"znolnMDHBkXk"},"source":["#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypEpd4DGBkXk"},"outputs":[],"source":["# new code for more memory-efficient approach using iterators to save the matching pairs\n","\n","import torch\n","import numpy as np\n","from transformers import pipeline\n","\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        total_lines = sum(1 for _ in f)\n","\n","    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))\n","    selected_texts = []\n","\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        for i, line in enumerate(f):\n","            if i in selected_indices:\n","                selected_texts.append(line.strip())\n","                if len(selected_texts) == batch_size:\n","                    try:\n","                        translations = translate_batch(selected_texts, translator)\n","                        save_pairs(selected_texts, translations, dataset_name)\n","                        print(f\"Processed batch: {len(selected_texts)} lines\")\n","                        selected_texts = []\n","                        if torch.cuda.is_available():\n","                            torch.cuda.empty_cache()\n","                    except Exception as e:\n","                        print(f\"Error processing batch: {str(e)}\")\n","                        continue\n","\n","    if selected_texts:\n","        try:\n","            translations = translate_batch(selected_texts, translator)\n","            save_pairs(selected_texts, translations)\n","            print(f\"Processed remaining {len(selected_texts)} lines\")\n","        except Exception as e:\n","            print(f\"Error processing final batch: {str(e)}\")\n","\n","def save_pairs(originals, translations, dataset_name):\n","    with open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt\", \"a\", encoding=\"utf-8\") as f1, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt\", \"a\", encoding=\"utf-8\") as f2, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt\", \"a\", encoding=\"utf-8\") as f3:\n","        for orig, trans in zip(originals, translations):\n","            f1.write(orig + \"\\n\")\n","            f2.write(trans + \"\\n\")\n","            f3.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")"]},{"cell_type":"markdown","metadata":{"id":"fLQIEANsBkXl"},"source":["#### Translating para-nmt-50m-small dataset - executing for 59m 40s"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["30b77ea0da62446582596ae92d972d50","5e0c6bc44aba4ec9bae1f53af8066c48","e4c09fd264e84828aea20aff6014f9e4","b392e5e08a16406d9858aef7a8e1ca2d","66cb186581554c4ca021d9f7cf0895ab","b420d55a600a426699b86d852d6010ec","3880607120a54e55b03853d13da7cbbf","7ef590e67f8a48ad8405031da8bbc0e5","c5fa31106ac443eba526ac05ac9bbde2","0baf4216ec334550905c01076386eb74","6127a90a81d24211b53cee45b25d08d4","871ef377f0bb475ea5942f397fcf171f","3a70636325ac46c8aef83adb8d2ab9c6","ca9c3d4b81184ca3b676dba06445ffc9","c3ba586a868040bcb34c718d69c6b6d5","69a978aae40740dd930fc2e5c100870f","04155872a82e474da6e61cd55e8a4e8b","5382b29abdff414d9aafd9f1394e1437","1e5d98847309441bb7246a82b5adfa1b","5cf99a06606d452192b3c3e2d27afcc7","34b94a0f80b04cd69a57bc6860a1cca9","9dca3607e9104e0184cd4b4c3223fb33","233f8b8b0676425ca4c182fe75e43cb1","b11248ee8d864146b64382943612fb56","7431bd15c0e64d049b5184b7f28e4676","201b094b6cf34494ba0c7c22771ce001","3661b742dc0e4c9b871fc73102600bb2","0e33e97a5c194ab4a42e610e5187c0e7","b5faa536110643b8b6b898cd1375957a","49b55e097580407b86e35bfc27cc3aeb","47e7d007e2a14ddca24f3b98045af790","965d8a94bf564bc5880f3de8cb49a83c","c0f9afeea14a42ba88f7abee5fa7b4d8","9a995d0b11a84827a2066aa33243a57d","990eae43d46843d088a4ad7413004aa1","674af803faeb4d6fbb62c0c03ed37359","1b18fd49deef40609ea0f4b25ab8cebd","afcbf7a7e6e949d3b01435049cc40e33","129cb1f20217474ebe050340e66272be","fe57d589a51c4c0ea5f9d3b5f887b843","50f0edd9e11945ba98a6c6c89971784e","ca4617ae273a417ebaa3f63177eb197a","cd3c42b44caa49409e36c7290e3e98bd","8f98d24a9e374ec198f2d38db9fb7818","050a59b327314eca8bc3a4a64e675a5c","69facca08ed1490eab1d1336ecde8371","15b92aea3001497eb2d3a89d050da56c","10b10cb472ab4d058ccecf25be2f3249","3d4de7a807d64585818e9627f43d962b","558d582e6979484184888b4c078f255e","368737d8471943a2a7b9c0aeea0054fc","3dbd8142b3e44a8a9496c16e44445846","e4d9b41232c542fd982f1e01e4568834","8441616b5a4d4ddca5ce72c254507399","0d6db61ca20947bca5b9164772410ffb","3d1864f015e246089758b3e446e44693","826f15c454af4ce297eb1a3e38ca53d5","69d241da292544338b1a385ce4b4830a","08481755c5aa4fe9a0d7b0336fe43c1e","0cccef2924f24d2a8609505dacc46251","6de7207c43b741629034db29f621ea02","2167113a4f6d44b59c17bdd3393f95af","7bf12fd3c37f44b3976dcec840305b3a","ec58c4ea7f884a75bb568845df8e64c1","918ca344b76d4c75accfd6857129427b","afce05e70f4f4324829f79fcfee94845","aa6a974cf1c64a829deb2c72c9dc2cfa","6e16f7363541446ea186319dfaf7e8ef","2e8321d2816a49dcbc84bf0ba4b0dcc5","c7ede456e99b43cbad4fd001ff4365d7","f707f917ee614bb5980e9a0490afe361","b5698aee625143ef8cd05bc78fad04d6","c51f0161c0bd477c8a130fa316340d03","4364a5000f174c8cbd53cca2304ff3e3","00aee409da1644f8b4115eaa4ce69c52","b23882c4a5f74d74bb0f09ec6ecedfbd","779fd5065dab4535b84e5cee501f8a8e","a41c3fefa9e646dcaf8af7ee6c65cbab","553b8beaa2ac450c88f63414ac2a82f0","51bb252a86094c79a65f6d68f1c57756","9615cf5194e04724b52ee4d3fdb2a0df","bc3de210a4034fc78a6ebfe4bbe1537d","95902a780de041fe98cff1feef6911b8","5f9dc424daa14ed3a174d3fa90e8714b","e1804d7b8a3942f78e39cf02641e83fc","0f385d231c3c4c5097c88c66c09c933d","e2c894d5627f44dca6d24a36b5be0d5b","57bb6d6778874920adf26e9723850798","2bc55f899f294ddb9c878e4738efa39d","f0ef9a1f7c4e444fb5c3576295227afa","d65ead1372dd496c93a0ec62c167e3a8","f81362adf0af4ef89fcb4d55dd1f682d","0a708ec96a5144eda4731bce6402bf98","38bbdd38bfe34dcebd6545b558d853b2","2ad5b79159814d239f3d3636c18faa6a","60cbc5726d9d4195bebf8cf8f290cc66","c22feb7e6f2d4f3884c000c48f2e5d95","7789f9ed1318495a98992959bddc6d11","f329a51cc4154f60ac01cfc413ce1799","78e732a6e3ec45918db0194c38585d4e","0d4bcbb1f6664ea3b20e7d8f3e6a02d6","e4117737d72541cea1dc95f2995d1384","eb15ed2ffb604c85a6709bf01c859744","71455e5e0a0e473da374d6a77e455160","a829daf79eb64a3c9598ace0ca3fedd5","68a0e824dfaf45399a895e71a6e967b7","a49197f3ac3b4eb2a063bd78ae2a7216","0ee106af88c84f78bcc230164d33147a","667da5923ab5405a9568c41801fa3a0a","40ccd98af088495684f125adc26fcfcc"]},"outputId":"a9403ea3-d19d-4186-ad70-b4cd3ebdab55","id":"bkV0FpdFBkXl"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30b77ea0da62446582596ae92d972d50","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"871ef377f0bb475ea5942f397fcf171f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/33.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"233f8b8b0676425ca4c182fe75e43cb1","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a995d0b11a84827a2066aa33243a57d","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"050a59b327314eca8bc3a4a64e675a5c","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d1864f015e246089758b3e446e44693","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa6a974cf1c64a829deb2c72c9dc2cfa","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a41c3fefa9e646dcaf8af7ee6c65cbab","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bc55f899f294ddb9c878e4738efa39d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.19M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78e732a6e3ec45918db0194c38585d4e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"name":"stdout","output_type":"stream","text":["Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","translator = load_model()\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/para-nmt-50m-small.txt\"\n","select_and_translate(input_file_path, translator, \"para-nmt-50m\")"]},{"cell_type":"markdown","source":["#### Indexing the aligned pairs - additionally done ofr para-nmt-50m"],"metadata":{"id":"Ik4aBvEeBkXp"}},{"cell_type":"code","source":["def add_indices_to_pairs(aligned_pairs_path):\n","    # Read the entire file\n","    with open(aligned_pairs_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","\n","    # Split the content into pairs based on the separator\n","    pairs = content.split('---\\n')\n","\n","    # Process each pair and add indices\n","    indexed_content = ''\n","    for idx, pair in enumerate(pairs, 1):\n","        if not pair.strip():  # Skip empty pairs\n","            continue\n","\n","        # Add index to the Original line\n","        if 'Original:' in pair:\n","            pair = pair.replace('Original:', f'Original [{idx}]:')\n","\n","        # Add the separator back except for the last pair\n","        indexed_content += pair + ('---\\n' if idx < len(pairs) else '')\n","\n","    # Write back to file\n","    with open(aligned_pairs_path + '.indexed', 'w', encoding='utf-8') as file:\n","        file.write(indexed_content)\n","\n","    print(f\"Added indices to {len(pairs)} pairs\")\n","    return aligned_pairs_path + '.indexed'\n"],"metadata":{"id":"huH2D5NhBkXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Usage example:\n","file_path = \"/content/drive/My Drive/Colab Notebooks/para-nmt-50m_aligned_pairs.txt\"\n","indexed_file = add_indices_to_pairs(file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736940890897,"user_tz":-60,"elapsed":813,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"edefa63a-da1e-4d6f-8eee-5bd2f667ad79","id":"Xx0cU9D0BkXp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Added indices to 5761 pairs\n"]}]},{"cell_type":"markdown","metadata":{"id":"iZetRXUWBkXq"},"source":["## ppdb-1.0-s-m2o"]},{"cell_type":"markdown","metadata":{"id":"GyPLNCoTBkXq"},"source":["#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"10yR_RzYBkXq"},"outputs":[],"source":["# new code for more memory-efficient approach using iterators to save the matching pairs\n","\n","import torch\n","import numpy as np\n","from transformers import pipeline\n","\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        total_lines = sum(1 for _ in f)\n","\n","    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))\n","    selected_texts = []\n","\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        for i, line in enumerate(f):\n","            if i in selected_indices:\n","                selected_texts.append(line.strip())\n","                if len(selected_texts) == batch_size:\n","                    try:\n","                        translations = translate_batch(selected_texts, translator)\n","                        save_pairs(selected_texts, translations, dataset_name)\n","                        print(f\"Processed batch: {len(selected_texts)} lines\")\n","                        selected_texts = []\n","                        if torch.cuda.is_available():\n","                            torch.cuda.empty_cache()\n","                    except Exception as e:\n","                        print(f\"Error processing batch: {str(e)}\")\n","                        continue\n","\n","    if selected_texts:\n","        try:\n","            translations = translate_batch(selected_texts, translator)\n","            save_pairs(selected_texts, translations)\n","            print(f\"Processed remaining {len(selected_texts)} lines\")\n","        except Exception as e:\n","            print(f\"Error processing final batch: {str(e)}\")\n","\n","def save_pairs(originals, translations, dataset_name):\n","    with open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt\", \"a\", encoding=\"utf-8\") as f1, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt\", \"a\", encoding=\"utf-8\") as f2, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt\", \"a\", encoding=\"utf-8\") as f3:\n","        for orig, trans in zip(originals, translations):\n","            f1.write(orig + \"\\n\")\n","            f2.write(trans + \"\\n\")\n","            f3.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")"]},{"cell_type":"markdown","metadata":{"id":"nojn-RHRBkXq"},"source":["#### Translating ppdb-1.0-s-m2o - executing for 1h 9m 30s\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["65fdf469237c4d1dac27ced4f97d8fbd","bf61dc1c1c7b4313b08d267be6ce5735","fdd8e83aefcb4562b4ff06718af51a74","3ce1cacf5743438594898031430b7ebe","48b0662b7a284805ba9d7b32e24c1162","af0174f466764ddf8ff6d7372bb21e75","e72ff8e2b0574b278d5bbf2dc4f99ad3","1ef9b8babdcb445cbe55a3791b0df567","99589405bd8e4aa0993cc5123ecbc82e","2c0f19d5de004a7e9a066ba8c3dd16f3","b20c2ff67d6b4abba64f7003d7097ee9","29204805bfcd4f1f83194ffcd7b795e4","717743b938364a99bf9d063dc4a8a6af","f0921afc046e44b9a3c0b0e32a725002","fa7a0e1e83514f67ac84c63376f8b7fc","410d4f7193dd4c4fac317ccb4df4d35b","8eb288dc3478400591c159d2c0de0085","a41a5b561ca54b009bab418d3e09e39e","d402ea2d21154eeeac6d136cfe4078f5","5d70f902f149453e8dde08aa3313fa86","8e06e7caac094ecb8a18b4b77e943029","efd5c8bb86f74eb8b7f8af0e2f6731b9","4b93a48b06da4580bf6491991e9f4837","a394cd3219424e31ad7c3612ee3bbc47","7f03321a957e4f049f74cf4503eea67c","19a1fb1d8048444abcd2d05634ccf91b","63e127f3e68c4bdda6c7974a75dc0353","b66312e71d7e4f759ea8dab7c0f04bc4","1c3d82bb366b45799250da346ea6c5a9","49ed1d01ae54480298997998c7f98e6b","3bb19962b27546019acff5c6d7dd0832","cc813a18ddf049c1978c17443bc737e0","6753100d510b43fe88eef396ede1d5df","853179b77fca4cbca16fe279bda47de2","cdcf26b96e7346319fcc94b372ffca3f","438b15af113f4e489201016804f92cb1","74d8ba948d004cb1a2e3793e716e9cce","3d1f327ca7a046869413e82b6be3fa66","7859854cfce849019102c9356c992050","62fa9ce5392b4b37bb0b3ca1c0ebaa11","1e2212d72a0a4573b6894af4369d1fa6","5fd8ccbe4d7e41a495d45056602e4185","7205a6c1aab0471baa35fd0a3f5eaed5","e8da66fffd95472d827e08855613af88","cc3158e553bd4450947e3542aa284369","3bb4cbb492954f93a065f2e94974eaac","78fb10c62f3a451baebdd352433308b9","847e174c64ab4762bc3a7c9e3db17813","1d54631a21474a94bb25cc1639765e9f","3401cbf89ae34147a74a1787584ef28e","b22fb6b7c862414992112ecf0af5f2de","c6edc715d3064caca68d0815376c3197","2e673f9a80154c5cb419efe2414f8f16","9d4cfa994d2b4b3face6c036e2a5dd4e","78174b415855416685b707442baadf7c","86e28d7632274964869ab844bd06bfd2","c668d17e0f8a42ed807b1f05ca33f598","e0a0c921f53c4e6996029c5074d8f057","b727bf9ffba449108649674aa5594099","b6cdf366681e4c44871c801b0a8f6c18","4a4b86b71263433fa8d698a8d34e18b4","566221a4809c4d91952b6f562dbe1e9e","e3a9fe9dca3a4324aedcbac8da1d02aa","4cc1aaad3583478986ce3decfa79e225","94b54f2fd2ca4575afef8b8b70e126c3","30731f0d648742cda495ee22baf7a2c8","0c27421b1d40471d916d66a045ad92bd","932101456c11476e91373d8e5709faeb","3d391a341a2b4badb4bd917b2ebb2901","63fbb5e100844c6f832cbe1b6a431bf8","9a76f0db41594b2a9e4195028e07d482","094fa15f9def467997d55729a2e25b83","448dd3897aee4066979b07f6321cc9eb","d0026e24989a469a870cf32df76613a6","7921f9efcadb4542b4c206144871d565","a00865e7424e4de6a82f4785d1dbec76","c052098e37ac4e6499a6b5f9e22687d4","b70c6b0c871749b3ba754f79420699af","71a223517492448382e1dae8acc2011d","6553b300e1b24918b6d833ea12d31a7a","1cffc93b4c214b01bf2f129dcfe2fdd4","173dd303ea824964911d5f1725fa30bf","4bcec72854db46f58c1bebfe49e064a7","15f32d63e4824414930bd794fa82c32b","1af95f5ca8ed45be86a01040a26c2663","4ff7459b71cc452fbae78f03d3bc8d8c","696440941e384daca4388b19ecb98d33","1b22c9d76e4c4dd0867dac47c5cdb9cb","194b39f451f941aaa9fa20532aefeb8a","4b6cb5fe9c7f477ab03c20928bbce302","30391b5a6ec0456e927b48baa610bcf2","66bd175e090f4248928264f4dd327c4f","ae4b1cfc0df844a59366d78292135fb5","1984e7cfdde942a1b35d16d8010eafec","ca93f1866d5845fca3879ad5224478d5","95046229db52435a9171e86878a12c3e","d277ce75e44846af87e07fa1ecf504c7","5db4779befcd4aef8023679512eff619","d281c2ec573e488fa13c8d24cd7355fb","88342b3d50aa482294745e208eb1c437","d2996dd7ae7e48388ecb40047b87bc56","d2513f57c0ae4cd996332ac020d63f84","6e2b4b4037844c77a02c04259b7cc313","e13fdd99d74f4ec0b45ee87aeace2c54","0d47991ecd44431db82f2a225ec8b5b0","a39f631a5de8431186e436d752bad63a","a4ae1285ba4444789c99a849cf551914","62e30324b0af45919fa0f669afe1366c","57fbc6a30b464cbdb22cc655f5eeadf3","fa22f66105164698bba1aabde8521f79"]},"outputId":"f99825c9-67d3-4835-909f-c457d853a3b8","id":"hUfCRYIrBkXr"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"65fdf469237c4d1dac27ced4f97d8fbd","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"29204805bfcd4f1f83194ffcd7b795e4","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/33.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4b93a48b06da4580bf6491991e9f4837","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"853179b77fca4cbca16fe279bda47de2","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"cc3158e553bd4450947e3542aa284369","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86e28d7632274964869ab844bd06bfd2","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c27421b1d40471d916d66a045ad92bd","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b70c6b0c871749b3ba754f79420699af","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"194b39f451f941aaa9fa20532aefeb8a","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.19M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"88342b3d50aa482294745e208eb1c437","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"name":"stdout","output_type":"stream","text":["Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","translator = load_model()\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/ppdb-1.0-s-m2o\"\n","select_and_translate(input_file_path, translator, \"ppdb-1.0-s-m2o\")"]},{"cell_type":"markdown","metadata":{"id":"2uD21LepBkXu"},"source":["## paws_train.tsv"]},{"cell_type":"code","source":["import numpy as np\n","import openai\n","import json\n","import asyncio\n","import os\n","from pathlib import Path\n","from typing import Iterator, List, Dict\n","from datetime import datetime\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\"):\n","        self.api_key = api_key\n","        openai.api_key = api_key\n","        self.cache_dir = Path(cache_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    async def translate_batch(self, texts: List[str], batch_id: str) -> List[str]:\n","        cache = self._load_cache(batch_id)\n","        translations = [None] * len(texts)  # Pre-allocate list with None values\n","        uncached_texts = []\n","        uncached_indices = []\n","\n","        # Check cache first\n","        for i, text in enumerate(texts):\n","            if text in cache:\n","                translations[i] = cache[text]\n","            else:\n","                uncached_texts.append(text)\n","                uncached_indices.append(i)\n","\n","        if uncached_texts:\n","            try:\n","                response = await openai.ChatCompletion.acreate(\n","                    model=\"gpt-3.5-turbo\",\n","                    messages=[\n","                        {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian. Maintain the same delimiter (---) between texts:\"},\n","                        {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n","                    ],\n","                    temperature=0.3\n","                )\n","\n","                new_translations = response.choices[0].message['content'].split(\"\\n---\\n\")\n","\n","                # Verify we got the expected number of translations\n","                if len(new_translations) != len(uncached_texts):\n","                    raise ValueError(f\"Expected {len(uncached_texts)} translations, got {len(new_translations)}\")\n","\n","                # Update cache and translations list\n","                for idx, (text, trans) in enumerate(zip(uncached_texts, new_translations)):\n","                    self.batch_cache[text] = trans\n","                    self.current_cache_size += 1\n","                    translations[uncached_indices[idx]] = trans\n","\n","                if self.current_cache_size >= self.max_cache_size:\n","                    self._save_batch_cache(batch_id)\n","\n","            except Exception as e:\n","                print(f\"Error in batch translation: {str(e)}\")\n","                # Fill remaining None values with error messages\n","                for i, trans in enumerate(translations):\n","                    if trans is None:\n","                        translations[i] = f\"ERROR: {str(e)}\"\n","\n","        return translations\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.total_lines = self._count_lines()\n","\n","    def _count_lines(self):\n","        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","            return sum(1 for _ in f)\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","            header = next(f).strip().split('\\t')  # Skip header\n","\n","            for line in f:\n","                columns = line.strip().split('\\t')\n","                if len(columns) >= 3:  # Make sure we have both sentences\n","                    sentence1 = columns[1]  # sentence1 column\n","                    sentence2 = columns[2]  # sentence2 column\n","                    current_batch.extend([sentence1, sentence2])  # Add sentences separately\n","                    if len(current_batch) >= self.batch_size:\n","                        yield current_batch[:self.batch_size]\n","                        current_batch = current_batch[self.batch_size:]\n","        if current_batch:\n","            yield current_batch\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    # Save original texts\n","    with open(base_path / f\"{dataset_name}_originals_GPT3.5.txt\", \"a\", encoding=\"utf-8\") as f1:\n","        for i in range(0, len(originals), 2):\n","            if i + 1 < len(originals):\n","                f1.write(f\"Sentence1: {originals[i]}\\nSentence2: {originals[i+1]}\\n---\\n\")\n","\n","    # Save translations\n","    with open(base_path / f\"{dataset_name}_translations_GPT3.5.txt\", \"a\", encoding=\"utf-8\") as f2:\n","        for trans in translations:\n","            f2.write(f\"{trans}\\n---\\n\")\n","\n","    # Save aligned pairs\n","    with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", \"a\", encoding=\"utf-8\") as f3:\n","        for i in range(0, len(originals), 2):\n","            if i + 1 < len(originals):\n","                f3.write(f\"Original Sentence1: {originals[i]}\\nTranslation1: {translations[i]}\\n\")\n","                f3.write(f\"Original Sentence2: {originals[i+1]}\\nTranslation2: {translations[i+1]}\\n---\\n\")\n","\n","async def process_dataset(input_file: str, dataset_name: str, api_key: str, batch_size: int = 32):\n","    translator = TranslationManager(api_key)\n","    dataset_iter = DatasetIterator(input_file, batch_size)\n","\n","    batch_id = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n","\n","    for batch_num, batch in enumerate(dataset_iter):\n","        print(f\"Processing batch {batch_num + 1}...\")\n","\n","        translations = await translator.translate_batch(batch, f\"{batch_id}_{batch_num}\")\n","\n","        # Save after each batch to prevent data loss\n","        save_pairs(batch, translations, dataset_name)\n","\n","        print(f\"Completed batch {batch_num + 1}\")\n"],"metadata":{"id":"qAZTF8It6B81"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","    dataset_name = \"msr_paraphrase\"\n","\n","    # Get the OpenAI API key using userdata\n","    from google.colab import userdata\n","    api_key = userdata.get('OPENAI_API_KEY')\n","\n","    if api_key is None:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets.\")\n","\n","    # Use nest_asyncio to integrate with the existing loop\n","    import nest_asyncio\n","    nest_asyncio.apply()\n","\n","    # Get the current event loop\n","    loop = asyncio.get_event_loop()\n","\n","    # Run the process_dataset function using the existing loop\n","    loop.run_until_complete(process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=32\n","    ))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"Pvsa3wqq6LwP","executionInfo":{"status":"error","timestamp":1738879005691,"user_tz":-60,"elapsed":1302395,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"4261b532-1bcd-4f47-b20e-d3d7f5189fea"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Processing batch 1...\n","Completed batch 1\n","Processing batch 2...\n","Error in batch translation: Expected 32 translations, got 31\n","Completed batch 2\n","Processing batch 3...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 3\n","Processing batch 4...\n","Completed batch 4\n","Processing batch 5...\n","Completed batch 5\n","Processing batch 6...\n","Completed batch 6\n","Processing batch 7...\n","Completed batch 7\n","Processing batch 8...\n","Completed batch 8\n","Processing batch 9...\n","Completed batch 9\n","Processing batch 10...\n","Completed batch 10\n","Processing batch 11...\n","Error in batch translation: Expected 32 translations, got 1\n","Completed batch 11\n","Processing batch 12...\n","Completed batch 12\n","Processing batch 13...\n","Completed batch 13\n","Processing batch 14...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 14\n","Processing batch 15...\n","Completed batch 15\n","Processing batch 16...\n","Completed batch 16\n","Processing batch 17...\n","Completed batch 17\n","Processing batch 18...\n","Completed batch 18\n","Processing batch 19...\n","Completed batch 19\n","Processing batch 20...\n","Completed batch 20\n","Processing batch 21...\n","Completed batch 21\n","Processing batch 22...\n","Completed batch 22\n","Processing batch 23...\n","Completed batch 23\n","Processing batch 24...\n","Completed batch 24\n","Processing batch 25...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 25\n","Processing batch 26...\n","Completed batch 26\n","Processing batch 27...\n","Completed batch 27\n","Processing batch 28...\n","Completed batch 28\n","Processing batch 29...\n","Completed batch 29\n","Processing batch 30...\n","Completed batch 30\n","Processing batch 31...\n","Completed batch 31\n","Processing batch 32...\n","Completed batch 32\n","Processing batch 33...\n","Completed batch 33\n","Processing batch 34...\n","Completed batch 34\n","Processing batch 35...\n","Completed batch 35\n","Processing batch 36...\n","Completed batch 36\n","Processing batch 37...\n","Completed batch 37\n","Processing batch 38...\n","Completed batch 38\n","Processing batch 39...\n","Completed batch 39\n","Processing batch 40...\n","Completed batch 40\n","Processing batch 41...\n","Completed batch 41\n","Processing batch 42...\n","Completed batch 42\n","Processing batch 43...\n","Completed batch 43\n","Processing batch 44...\n","Completed batch 44\n","Processing batch 45...\n","Error in batch translation: Expected 32 translations, got 25\n","Completed batch 45\n","Processing batch 46...\n","Completed batch 46\n","Processing batch 47...\n","Completed batch 47\n","Processing batch 48...\n","Completed batch 48\n","Processing batch 49...\n","Completed batch 49\n","Processing batch 50...\n","Completed batch 50\n","Processing batch 51...\n","Completed batch 51\n","Processing batch 52...\n","Completed batch 52\n","Processing batch 53...\n","Completed batch 53\n","Processing batch 54...\n","Completed batch 54\n","Processing batch 55...\n","Completed batch 55\n","Processing batch 56...\n","Completed batch 56\n","Processing batch 57...\n","Completed batch 57\n","Processing batch 58...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 58\n","Processing batch 59...\n","Completed batch 59\n","Processing batch 60...\n","Completed batch 60\n","Processing batch 61...\n","Completed batch 61\n","Processing batch 62...\n","Completed batch 62\n","Processing batch 63...\n","Error in batch translation: Expected 32 translations, got 31\n","Completed batch 63\n","Processing batch 64...\n","Completed batch 64\n","Processing batch 65...\n","Completed batch 65\n","Processing batch 66...\n","Completed batch 66\n","Processing batch 67...\n","Completed batch 67\n","Processing batch 68...\n","Completed batch 68\n","Processing batch 69...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 69\n","Processing batch 70...\n","Error in batch translation: Expected 32 translations, got 26\n","Completed batch 70\n","Processing batch 71...\n","Completed batch 71\n","Processing batch 72...\n","Completed batch 72\n","Processing batch 73...\n","Completed batch 73\n","Processing batch 74...\n","Completed batch 74\n","Processing batch 75...\n","Completed batch 75\n","Processing batch 76...\n","Completed batch 76\n","Processing batch 77...\n","Completed batch 77\n","Processing batch 78...\n","Error in batch translation: Expected 32 translations, got 26\n","Completed batch 78\n","Processing batch 79...\n","Error in batch translation: Expected 32 translations, got 31\n","Completed batch 79\n","Processing batch 80...\n","Completed batch 80\n","Processing batch 81...\n","Error in batch translation: Expected 32 translations, got 31\n","Completed batch 81\n","Processing batch 82...\n","Completed batch 82\n","Processing batch 83...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 83\n","Processing batch 84...\n","Completed batch 84\n","Processing batch 85...\n","Completed batch 85\n","Processing batch 86...\n","Completed batch 86\n","Processing batch 87...\n","Completed batch 87\n","Processing batch 88...\n","Completed batch 88\n","Processing batch 89...\n","Completed batch 89\n","Processing batch 90...\n","Completed batch 90\n","Processing batch 91...\n","Completed batch 91\n","Processing batch 92...\n","Completed batch 92\n","Processing batch 93...\n","Completed batch 93\n","Processing batch 94...\n","Completed batch 94\n","Processing batch 95...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 95\n","Processing batch 96...\n","Completed batch 96\n","Processing batch 97...\n","Completed batch 97\n","Processing batch 98...\n","Error in batch translation: Expected 32 translations, got 31\n","Completed batch 98\n","Processing batch 99...\n","Completed batch 99\n","Processing batch 100...\n","Completed batch 100\n","Processing batch 101...\n","Error in batch translation: Expected 32 translations, got 28\n","Completed batch 101\n","Processing batch 102...\n","Completed batch 102\n","Processing batch 103...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 103\n","Processing batch 104...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 104\n","Processing batch 105...\n","Error in batch translation: Expected 32 translations, got 31\n","Completed batch 105\n","Processing batch 106...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 106\n","Processing batch 107...\n","Completed batch 107\n","Processing batch 108...\n","Error in batch translation: Expected 32 translations, got 16\n","Completed batch 108\n","Processing batch 109...\n","Completed batch 109\n","Processing batch 110...\n","Completed batch 110\n","Processing batch 111...\n","Completed batch 111\n","Processing batch 112...\n","Completed batch 112\n","Processing batch 113...\n","Completed batch 113\n","Processing batch 114...\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-5-6164bae3ef32>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;31m# Run the process_dataset function using the existing loop\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     loop.run_until_complete(process_dataset(\n\u001b[0m\u001b[1;32m     21\u001b[0m         \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36mrun_until_complete\u001b[0;34m(self, future)\u001b[0m\n\u001b[1;32m     90\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_log_destroy_pending\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stopping\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m                     \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/nest_asyncio.py\u001b[0m in \u001b[0;36m_run_once\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    113\u001b[0m                 scheduled[0]._when - self.time(), 0), 86400) if scheduled\n\u001b[1;32m    114\u001b[0m             else None)\n\u001b[0;32m--> 115\u001b[0;31m         \u001b[0mevent_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_process_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/selectors.py\u001b[0m in \u001b[0;36mselect\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    466\u001b[0m             \u001b[0mready\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 468\u001b[0;31m                 \u001b[0mfd_event_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_selector\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpoll\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_ev\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    469\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mInterruptedError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mready\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"faVNT2NCBkXu"},"source":["#### Check the number of lines in the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4976,"status":"ok","timestamp":1734602901137,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"},"user_tz":-60},"outputId":"53d36e7d-e4e4-4048-bf6d-b1ca44406f92","id":"sFJknMiJBkXu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File found!\n","The file has 49402 rows.\n"]}],"source":["# Chech the size of the file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","\n","# Check if file exists\n","import os\n","if os.path.exists(file_path):\n","    print(\"File found!\")\n","else:\n","    print(\"File not found. Check the path!\")\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","\n","# Count lines in the file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n","\n","print(f\"The file has {line_count} rows.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["827270af9be049789f715f568bf33333","39ebd284abe7428aa1da678f4f1311b4","b1028910c80a42c0858883b35e5fb018","86725f3bfc48474bb1cdf15687311c4f","f1a36aa40f2643488effee44205905d3","f44bbe4484e0485bacef6bae4c896738","4e9587f23fb841e583adbec831b0e9b9","58e243e3242f4de6b94d850964e44a66","046cf7e2bba24d76981ac3c8ac5adf63","17ccb71754574c299ccbb536b929bc83","4e23cd19624042b3827737dd8d567937"]},"executionInfo":{"elapsed":111895,"status":"ok","timestamp":1734613731945,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"},"user_tz":-60},"outputId":"6a6eecb0-1091-4f1b-a004-91241420f527","id":"IpQAwpsaBkXv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"827270af9be049789f715f568bf33333","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n"]},{"name":"stdout","output_type":"stream","text":["Model loaded successfully\n","Reading file and selecting samples...\n","Total lines in file: 49402\n","Selected 10000 random lines for translation\n"]},{"name":"stderr","output_type":"stream","text":["Translating: 100%|| 313/313 [2:07:43<00:00, 24.49s/it]"]},{"name":"stdout","output_type":"stream","text":["Translation complete! Translated file saved to: /content/drive/My Drive/Colab Notebooks/translated_paws_train.tsv\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch\n","from transformers import pipeline\n","import random\n","from tqdm import tqdm  # Adding progress bar\n","\n","# Function to load model\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","# Batch translation function\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","# File paths\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","output_file_path = \"/content/drive/My Drive/Colab Notebooks/translated_paws_train.tsv\"\n","\n","# Setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load model\n","translator = load_model()\n","print(\"Model loaded successfully\")\n","\n","# Parameters\n","num_samples = 10000\n","batch_size = 32\n","\n","try:\n","    # First, read all lines and select random samples\n","    print(\"Reading file and selecting samples...\")\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as infile:\n","        all_lines = [line.strip() for line in infile if line.strip()]\n","\n","    # Select random samples\n","    total_lines = len(all_lines)\n","    print(f\"Total lines in file: {total_lines}\")\n","\n","    selected_lines = random.sample(all_lines, num_samples)\n","    print(f\"Selected {num_samples} random lines for translation\")\n","\n","    # Process selected lines in batches\n","    batches = [selected_lines[i:i + batch_size] for i in range(0, len(selected_lines), batch_size)]\n","\n","    # Translate and save\n","    with open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n","        for batch_num, batch in enumerate(tqdm(batches, desc=\"Translating\")):\n","            try:\n","                translated_batch = translate_batch(batch, translator)\n","                outfile.write(\"\\n\".join(translated_batch) + \"\\n\")\n","\n","                # Clear CUDA cache periodically\n","                if torch.cuda.is_available() and batch_num % 10 == 0:\n","                    torch.cuda.empty_cache()\n","\n","            except Exception as e:\n","                print(f\"Error processing batch {batch_num}: {str(e)}\")\n","                continue\n","\n","    print(f\"Translation complete! Translated file saved to: {output_file_path}\")\n","\n","except Exception as e:\n","    print(f\"An error occurred: {str(e)}\")\n","\n","# Optional: Save indices of selected lines for reproducibility\n","with open(output_file_path + \".indices\", \"w\", encoding=\"utf-8\") as f:\n","    for i, line in enumerate(selected_lines):\n","        f.write(f\"{i}\\t{line}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"E3utXiV_BkXv"},"source":[]},{"cell_type":"markdown","metadata":{"id":"fdUr6j_0BkXz"},"source":["# Quora Duplicate Questions"]},{"cell_type":"markdown","metadata":{"id":"xVsiYu9eBkXz"},"source":["#### Checking the size of the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737021941636,"user_tz":-60,"elapsed":5154,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"241791b7-a107-4389-aac8-baa2ca1f5e7e","id":"w1RGOCsPBkXz"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File found!\n","The file has 404302 rows.\n"]}],"source":["# Check the size of the file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n","\n","# Check if file exists\n","import os\n","if os.path.exists(file_path):\n","    print(\"File found!\")\n","else:\n","    print(\"File not found. Check the path!\")\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n","\n","# Count lines in the file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n","\n","print(f\"The file has {line_count} rows.\")\n"]},{"cell_type":"markdown","metadata":{"id":"lxKAjCS6BkX0"},"source":["#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVXaiqFIBkX0"},"outputs":[],"source":["# new code for more memory-efficient approach using iterators to save the matching pairs\n","\n","import torch\n","import numpy as np\n","from transformers import pipeline\n","\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        total_lines = sum(1 for _ in f)\n","\n","    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))\n","    selected_texts = []\n","\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        for i, line in enumerate(f):\n","            if i in selected_indices:\n","                selected_texts.append(line.strip())\n","                if len(selected_texts) == batch_size:\n","                    try:\n","                        translations = translate_batch(selected_texts, translator)\n","                        save_pairs(selected_texts, translations, dataset_name)\n","                        print(f\"Processed batch: {len(selected_texts)} lines\")\n","                        selected_texts = []\n","                        if torch.cuda.is_available():\n","                            torch.cuda.empty_cache()\n","                    except Exception as e:\n","                        print(f\"Error processing batch: {str(e)}\")\n","                        continue\n","\n","    if selected_texts:\n","        try:\n","            translations = translate_batch(selected_texts, translator)\n","            save_pairs(selected_texts, translations)\n","            print(f\"Processed remaining {len(selected_texts)} lines\")\n","        except Exception as e:\n","            print(f\"Error processing final batch: {str(e)}\")\n","\n","def save_pairs(originals, translations, dataset_name):\n","    with open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt\", \"a\", encoding=\"utf-8\") as f1, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt\", \"a\", encoding=\"utf-8\") as f2, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt\", \"a\", encoding=\"utf-8\") as f3:\n","        for orig, trans in zip(originals, translations):\n","            f1.write(orig + \"\\n\")\n","            f2.write(trans + \"\\n\")\n","            f3.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")"]},{"cell_type":"markdown","metadata":{"id":"_av7NrelBkX0"},"source":["#### Translating quora duplicate questions - executing for\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d396a1f97a9b402aa1e9932295ac07ae","17ce541421814b9dba563683f2c5d3f6","6e7807eb3031485b9c5860ac49ce188f","801612be699a415382733c1bc9f9cbbc","9cfbff66c74b4630ac11cc7d9c2672f7","a1d62f8fe66d4ba68cace96c022ef611","c40dad2b3dbd4cef8f72d75b4cac51be","4e9c16911076461fa4a55e155d271b7a","0cbfce6c64ee4beb81c2194243a347d4","c0146e088fe64e86aa64757c310e8ddb","72c92dacce26432c93b16705090b3a82","9c14c296fcd842aeaa4756087672fe17","70f860bd97924bb699f4095afd2f7359","468d60d8d7f44713a4c40f8cc0dea802","ee5ed91618694e0fb50f93405676b715","190a05e36f1a4d138dd723607cd172e5","dd94e079abd74d11a75f1cb37d7771c7","39f6c8ae33264721b3537aa5fd435469","e6a1739732774b7399d7dabb734f44ab","ceaab7d73eb843fcb0fbb4e0d924c813","b8f775941848479b8f6bf06de49ff423","963ac02d34f946a0ad8301db079ae4db","b9dfdd45c6db48ab91782f925ed05891","e0c79d53a9574f81a890c5bfcab2cc97","5923cb6ffa48450b9ff0f559d5ec886e","ec0294adb73a41c3a25ba751cfa2763a","c2285cf091774e5997c71811d59b5429","ed4d6b8342914b6fa165f420cdd535a2","d368061f36d0411a90412019ce14a341","79f9151374514624a80be7f82a61a834","13373ab855584ddcb3531fc04d6b9b7c","1cf7ae8f6ae14c00bb1fd1e7539f0345","0fc2bf50f6ac4c6db34e09098a9c654c","a533c0fb3de7442a88ea5cb0c469b648","1c3ecc6b086741d5b14939c7320d6fc2","eb17751102bb4885957236a1a16953fd","aabc468c224d42da812f29a7a10da868","a13f7b074af341f989d73414513d84b6","38f55f40c7304f60bbac27c1e035e905","0aff301f3b5e485ab80d18c49d0f96a9","a83cdc340d1b4a3e96c2876497f05707","f9c9ce81ab2849339499e9d9d8ba364b","1d27adea7cc941319a5b8b10890b7b80","a9a0a3ea36744328aa74c74ded39edd8","fa31365072da404eac5643961c03852e","99e47288464e4ff591f3fb9c1e8e49d0","49532c48e5534dd794f15d95a669680b","ae7209517112467aba76b688ff4f777b","d2216956e4ea4ef1992adeeb7893353b","0afae8bbb1aa4bf59a9dc5a1ee4afcd8","cd3f12c1b8fa4d4e8bff5ed7ea19a31f","b4a0d96174b245339f176af680e4ed97","48832ff395d049cf9563b7efb44833bc","ee45cf497eda4aee99573e5189db4958","239035ee78dd4f08994a97bebed48ed9","1ba315f2b6a6470aa250368f2ce3347e","7708fde1cd0148539dd8747d3e9e2b00","a149dc0a071f450ca9e7a8edf1714beb","473a646a5f67443e9be38c05a92b4d33","d2ece034a964418abae8e4dcdd54cddc","ddd7a828f3d74dfe953d37971384287f","f361f9154fd34c03ae00dc997dc6ed44","2761bd917d1b4c96b6f98d08fcdc58ae","d41bdd7e965447eaabbe81835aa3d3bd","8e62e4eb20f0412b8f8ab1b407507778","926b7374470b4a31a5202cbdf48c6bbb","0a76a9b3ca154682ba42052e84b5aee2","259171e08aa345d5b45b1c65c72db023","e955496bbda042e7aaf6ada08643dbb2","96d974751c7546558ecc5655e923f5bc","974ad51b88b44ed091b45696a564b5e9","97982ebafb5640bfa8858b066568f37b","0b63519fd2dd45eaa23a67c36ded740b","dfbb3d5045a24818a1b41bde950b2d9c","724465cd5a5a46548a05945e7c5c5980","302e7e442d3a4151b5125784534c97a6","a3983dad77eb4173998f2f8aac3408e4","2a224c5745ac4e3ea30b1fd6c32202b8","799906dc3425492096cccc0f75199324","75024ada41614cbd91461cf6a01605cf","529d7eb8d84846718b387c8915c32c6b","f281cae9a2e740c480746137d1990e39","332f8313fe5e4f54a65f6a0be11f1c1a","ec86805562dc4db89600d41fe43a2293","be119df5ecfd4c6897b7beced51b37ea","0767f2af1a3b4d4f832cf6eada9b549e","f6547ba510f649b482d1c554d3e42ced","5ff36b5bb08c4292a46cc4eb4b3774c3","f28b0c420709475e88b3efefaab477d0","91acfac934f2420694602a19fc027ec8","22c3f365869144f4b6485e5ef3307698","e16127b9de5c4e8087e304d4da81e393","fcb96432a7e24fe6890feaa5ff3a833a","d8da9f51758f4cb5bb8c72b898ecfe77","2fd90a78cc474d8e80f875ee644c210b","11dc513db76840c8a7e5c738763ac5aa","6705b79663304da583c498ae6c43959b","d6bd1b287d114e0c92c15782dd0c148b","da209107cc324c2bbdfdd96475fe99a1","6b187b24016c4f3698c2b390084da669","2a071d1bfb3e44a0a4db08e0324ac5af","482f7c16e5054b44bb08659d64b9e787","1d71bc0ef6d24ff7aa90c5cfb5d5c403","a408e8202d0046fe9505701b0251b4d8","accb502740634c389fa140fcd5c500b7","7c599bc10da7497da6b68f6d70235b04","2551e2b1e3624198a268c396c5895bac","10e4b6b260624b4e8fda7ffbe48570e2","4c220ef243e3434286fe57e4ba30f369","65399957dc654a9988a2a1e72be7fcda"]},"outputId":"677ecba5-386f-4d10-f291-31c8bc0d3659","executionInfo":{"status":"ok","timestamp":1737025258465,"user_tz":-60,"elapsed":876219,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"id":"fcOY6aaCBkX0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d396a1f97a9b402aa1e9932295ac07ae","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c14c296fcd842aeaa4756087672fe17","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/33.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9dfdd45c6db48ab91782f925ed05891","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a533c0fb3de7442a88ea5cb0c469b648","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa31365072da404eac5643961c03852e","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ba315f2b6a6470aa250368f2ce3347e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a76a9b3ca154682ba42052e84b5aee2","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a224c5745ac4e3ea30b1fd6c32202b8","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f28b0c420709475e88b3efefaab477d0","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.19M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b187b24016c4f3698c2b390084da669","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"output_type":"stream","name":"stdout","text":["Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Error processing final batch: save_pairs() missing 1 required positional argument: 'dataset_name'\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","translator = load_model()\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n","select_and_translate(input_file_path, translator, \"quora_duplicate_questions.tsv\")"]}]}