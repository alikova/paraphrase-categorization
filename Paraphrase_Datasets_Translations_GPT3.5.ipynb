{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["8rUN7eCZBkXU","-BPcvwxZBkXY","sX-obQ9KBkXZ","4FhHiV9ewezJ","jCSpfR0I_0zc","BCcetVRwBkXc","X7Li0568BkXc","GOjH7N941z9z","tnJIJ9oNBkXf","7teb07y3BkXj","znolnMDHBkXk","fLQIEANsBkXl","Ik4aBvEeBkXp","2uD21LepBkXu","2F5tExbJ_kNs","faVNT2NCBkXu","fdUr6j_0BkXz","xVsiYu9eBkXz","lxKAjCS6BkX0","_av7NrelBkX0"],"machine_shape":"hm","authorship_tag":"ABX9TyO9d9BaWTXfzsdfl6BRcFsv"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["#### Optimized code for translation (*test_code_Paraphrase Datasets Translations - GaMS.ipynb*) with OpenAI model. This script uses cache patterns and batch processing inside of pipeline programming, enabling parallel processing of translation batches. As a result, the API calls are optimized, and if an error occurs during the translation of a single sentence, the entire batch is not lost. We are using ChatGPT-3.5 via the OpenAI API key."],"metadata":{"id":"B7cJnBTxBa3h"}},{"cell_type":"markdown","source":["## Mount Google Drive"],"metadata":{"id":"j9NLS82cGq5F"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n"],"metadata":{"id":"msAJWILyGttj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739354926467,"user_tz":-60,"elapsed":2052,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"63b9e1eb-5e95-4856-eb32-097d79d87d8e"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","source":["## Git add and commit"],"metadata":{"id":"8rUN7eCZBkXU"}},{"cell_type":"code","source":["!git clone https://alikova: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@github.com/alikova/paraphrase-categorization.git\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291768688,"user_tz":-60,"elapsed":516,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"cce2c32c-283c-472d-82d5-38dd38eb3c63","id":"9s71E83_BkXU"},"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'paraphrase-categorization'...\n","remote: Enumerating objects: 35, done.\u001b[K\n","remote: Counting objects: 100% (35/35), done.\u001b[K\n","remote: Compressing objects: 100% (26/26), done.\u001b[K\n","remote: Total 35 (delta 14), reused 22 (delta 7), pack-reused 0 (from 0)\u001b[K\n","Receiving objects: 100% (35/35), 531.29 KiB | 8.57 MiB/s, done.\n","Resolving deltas: 100% (14/14), done.\n"]}]},{"cell_type":"code","source":["!ls /content\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291769805,"user_tz":-60,"elapsed":224,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"fccd2dd6-da5d-44f5-cbef-49ddb54f044e","id":"tAfkbrc3BkXV"},"execution_count":31,"outputs":[{"output_type":"stream","name":"stdout","text":["'=1.0.0'   drive   paraphrase-categorization   sample_data\n"]}]},{"cell_type":"code","source":["%cd /content/paraphrase-categorization\n","!pwd"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291770987,"user_tz":-60,"elapsed":152,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"130da21a-6f9c-4bcb-d714-3f0682b71814","id":"-tcdAMSyBkXV"},"execution_count":32,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/paraphrase-categorization\n","/content/paraphrase-categorization\n"]}]},{"cell_type":"code","source":["!find /content -name \"Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n","\n","!ls /content/paraphrase-categorization/\n","\n","!ls /content/drive/MyDrive/\n"],"metadata":{"executionInfo":{"status":"ok","timestamp":1739291772901,"user_tz":-60,"elapsed":267,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"id":"uVLQ8AlpBkXW","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e17573d-bfd9-4a7d-8d4c-4b533aa654e9"},"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/paraphrase-categorization/paraphrase-categorization/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n","/content/paraphrase-categorization/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n","/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," checkpoints\n"," kaggle_prevajajanje_zbirk.html\n","'Load and translate Paraphrases with GaMS _ Kaggle.html'\n"," paraphrase-categorization\n"," paraphrase_datasets_translation_GaMS_gpu\n"," Paraphrase_Datasets_Translations_GaMS.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," paraphrase_read-and-translate.py\n"," README.md\n"," translation_cache\n","'Colab Notebooks'   Translated_Datasets\n"]}]},{"cell_type":"code","source":["!find /content/drive/ -name \"Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bLpfCqYYHay-","executionInfo":{"status":"ok","timestamp":1739291774579,"user_tz":-60,"elapsed":157,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"cd25f992-769b-4429-ad69-a570a526c696"},"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"]}]},{"cell_type":"code","source":["!cp \"/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\" /content/paraphrase-categorization/\n","\n","!ls /content/paraphrase-categorization/\n","%cd /content/paraphrase-categorization\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291796404,"user_tz":-60,"elapsed":574,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"51b3cd14-1f3d-48e6-ebdc-73dd77e103ee","id":"HlEcO_pxBkXW"},"execution_count":37,"outputs":[{"output_type":"stream","name":"stdout","text":[" checkpoints\n"," kaggle_prevajajanje_zbirk.html\n","'Load and translate Paraphrases with GaMS _ Kaggle.html'\n"," paraphrase-categorization\n"," paraphrase_datasets_translation_GaMS_gpu\n"," Paraphrase_Datasets_Translations_GaMS.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," paraphrase_read-and-translate.py\n"," README.md\n"," translation_cache\n","/content/paraphrase-categorization\n"]}]},{"cell_type":"code","source":["!ls -a\n","\n","!git add Paraphrase_Datasets_Translations_GPT3.5.ipynb\n","!git status\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291798874,"user_tz":-60,"elapsed":379,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"c3823d76-566f-4329-f726-fd125d37f897","id":"sZYez0CLBkXX"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":[" .\n"," ..\n"," checkpoints\n"," .git\n"," kaggle_prevajajanje_zbirk.html\n","'Load and translate Paraphrases with GaMS _ Kaggle.html'\n"," paraphrase-categorization\n"," paraphrase_datasets_translation_GaMS_gpu\n"," Paraphrase_Datasets_Translations_GaMS.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n"," Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," paraphrase_read-and-translate.py\n"," README.md\n"," translation_cache\n","On branch main\n","Your branch is up to date with 'origin/main'.\n","\n","Changes to be committed:\n","  (use \"git restore --staged <file>...\" to unstage)\n","\t\u001b[32mmodified:   Paraphrase_Datasets_Translations_GPT3.5.ipynb\u001b[m\n","\n","Untracked files:\n","  (use \"git add <file>...\" to include in what will be committed)\n","\t\u001b[31mcheckpoints/\u001b[m\n","\t\u001b[31mparaphrase-categorization/\u001b[m\n","\t\u001b[31mtranslation_cache/\u001b[m\n","\n"]}]},{"cell_type":"code","source":["!git config --global user.email \"z.alenka7@gmail.com\"\n","!git config --global user.name \"alikova\"\n"],"metadata":{"id":"95QXRLcUBkXX","executionInfo":{"status":"ok","timestamp":1739291802829,"user_tz":-60,"elapsed":259,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":39,"outputs":[]},{"cell_type":"code","source":["!git commit -m \"Update Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291805178,"user_tz":-60,"elapsed":210,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"cf2055e4-a87f-43dc-87e0-e5dd06bf08fc","id":"GcR-ZHLjBkXX"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["[main 6d795d5] Update Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"," 1 file changed, 1 insertion(+), 1 deletion(-)\n"," rewrite Paraphrase_Datasets_Translations_GPT3.5.ipynb (93%)\n"]}]},{"cell_type":"code","source":["!git push origin main  # or the appropriate branch name if it's not 'main'\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1739291807959,"user_tz":-60,"elapsed":623,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"b847b9ea-7f79-4ec9-e0c7-b145d7a28e3e","id":"sj0U_6hoBkXY"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Enumerating objects: 5, done.\n","Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n","Delta compression using up to 8 threads\n","Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n","Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 3.34 KiB | 1.11 MiB/s, done.\n","Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n","remote: Resolving deltas:   0% (0/2)\u001b[K\rremote: Resolving deltas:  50% (1/2)\u001b[K\rremote: Resolving deltas: 100% (2/2)\u001b[K\rremote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n","To https://github.com/alikova/paraphrase-categorization.git\n","   282e8c4..6d795d5  main -> main\n"]}]},{"cell_type":"markdown","source":["## Installations"],"metadata":{"id":"-BPcvwxZBkXY"}},{"cell_type":"code","source":["pip uninstall openai"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cftOsVoceHhc","executionInfo":{"status":"ok","timestamp":1739278323309,"user_tz":-60,"elapsed":6983,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"3d166a23-57c1-4b2b-9b30-15fed7ed3e54"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: openai 1.61.1\n","Uninstalling openai-1.61.1:\n","  Would remove:\n","    /usr/local/bin/openai\n","    /usr/local/lib/python3.11/dist-packages/openai-1.61.1.dist-info/*\n","    /usr/local/lib/python3.11/dist-packages/openai/*\n","Proceed (Y/n)? Y\n","  Successfully uninstalled openai-1.61.1\n"]}]},{"cell_type":"code","source":["!pip uninstall -y openai\n","!pip install openai>=1.0.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"umygU4wve8r0","executionInfo":{"status":"ok","timestamp":1739351731412,"user_tz":-60,"elapsed":3720,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"202d4756-e6c7-45cb-c832-525e4fb6c194"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: openai 1.61.1\n","Uninstalling openai-1.61.1:\n","  Successfully uninstalled openai-1.61.1\n"]}]},{"cell_type":"code","source":["#!pip uninstall -y openai\n","!pip install openai>=1.0.0\n","!python -c \"import openai; print(openai.__version__)\"  # Should print 1.x.x"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lqjl4jP7_R7t","executionInfo":{"status":"ok","timestamp":1739278345567,"user_tz":-60,"elapsed":3643,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"426e3c3b-dd3e-4520-a067-60e9af424082"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["1.61.1\n"]}]},{"cell_type":"markdown","metadata":{"id":"sX-obQ9KBkXZ"},"source":["## Connect to OpenAI with an API key"]},{"cell_type":"code","source":["import os\n","import openai"],"metadata":{"id":"rZdz89fScAR7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from google.colab import userdata\n","#userdata.get('OPENAI_API_KEY')\n"],"metadata":{"id":"gaQuw2A9BSvb","executionInfo":{"status":"ok","timestamp":1739278355382,"user_tz":-60,"elapsed":7,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Retrieve the API key\n","api_key = userdata.get(\"OPENAI_API_KEY\")\n","\n","# Verify if the key exists (good practice)\n","if api_key is None:\n","    raise ValueError(\"API key not found in environment variables\")\n","\n","print(\"API key successfully loaded\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9wGD1XIodUgT","executionInfo":{"status":"ok","timestamp":1738916298717,"user_tz":-60,"elapsed":698,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"ddcedc92-07b3-4c44-8f4c-6e4e5cc59af9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["API key successfully loaded\n"]}]},{"cell_type":"code","source":["# This will retrieve the key you configured in Colab's secrets\n","api_key = userdata.get('OPENAI_API_KEY')"],"metadata":{"id":"evqZmL5YhEIk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create an OpenAI client (New API format)\n","from openai import OpenAI  # We'll use synchronous version instead\n","\n","# Instead of AsyncOpenAI, we'll modify our code to use the synchronous version:\n","client = OpenAI(api_key=api_key)"],"metadata":{"id":"o3fsR0bCmXa5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Example OpenAI API request\n","response = client.chat.completions.create(\n","    model=\"gpt-3.5-turbo\",\n","    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}]\n",")\n","\n","print(response.choices[0].message.content)\n","print(response.usage.total_tokens)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yBzGPkvFgr2w","executionInfo":{"status":"ok","timestamp":1738919275914,"user_tz":-60,"elapsed":738,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"fe35311e-4649-4b4f-9b4d-36d39bad57db"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Why was the math book sad?\n","\n","Because it had too many problems.\n","27\n"]}]},{"cell_type":"markdown","metadata":{"id":"9_PrDfecBkXb"},"source":["# Translate with Pipeline Programming"]},{"cell_type":"markdown","source":["#####           Lower Temperature (closer to 0): The model will be more deterministic and predictable, choosing the words with the highest probability. It will tend to produce more repetitive and safe outputs. This is often preferred for tasks where accuracy and consistency are paramount, such as translation or factual question answering. Higher Temperature (closer to 1 or above): The model becomes more creative and unpredictable. It's more likely to sample from less probable words, leading to more diverse and unexpected outputs. This is suitable for tasks where creativity and novelty are desired, such as creative writing or brainstorming.\n"],"metadata":{"id":"lq-c_WA97e72"}},{"cell_type":"markdown","source":["#### Code without asynchronous function - testing the translation on the first n sentences"],"metadata":{"id":"4FhHiV9ewezJ"}},{"cell_type":"code","source":["# Code is working, it fecthes first n sentences\n","\n","from typing import Iterator, List, Dict, Any, Tuple, Optional\n","import numpy as np\n","import json\n","import os\n","from pathlib import Path\n","from datetime import datetime\n","from openai import OpenAI\n","from openai.types.chat import ChatCompletion\n","import time\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n","        self.client = OpenAI(api_key=api_key)\n","        self.cache_dir = Path(cache_dir)\n","        self.checkpoint_dir = Path(checkpoint_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.checkpoint_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def get_checkpoint_file(self, dataset_name: str) -> Path:\n","        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n","        checkpoint_data = {\n","            \"last_batch\": batch_num,\n","            \"translations_count\": translations_count,\n","            \"timestamp\": datetime.now().isoformat()\n","        }\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","\n","    def load_checkpoint(self, dataset_name: str) -> Dict:\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        if checkpoint_file.exists():\n","            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {\"last_batch\": -1, \"translations_count\": 0}\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = []\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if not text:  # Skip empty strings\n","                    translations.append(\"\")\n","                    continue\n","                if text in cache:\n","                    translations.append(cache[text])\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                try:\n","                    response = self.client.chat.completions.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                            {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n","                        ],\n","                        temperature=0.3\n","                    )\n","\n","                    new_translations = [choice.message.content for choice in response.choices]\n","\n","                    # Ensure we have the same number of translations as input texts\n","                    if len(new_translations) != len(uncached_texts):\n","                        new_translations = new_translations[:len(uncached_texts)]\n","                        if len(new_translations) < len(uncached_texts):\n","                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                    for text, trans in zip(uncached_texts, new_translations):\n","                        self.batch_cache[text] = trans\n","                        self.current_cache_size += 1\n","\n","                    for idx, trans in zip(uncached_indices, new_translations):\n","                        translations.insert(idx, trans)\n","\n","                    if self.current_cache_size >= self.max_cache_size:\n","                        self._save_batch_cache(batch_id)\n","\n","                    return translations, response\n","\n","                except Exception as e:\n","                    print(f\"Error in API call: {str(e)}\")\n","                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 100):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.max_sentences = max_sentences\n","        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n","        self.start_line = min(start_line, max(0, self.total_lines - 1))\n","\n","    def _count_lines(self) -> int:\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                return sum(1 for _ in f)\n","        except Exception as e:\n","            print(f\"Error counting lines: {str(e)}\")\n","            return 0\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        processed_lines = 0\n","\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                # Skip to start line\n","                for _ in range(self.start_line):\n","                    next(f, None)\n","\n","                for line in f:\n","                    if processed_lines >= self.max_sentences:\n","                        break\n","\n","                    line = line.strip()\n","                    if line:  # Only add non-empty lines\n","                        current_batch.append(line)\n","                        processed_lines += 1\n","\n","                        if len(current_batch) == self.batch_size:\n","                            yield current_batch\n","                            current_batch = []\n","\n","                    if processed_lines >= self.max_sentences:\n","                        break\n","\n","                if current_batch:  # Don't forget last partial batch\n","                    yield current_batch\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {str(e)}\")\n","            if current_batch:  # Yield any remaining batch on error\n","                yield current_batch\n","\n","class CostTracker:\n","    def __init__(self):\n","        self.requests = 0\n","        self.total_tokens = 0\n","        self.price_per_1k_tokens = 0.002\n","\n","    def update(self, response: ChatCompletion):\n","        self.requests += 1\n","        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n","\n","    def get_cost(self) -> float:\n","        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n","\n","    def report(self) -> str:\n","        return f\"\"\"\n","        API Calls: {self.requests}\n","        Total Tokens: {self.total_tokens}\n","        Estimated Cost: ${self.get_cost():.2f}\n","        \"\"\"\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    for filename, data in [\n","        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n","        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n","    ]:\n","        try:\n","            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n","                for item in data:\n","                    f.write(f\"{item}\\n\")\n","        except Exception as e:\n","            print(f\"Error saving to {filename}: {str(e)}\")\n","\n","    try:\n","        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n","            for orig, trans in zip(originals, translations):\n","                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n","    except Exception as e:\n","        print(f\"Error saving aligned pairs: {str(e)}\")\n","\n","def process_dataset(\n","    input_file: str,\n","    dataset_name: str,\n","    api_key: str,\n","    batch_size: int = 32,\n","    checkpoint_interval: int = 5,\n","    max_sentences: int = 10949 # Parameter to limit number of sentences in the dataset - write manually since this code is working okay\n","):\n","    translator = TranslationManager(api_key=api_key)\n","\n","    # Load checkpoint if exists\n","    checkpoint = translator.load_checkpoint(dataset_name)\n","    start_batch = checkpoint[\"last_batch\"] + 1\n","    translations_count = checkpoint[\"translations_count\"]\n","\n","    # Calculate starting line\n","    start_line = start_batch * batch_size\n","\n","    # Create iterator with sentence limit\n","    dataset_iterator = DatasetIterator(\n","        file_path=input_file,\n","        batch_size=batch_size,\n","        start_line=start_line,\n","        max_sentences=max_sentences\n","    )\n","\n","    cost_tracker = CostTracker()\n","    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n","\n","    print(f\"Starting from batch {start_batch}, line {start_line}\")\n","    print(f\"Will process up to {max_sentences} sentences\")\n","    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n","\n","    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n","        try:\n","            batch_id = f\"{dataset_name}_{batch_num}\"\n","            translations, response = translator.translate_batch(batch, batch_id)\n","\n","            if response is not None:\n","                cost_tracker.update(response)\n","\n","            save_pairs(batch, translations, dataset_name, mode=\"a\")\n","            translations_count += len(translations)\n","\n","            if batch_num % checkpoint_interval == 0:\n","                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n","                print(f\"Checkpoint saved at batch {batch_num}\")\n","\n","            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n","            if batch_num % 10 == 0:\n","                print(cost_tracker.report())\n","\n","            # Add a small delay to avoid rate limiting\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Error processing batch {batch_num}: {str(e)}\")\n","            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n","            time.sleep(1)  # Longer delay on error\n","            continue\n","\n","    # Final checkpoint and report\n","    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n","    print(\"\\nFinal Statistics:\")\n","    print(cost_tracker.report())\n","    print(f\"Total translations: {translations_count}\")"],"metadata":{"id":"Pj2w8X4eDJnm","executionInfo":{"status":"ok","timestamp":1739284526888,"user_tz":-60,"elapsed":102,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["# try\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase_test\"  # Using test suffix to distinguish from full runs\n","    max_sentences = 10949  # Parameter to limit number of sentences in the dataset - write manually since this code is working okay\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=32,  # Keeping original batch size\n","        checkpoint_interval=10,  # Frequent checkpoints for testing\n","        max_sentences=10949  # Parameter to limit number of sentences in the dataset - write manually since this code is working okay\n","    )"],"metadata":{"id":"gLFdSOLe2D1F"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["#### Code with asynchronous function - not working"],"metadata":{"id":"jCSpfR0I_0zc"}},{"cell_type":"code","source":["# Throving an error\n","\n","from typing import Iterator, List, Dict, Any, Tuple, Optional\n","from openai import OpenAI\n","import json\n","from pathlib import Path\n","from datetime import datetime\n","import time\n","import os\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n","        #self.api_key = api_key  # Store the API key\n","        self.client = OpenAI(api_key=api_key)  # Initialize client properly\n","        self.cache_dir = Path(cache_dir)\n","        self.checkpoint_dir = Path(checkpoint_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.checkpoint_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = []\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if not text:  # Skip empty strings\n","                    translations.append(\"\")\n","                    continue\n","                if text in cache:\n","                    translations.append(cache[text])\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                try:\n","                    response = self.client.chat.completions.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                            {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n","                        ],\n","                        temperature=0.3\n","                    )\n","\n","                    # Get translations from response\n","                    new_translations = [choice.message.content for choice in response.choices]\n","\n","                    # Ensure we have the same number of translations as input texts\n","                    if len(new_translations) != len(uncached_texts):\n","                        new_translations = new_translations[:len(uncached_texts)]\n","                        if len(new_translations) < len(uncached_texts):\n","                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                    # Update cache\n","                    for text, trans in zip(uncached_texts, new_translations):\n","                        self.batch_cache[text] = trans\n","                        self.current_cache_size += 1\n","\n","                    # Insert translations at correct positions\n","                    for idx, trans in zip(uncached_indices, new_translations):\n","                        translations.insert(idx, trans)\n","\n","                    if self.current_cache_size >= self.max_cache_size:\n","                        self._save_batch_cache(batch_id)\n","\n","                    return translations, response\n","\n","                except Exception as e:\n","                    print(f\"Error in API call: {str(e)}\")\n","                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def get_checkpoint_file(self, dataset_name: str) -> Path:\n","        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n","        checkpoint_data = {\n","            \"last_batch\": batch_num,\n","            \"translations_count\": translations_count,\n","            \"timestamp\": datetime.now().isoformat()\n","        }\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","\n","    def load_checkpoint(self, dataset_name: str) -> Dict:\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        if checkpoint_file.exists():\n","            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {\"last_batch\": -1, \"translations_count\": 0}\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = []\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if not text:  # Skip empty strings\n","                    translations.append(\"\")\n","                    continue\n","                if text in cache:\n","                    translations.append(cache[text])\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                try:\n","                    response = self.client.chat.completions.create(\n","                        model=\"gpt-3.5-turbo\",\n","                        messages=[\n","                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                            {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n","                        ],\n","                        temperature=0.3\n","                    )\n","\n","                    # new_translations = response.choices[0].message.content.split(\"\\n---\\n\") # for version 0.28\n","                    new_translations = [choice.message.content for choice in response.choices]\n","\n","                    # Ensure we have the same number of translations as input texts\n","                    if len(new_translations) != len(uncached_texts):\n","                        new_translations = new_translations[:len(uncached_texts)]\n","                        if len(new_translations) < len(uncached_texts):\n","                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                    for text, trans in zip(uncached_texts, new_translations):\n","                        self.batch_cache[text] = trans\n","                        self.current_cache_size += 1\n","\n","                    for idx, trans in zip(uncached_indices, new_translations):\n","                        translations.insert(idx, trans)\n","\n","                    if self.current_cache_size >= self.max_cache_size:\n","                        self._save_batch_cache(batch_id)\n","\n","                    return translations, response\n","\n","                except Exception as e:\n","                    print(f\"Error in API call: {str(e)}\")\n","                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.text_column_index = text_column_index  # Default to second column (index 1)\n","        self.total_lines = self._count_lines()\n","        # Add 1 to account for header\n","        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n","\n","    def _count_lines(self) -> int:\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                return sum(1 for _ in f)\n","        except Exception as e:\n","            print(f\"Error counting lines: {str(e)}\")\n","            return 0\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        current_line = 0\n","\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                # Skip header\n","                header = next(f, None)\n","                if not header:\n","                    raise ValueError(\"Empty file or no header found\")\n","\n","                # Skip to start line (accounting for already skipped header)\n","                for _ in range(self.start_line - 1):\n","                    next(f, None)\n","                    current_line += 1\n","\n","                for line in f:\n","                    try:\n","                        columns = line.strip().split('\\t')\n","                        if len(columns) > self.text_column_index:\n","                            text = columns[self.text_column_index].strip()\n","                            if text:  # Only add non-empty texts\n","                                current_batch.append(text)\n","                                if len(current_batch) == self.batch_size:\n","                                    yield current_batch\n","                                    current_batch = []\n","                    except Exception as e:\n","                        print(f\"Error processing line: {line.strip()}\")\n","                        print(f\"Error details: {str(e)}\")\n","                        continue\n","\n","                if current_batch:  # Don't forget last partial batch\n","                    yield current_batch\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {str(e)}\")\n","            if current_batch:  # Yield any remaining batch on error\n","                yield current_batch\n","\n","class CostTracker:\n","    def __init__(self):\n","        self.requests = 0\n","        self.total_tokens = 0\n","        self.price_per_1k_tokens = 0.002\n","\n","    def update(self, response: ChatCompletion):\n","        self.requests += 1\n","        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n","\n","    def get_cost(self) -> float:\n","        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n","\n","    def report(self) -> str:\n","        return f\"\"\"\n","        API Calls: {self.requests}\n","        Total Tokens: {self.total_tokens}\n","        Estimated Cost: ${self.get_cost():.2f}\n","        \"\"\"\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    for filename, data in [\n","        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n","        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n","    ]:\n","        try:\n","            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n","                for item in data:\n","                    f.write(f\"{item}\\n\")\n","        except Exception as e:\n","            print(f\"Error saving to {filename}: {str(e)}\")\n","\n","    try:\n","        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n","            for orig, trans in zip(originals, translations):\n","                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n","    except Exception as e:\n","        print(f\"Error saving aligned pairs: {str(e)}\")\n","\n","def process_dataset(\n","    input_file: str,\n","    dataset_name: str,\n","    api_key: str,\n","    batch_size: int = 32,\n","    checkpoint_interval: int = 5\n","):\n","    translator = TranslationManager(api_key=api_key)\n","\n","    # Load checkpoint if exists\n","    checkpoint = translator.load_checkpoint(dataset_name)\n","    start_batch = checkpoint[\"last_batch\"] + 1\n","    translations_count = checkpoint[\"translations_count\"]\n","\n","    # Calculate starting line\n","    start_line = start_batch * batch_size\n","\n","    dataset_iterator = DatasetIterator(input_file, batch_size, start_line)\n","    cost_tracker = CostTracker()\n","\n","    total_batches = max(1, dataset_iterator.total_lines // batch_size)\n","\n","    print(f\"Starting from batch {start_batch}, line {start_line}\")\n","    print(f\"Total lines in file: {dataset_iterator.total_lines}\")\n","\n","    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n","        try:\n","            batch_id = f\"{dataset_name}_{batch_num}\"\n","            translations, response = translator.translate_batch(batch, batch_id)\n","\n","            if response is not None:\n","                cost_tracker.update(response)\n","\n","            # Only save if we got valid translations\n","            if translations:\n","                save_pairs(batch, translations, dataset_name, mode=\"a\")\n","                translations_count += len(translations)\n","\n","            if batch_num % checkpoint_interval == 0:\n","                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n","                print(f\"Checkpoint saved at batch {batch_num}\")\n","\n","            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n","            if batch_num % 10 == 0:\n","                print(cost_tracker.report())\n","\n","            # Add a small delay to avoid rate limiting\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Error processing batch {batch_num}: {str(e)}\")\n","            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n","            time.sleep(5)  # Longer delay on error\n","            continue\n","\n","    # Final checkpoint and report\n","    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n","    print(\"\\nFinal Statistics:\")\n","    print(cost_tracker.report())\n","    print(f\"Total translations: {translations_count}\")"],"metadata":{"id":"k5De-X5UnN-8","executionInfo":{"status":"ok","timestamp":1739284139123,"user_tz":-60,"elapsed":36,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":23,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BCcetVRwBkXc"},"source":["## **msr_paraphrase_data.txt**"]},{"cell_type":"markdown","metadata":{"id":"X7Li0568BkXc"},"source":["#### Checking the size of the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737978986468,"user_tz":-60,"elapsed":4870,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"0e863ca1-6db2-418a-a059-f16b50fe1efc","id":"HVE4UDXYBkXc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File found!\n","The file has 10949 rows.\n"]}],"source":["# Check the size of the file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","\n","# Check if file exists\n","import os\n","if os.path.exists(file_path):\n","    print(\"File found!\")\n","else:\n","    print(\"File not found. Check the path!\")\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","\n","# Count lines in the file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n","\n","print(f\"The file has {line_count} rows.\")\n"]},{"cell_type":"markdown","source":["### MSR translation\n","\n","Final Statistics:\n","\n","        API Calls: 342\n","        Total Tokens: 1248224\n","        Estimated Cost: $2.50\n","        Time: 52min\n","        \n","Total translations: 10927"],"metadata":{"id":"GOjH7N941z9z"}},{"cell_type":"code","source":["# Successful\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase_test\"  # Using test suffix to distinguish from full runs\n","    max_sentences = 10949  # Limit to first 100 sentences\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=32,  # Keeping original batch size\n","        checkpoint_interval=10,  # Frequent checkpoints for testing\n","        max_sentences=10949  # Parameter to limit number of sentences\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_5tHrPhDDvMM","executionInfo":{"status":"ok","timestamp":1739291199654,"user_tz":-60,"elapsed":3176968,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"e8e7c91a-552e-45f7-fa54-792b510426ae"},"execution_count":29,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting from batch 1, line 32\n","Will process up to 10949 sentences\n","Total lines to process: 10949\n","Processed batch 1/342 (32 items)\n","Processed batch 2/342 (32 items)\n","Processed batch 3/342 (32 items)\n","Processed batch 4/342 (32 items)\n","Processed batch 5/342 (32 items)\n","Processed batch 6/342 (32 items)\n","Processed batch 7/342 (32 items)\n","Processed batch 8/342 (32 items)\n","Processed batch 9/342 (32 items)\n","Checkpoint saved at batch 10\n","Processed batch 10/342 (32 items)\n","\n","        API Calls: 10\n","        Total Tokens: 37928\n","        Estimated Cost: $0.08\n","        \n","Processed batch 11/342 (32 items)\n","Processed batch 12/342 (32 items)\n","Processed batch 13/342 (32 items)\n","Processed batch 14/342 (32 items)\n","Processed batch 15/342 (32 items)\n","Processed batch 16/342 (32 items)\n","Processed batch 17/342 (32 items)\n","Processed batch 18/342 (32 items)\n","Processed batch 19/342 (32 items)\n","Checkpoint saved at batch 20\n","Processed batch 20/342 (32 items)\n","\n","        API Calls: 20\n","        Total Tokens: 73322\n","        Estimated Cost: $0.15\n","        \n","Processed batch 21/342 (32 items)\n","Processed batch 22/342 (32 items)\n","Processed batch 23/342 (32 items)\n","Processed batch 24/342 (32 items)\n","Processed batch 25/342 (32 items)\n","Processed batch 26/342 (32 items)\n","Processed batch 27/342 (32 items)\n","Processed batch 28/342 (32 items)\n","Processed batch 29/342 (32 items)\n","Checkpoint saved at batch 30\n","Processed batch 30/342 (32 items)\n","\n","        API Calls: 30\n","        Total Tokens: 109329\n","        Estimated Cost: $0.22\n","        \n","Processed batch 31/342 (32 items)\n","Processed batch 32/342 (32 items)\n","Processed batch 33/342 (32 items)\n","Processed batch 34/342 (32 items)\n","Processed batch 35/342 (32 items)\n","Processed batch 36/342 (32 items)\n","Processed batch 37/342 (32 items)\n","Processed batch 38/342 (32 items)\n","Processed batch 39/342 (32 items)\n","Checkpoint saved at batch 40\n","Processed batch 40/342 (32 items)\n","\n","        API Calls: 40\n","        Total Tokens: 146025\n","        Estimated Cost: $0.29\n","        \n","Processed batch 41/342 (32 items)\n","Processed batch 42/342 (32 items)\n","Processed batch 43/342 (32 items)\n","Processed batch 44/342 (32 items)\n","Processed batch 45/342 (32 items)\n","Processed batch 46/342 (32 items)\n","Processed batch 47/342 (32 items)\n","Processed batch 48/342 (32 items)\n","Processed batch 49/342 (32 items)\n","Checkpoint saved at batch 50\n","Processed batch 50/342 (32 items)\n","\n","        API Calls: 50\n","        Total Tokens: 182819\n","        Estimated Cost: $0.37\n","        \n","Processed batch 51/342 (32 items)\n","Processed batch 52/342 (32 items)\n","Processed batch 53/342 (32 items)\n","Processed batch 54/342 (32 items)\n","Processed batch 55/342 (32 items)\n","Processed batch 56/342 (32 items)\n","Processed batch 57/342 (32 items)\n","Processed batch 58/342 (32 items)\n","Processed batch 59/342 (32 items)\n","Checkpoint saved at batch 60\n","Processed batch 60/342 (32 items)\n","\n","        API Calls: 60\n","        Total Tokens: 217586\n","        Estimated Cost: $0.44\n","        \n","Processed batch 61/342 (32 items)\n","Processed batch 62/342 (32 items)\n","Processed batch 63/342 (32 items)\n","Processed batch 64/342 (32 items)\n","Processed batch 65/342 (32 items)\n","Processed batch 66/342 (32 items)\n","Processed batch 67/342 (32 items)\n","Processed batch 68/342 (32 items)\n","Processed batch 69/342 (32 items)\n","Checkpoint saved at batch 70\n","Processed batch 70/342 (32 items)\n","\n","        API Calls: 70\n","        Total Tokens: 248324\n","        Estimated Cost: $0.50\n","        \n","Processed batch 71/342 (32 items)\n","Processed batch 72/342 (32 items)\n","Processed batch 73/342 (32 items)\n","Processed batch 74/342 (32 items)\n","Processed batch 75/342 (32 items)\n","Processed batch 76/342 (32 items)\n","Processed batch 77/342 (32 items)\n","Processed batch 78/342 (32 items)\n","Processed batch 79/342 (32 items)\n","Checkpoint saved at batch 80\n","Processed batch 80/342 (32 items)\n","\n","        API Calls: 80\n","        Total Tokens: 284445\n","        Estimated Cost: $0.57\n","        \n","Processed batch 81/342 (32 items)\n","Processed batch 82/342 (32 items)\n","Processed batch 83/342 (32 items)\n","Processed batch 84/342 (32 items)\n","Processed batch 85/342 (32 items)\n","Processed batch 86/342 (32 items)\n","Processed batch 87/342 (32 items)\n","Processed batch 88/342 (32 items)\n","Processed batch 89/342 (32 items)\n","Checkpoint saved at batch 90\n","Processed batch 90/342 (32 items)\n","\n","        API Calls: 90\n","        Total Tokens: 320208\n","        Estimated Cost: $0.64\n","        \n","Processed batch 91/342 (32 items)\n","Processed batch 92/342 (32 items)\n","Processed batch 93/342 (32 items)\n","Processed batch 94/342 (32 items)\n","Processed batch 95/342 (32 items)\n","Processed batch 96/342 (32 items)\n","Processed batch 97/342 (32 items)\n","Processed batch 98/342 (32 items)\n","Processed batch 99/342 (32 items)\n","Checkpoint saved at batch 100\n","Processed batch 100/342 (32 items)\n","\n","        API Calls: 100\n","        Total Tokens: 357002\n","        Estimated Cost: $0.71\n","        \n","Processed batch 101/342 (32 items)\n","Processed batch 102/342 (32 items)\n","Processed batch 103/342 (32 items)\n","Processed batch 104/342 (32 items)\n","Processed batch 105/342 (32 items)\n","Processed batch 106/342 (32 items)\n","Processed batch 107/342 (32 items)\n","Processed batch 108/342 (32 items)\n","Processed batch 109/342 (32 items)\n","Checkpoint saved at batch 110\n","Processed batch 110/342 (32 items)\n","\n","        API Calls: 110\n","        Total Tokens: 393340\n","        Estimated Cost: $0.79\n","        \n","Processed batch 111/342 (32 items)\n","Processed batch 112/342 (32 items)\n","Processed batch 113/342 (32 items)\n","Processed batch 114/342 (32 items)\n","Processed batch 115/342 (32 items)\n","Processed batch 116/342 (32 items)\n","Processed batch 117/342 (32 items)\n","Processed batch 118/342 (32 items)\n","Processed batch 119/342 (32 items)\n","Checkpoint saved at batch 120\n","Processed batch 120/342 (32 items)\n","\n","        API Calls: 120\n","        Total Tokens: 430714\n","        Estimated Cost: $0.86\n","        \n","Processed batch 121/342 (32 items)\n","Processed batch 122/342 (32 items)\n","Processed batch 123/342 (32 items)\n","Processed batch 124/342 (32 items)\n","Processed batch 125/342 (32 items)\n","Processed batch 126/342 (32 items)\n","Processed batch 127/342 (32 items)\n","Processed batch 128/342 (32 items)\n","Processed batch 129/342 (32 items)\n","Checkpoint saved at batch 130\n","Processed batch 130/342 (32 items)\n","\n","        API Calls: 130\n","        Total Tokens: 465011\n","        Estimated Cost: $0.93\n","        \n","Processed batch 131/342 (32 items)\n","Processed batch 132/342 (32 items)\n","Processed batch 133/342 (32 items)\n","Processed batch 134/342 (32 items)\n","Processed batch 135/342 (32 items)\n","Processed batch 136/342 (32 items)\n","Processed batch 137/342 (32 items)\n","Processed batch 138/342 (32 items)\n","Processed batch 139/342 (32 items)\n","Checkpoint saved at batch 140\n","Processed batch 140/342 (32 items)\n","\n","        API Calls: 140\n","        Total Tokens: 505688\n","        Estimated Cost: $1.01\n","        \n","Processed batch 141/342 (32 items)\n","Processed batch 142/342 (32 items)\n","Processed batch 143/342 (32 items)\n","Processed batch 144/342 (32 items)\n","Processed batch 145/342 (32 items)\n","Processed batch 146/342 (32 items)\n","Processed batch 147/342 (32 items)\n","Processed batch 148/342 (32 items)\n","Processed batch 149/342 (32 items)\n","Checkpoint saved at batch 150\n","Processed batch 150/342 (32 items)\n","\n","        API Calls: 150\n","        Total Tokens: 544197\n","        Estimated Cost: $1.09\n","        \n","Processed batch 151/342 (32 items)\n","Processed batch 152/342 (32 items)\n","Processed batch 153/342 (32 items)\n","Processed batch 154/342 (32 items)\n","Processed batch 155/342 (32 items)\n","Processed batch 156/342 (32 items)\n","Processed batch 157/342 (32 items)\n","Processed batch 158/342 (32 items)\n","Processed batch 159/342 (32 items)\n","Checkpoint saved at batch 160\n","Processed batch 160/342 (32 items)\n","\n","        API Calls: 160\n","        Total Tokens: 579162\n","        Estimated Cost: $1.16\n","        \n","Processed batch 161/342 (32 items)\n","Processed batch 162/342 (32 items)\n","Processed batch 163/342 (32 items)\n","Processed batch 164/342 (32 items)\n","Processed batch 165/342 (32 items)\n","Processed batch 166/342 (32 items)\n","Processed batch 167/342 (32 items)\n","Processed batch 168/342 (32 items)\n","Processed batch 169/342 (32 items)\n","Checkpoint saved at batch 170\n","Processed batch 170/342 (32 items)\n","\n","        API Calls: 170\n","        Total Tokens: 616551\n","        Estimated Cost: $1.23\n","        \n","Processed batch 171/342 (32 items)\n","Processed batch 172/342 (32 items)\n","Processed batch 173/342 (32 items)\n","Processed batch 174/342 (32 items)\n","Processed batch 175/342 (32 items)\n","Processed batch 176/342 (32 items)\n","Processed batch 177/342 (32 items)\n","Processed batch 178/342 (32 items)\n","Processed batch 179/342 (32 items)\n","Checkpoint saved at batch 180\n","Processed batch 180/342 (32 items)\n","\n","        API Calls: 180\n","        Total Tokens: 649967\n","        Estimated Cost: $1.30\n","        \n","Processed batch 181/342 (32 items)\n","Processed batch 182/342 (32 items)\n","Processed batch 183/342 (32 items)\n","Processed batch 184/342 (32 items)\n","Processed batch 185/342 (32 items)\n","Processed batch 186/342 (32 items)\n","Processed batch 187/342 (32 items)\n","Processed batch 188/342 (32 items)\n","Processed batch 189/342 (32 items)\n","Checkpoint saved at batch 190\n","Processed batch 190/342 (32 items)\n","\n","        API Calls: 190\n","        Total Tokens: 687474\n","        Estimated Cost: $1.37\n","        \n","Processed batch 191/342 (32 items)\n","Processed batch 192/342 (32 items)\n","Processed batch 193/342 (32 items)\n","Processed batch 194/342 (32 items)\n","Processed batch 195/342 (32 items)\n","Processed batch 196/342 (32 items)\n","Processed batch 197/342 (32 items)\n","Processed batch 198/342 (32 items)\n","Processed batch 199/342 (32 items)\n","Checkpoint saved at batch 200\n","Processed batch 200/342 (32 items)\n","\n","        API Calls: 200\n","        Total Tokens: 726060\n","        Estimated Cost: $1.45\n","        \n","Processed batch 201/342 (32 items)\n","Processed batch 202/342 (32 items)\n","Processed batch 203/342 (32 items)\n","Processed batch 204/342 (32 items)\n","Processed batch 205/342 (32 items)\n","Processed batch 206/342 (32 items)\n","Processed batch 207/342 (32 items)\n","Processed batch 208/342 (32 items)\n","Processed batch 209/342 (32 items)\n","Checkpoint saved at batch 210\n","Processed batch 210/342 (32 items)\n","\n","        API Calls: 210\n","        Total Tokens: 760956\n","        Estimated Cost: $1.52\n","        \n","Processed batch 211/342 (32 items)\n","Processed batch 212/342 (32 items)\n","Processed batch 213/342 (32 items)\n","Processed batch 214/342 (32 items)\n","Processed batch 215/342 (32 items)\n","Processed batch 216/342 (32 items)\n","Processed batch 217/342 (32 items)\n","Processed batch 218/342 (32 items)\n","Processed batch 219/342 (32 items)\n","Checkpoint saved at batch 220\n","Processed batch 220/342 (32 items)\n","\n","        API Calls: 220\n","        Total Tokens: 800130\n","        Estimated Cost: $1.60\n","        \n","Processed batch 221/342 (32 items)\n","Processed batch 222/342 (32 items)\n","Processed batch 223/342 (32 items)\n","Processed batch 224/342 (32 items)\n","Processed batch 225/342 (32 items)\n","Processed batch 226/342 (32 items)\n","Processed batch 227/342 (32 items)\n","Processed batch 228/342 (32 items)\n","Processed batch 229/342 (32 items)\n","Checkpoint saved at batch 230\n","Processed batch 230/342 (32 items)\n","\n","        API Calls: 230\n","        Total Tokens: 836066\n","        Estimated Cost: $1.67\n","        \n","Processed batch 231/342 (32 items)\n","Processed batch 232/342 (32 items)\n","Processed batch 233/342 (32 items)\n","Processed batch 234/342 (32 items)\n","Processed batch 235/342 (32 items)\n","Processed batch 236/342 (32 items)\n","Processed batch 237/342 (32 items)\n","Processed batch 238/342 (32 items)\n","Processed batch 239/342 (32 items)\n","Checkpoint saved at batch 240\n","Processed batch 240/342 (32 items)\n","\n","        API Calls: 240\n","        Total Tokens: 874207\n","        Estimated Cost: $1.75\n","        \n","Processed batch 241/342 (32 items)\n","Processed batch 242/342 (32 items)\n","Processed batch 243/342 (32 items)\n","Processed batch 244/342 (32 items)\n","Processed batch 245/342 (32 items)\n","Processed batch 246/342 (32 items)\n","Processed batch 247/342 (32 items)\n","Processed batch 248/342 (32 items)\n","Processed batch 249/342 (32 items)\n","Checkpoint saved at batch 250\n","Processed batch 250/342 (32 items)\n","\n","        API Calls: 250\n","        Total Tokens: 911818\n","        Estimated Cost: $1.82\n","        \n","Processed batch 251/342 (32 items)\n","Processed batch 252/342 (32 items)\n","Processed batch 253/342 (32 items)\n","Processed batch 254/342 (32 items)\n","Processed batch 255/342 (32 items)\n","Processed batch 256/342 (32 items)\n","Processed batch 257/342 (32 items)\n","Processed batch 258/342 (32 items)\n","Processed batch 259/342 (32 items)\n","Checkpoint saved at batch 260\n","Processed batch 260/342 (32 items)\n","\n","        API Calls: 260\n","        Total Tokens: 949344\n","        Estimated Cost: $1.90\n","        \n","Processed batch 261/342 (32 items)\n","Processed batch 262/342 (32 items)\n","Processed batch 263/342 (32 items)\n","Processed batch 264/342 (32 items)\n","Processed batch 265/342 (32 items)\n","Processed batch 266/342 (32 items)\n","Processed batch 267/342 (32 items)\n","Processed batch 268/342 (32 items)\n","Processed batch 269/342 (32 items)\n","Checkpoint saved at batch 270\n","Processed batch 270/342 (32 items)\n","\n","        API Calls: 270\n","        Total Tokens: 987972\n","        Estimated Cost: $1.98\n","        \n","Processed batch 271/342 (32 items)\n","Processed batch 272/342 (32 items)\n","Processed batch 273/342 (32 items)\n","Processed batch 274/342 (32 items)\n","Processed batch 275/342 (32 items)\n","Processed batch 276/342 (32 items)\n","Processed batch 277/342 (32 items)\n","Processed batch 278/342 (32 items)\n","Processed batch 279/342 (32 items)\n","Checkpoint saved at batch 280\n","Processed batch 280/342 (32 items)\n","\n","        API Calls: 280\n","        Total Tokens: 1024726\n","        Estimated Cost: $2.05\n","        \n","Processed batch 281/342 (32 items)\n","Processed batch 282/342 (32 items)\n","Processed batch 283/342 (32 items)\n","Processed batch 284/342 (32 items)\n","Processed batch 285/342 (32 items)\n","Processed batch 286/342 (32 items)\n","Processed batch 287/342 (32 items)\n","Processed batch 288/342 (32 items)\n","Processed batch 289/342 (32 items)\n","Checkpoint saved at batch 290\n","Processed batch 290/342 (32 items)\n","\n","        API Calls: 290\n","        Total Tokens: 1061975\n","        Estimated Cost: $2.12\n","        \n","Processed batch 291/342 (32 items)\n","Processed batch 292/342 (32 items)\n","Processed batch 293/342 (32 items)\n","Processed batch 294/342 (32 items)\n","Processed batch 295/342 (32 items)\n","Processed batch 296/342 (32 items)\n","Processed batch 297/342 (32 items)\n","Processed batch 298/342 (32 items)\n","Processed batch 299/342 (32 items)\n","Checkpoint saved at batch 300\n","Processed batch 300/342 (32 items)\n","\n","        API Calls: 300\n","        Total Tokens: 1097615\n","        Estimated Cost: $2.20\n","        \n","Processed batch 301/342 (32 items)\n","Processed batch 302/342 (32 items)\n","Processed batch 303/342 (32 items)\n","Processed batch 304/342 (32 items)\n","Processed batch 305/342 (32 items)\n","Processed batch 306/342 (32 items)\n","Processed batch 307/342 (32 items)\n","Processed batch 308/342 (32 items)\n","Processed batch 309/342 (32 items)\n","Checkpoint saved at batch 310\n","Processed batch 310/342 (32 items)\n","\n","        API Calls: 310\n","        Total Tokens: 1133789\n","        Estimated Cost: $2.27\n","        \n","Processed batch 311/342 (32 items)\n","Processed batch 312/342 (32 items)\n","Processed batch 313/342 (32 items)\n","Processed batch 314/342 (32 items)\n","Processed batch 315/342 (32 items)\n","Processed batch 316/342 (32 items)\n","Processed batch 317/342 (32 items)\n","Processed batch 318/342 (32 items)\n","Processed batch 319/342 (32 items)\n","Checkpoint saved at batch 320\n","Processed batch 320/342 (32 items)\n","\n","        API Calls: 320\n","        Total Tokens: 1170206\n","        Estimated Cost: $2.34\n","        \n","Processed batch 321/342 (32 items)\n","Processed batch 322/342 (32 items)\n","Processed batch 323/342 (32 items)\n","Processed batch 324/342 (32 items)\n","Processed batch 325/342 (32 items)\n","Processed batch 326/342 (32 items)\n","Processed batch 327/342 (32 items)\n","Processed batch 328/342 (32 items)\n","Processed batch 329/342 (32 items)\n","Checkpoint saved at batch 330\n","Processed batch 330/342 (32 items)\n","\n","        API Calls: 330\n","        Total Tokens: 1210258\n","        Estimated Cost: $2.42\n","        \n","Processed batch 331/342 (32 items)\n","Processed batch 332/342 (32 items)\n","Processed batch 333/342 (32 items)\n","Processed batch 334/342 (32 items)\n","Processed batch 335/342 (32 items)\n","Processed batch 336/342 (32 items)\n","Processed batch 337/342 (32 items)\n","Processed batch 338/342 (32 items)\n","Processed batch 339/342 (32 items)\n","Checkpoint saved at batch 340\n","Processed batch 340/342 (32 items)\n","\n","        API Calls: 340\n","        Total Tokens: 1243900\n","        Estimated Cost: $2.49\n","        \n","Processed batch 341/342 (32 items)\n","Processed batch 342/342 (5 items)\n","\n","Final Statistics:\n","\n","        API Calls: 342\n","        Total Tokens: 1248224\n","        Estimated Cost: $2.50\n","        \n","Total translations: 10927\n"]}]},{"cell_type":"markdown","metadata":{"id":"tnJIJ9oNBkXf"},"source":["### Few tries of translation of msr paraphrase dataset - Executing the code for 1h\n","\n","Final Statistics:\n","\n","        API Calls: 172\n","        Total Tokens: 1094704\n","        Estimated Cost: $2.19\n"]},{"cell_type":"code","source":["# new, more robust and for non-asynchronous function\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase\"\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=64,\n","        checkpoint_interval=5\n","    )\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GNao8vk4CdtN","outputId":"e25df440-3d7f-476a-ee54-07a3eedf366c","executionInfo":{"status":"ok","timestamp":1739284153125,"user_tz":-60,"elapsed":5353,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":24,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting from batch 171, line 10944\n","Total lines in file: 10949\n","Processed batch 171/171 (4 items)\n","\n","Final Statistics:\n","\n","        API Calls: 1\n","        Total Tokens: 308\n","        Estimated Cost: $0.00\n","        \n","Total translations: 11016\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"wwQOdDo6BkXf","executionInfo":{"status":"ok","timestamp":1738865559047,"user_tz":-60,"elapsed":5157663,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"cfbf12d0-a925-456c-d94f-f62cdeddad9c"},"outputs":[{"output_type":"stream","name":"stdout","text":["Processed batch 5/343\n","Processed batch 6/343\n","Processed batch 0/172\n","\n","        API Calls: 1\n","        Total Tokens: 7487\n","        Estimated Cost: $0.01\n","        \n","Processed batch 7/343\n","Processed batch 1/172\n","Processed batch 2/172\n","Processed batch 8/343\n","Processed batch 9/343\n","Processed batch 3/172\n","Processed batch 10/343\n","\n","        API Calls: 11\n","        Total Tokens: 39969\n","        Estimated Cost: $0.08\n","        \n","Processed batch 11/343\n","Processed batch 4/172\n","Processed batch 12/343\n","Processed batch 5/172\n","Processed batch 6/172\n","Processed batch 13/343\n","Processed batch 14/343\n","Processed batch 7/172\n","Processed batch 15/343\n","Processed batch 16/343\n","Processed batch 8/172\n","Processed batch 9/172\n","Processed batch 10/172\n","\n","        API Calls: 11\n","        Total Tokens: 70814\n","        Estimated Cost: $0.14\n","        \n","Processed batch 11/172\n","Processed batch 17/343\n","Processed batch 18/343\n","Processed batch 19/343\n","Processed batch 12/172\n","Processed batch 20/343\n","\n","        API Calls: 21\n","        Total Tokens: 78213\n","        Estimated Cost: $0.16\n","        \n","Processed batch 21/343\n","Processed batch 13/172\n","Processed batch 22/343\n","Processed batch 14/172\n","Processed batch 23/343\n","Processed batch 24/343\n","Processed batch 25/343\n","Processed batch 15/172\n","Processed batch 16/172\n","Processed batch 26/343\n","Processed batch 27/343\n","Processed batch 17/172\n","Processed batch 28/343\n","Processed batch 18/172\n","Processed batch 19/172\n","Processed batch 20/172\n","\n","        API Calls: 21\n","        Total Tokens: 130578\n","        Estimated Cost: $0.26\n","        \n","Processed batch 29/343\n","Processed batch 30/343\n","\n","        API Calls: 31\n","        Total Tokens: 115868\n","        Estimated Cost: $0.23\n","        \n","Processed batch 31/343\n","Processed batch 21/172\n","Processed batch 32/343\n","Processed batch 33/343\n","Processed batch 22/172\n","Processed batch 34/343\n","Processed batch 35/343\n","Processed batch 36/343\n","Processed batch 23/172\n","Processed batch 37/343\n","Processed batch 38/343\n","Processed batch 24/172\n","Processed batch 39/343\n","Processed batch 25/172\n","Processed batch 40/343\n","\n","        API Calls: 41\n","        Total Tokens: 150629\n","        Estimated Cost: $0.30\n","        \n","Processed batch 41/343\n","Processed batch 26/172\n","Processed batch 42/343\n","Processed batch 43/343\n","Processed batch 44/343\n","Processed batch 27/172\n","Processed batch 45/343\n","Processed batch 28/172\n","Processed batch 46/343\n","Processed batch 29/172\n","Processed batch 47/343\n","Processed batch 30/172\n","\n","        API Calls: 31\n","        Total Tokens: 199562\n","        Estimated Cost: $0.40\n","        \n","Processed batch 31/172\n","Processed batch 48/343\n","Processed batch 49/343\n","Processed batch 50/343\n","\n","        API Calls: 51\n","        Total Tokens: 185868\n","        Estimated Cost: $0.37\n","        \n","Processed batch 32/172\n","Processed batch 33/172\n","Processed batch 51/343\n","Processed batch 34/172\n","Processed batch 52/343\n","Processed batch 53/343\n","Processed batch 35/172\n","Processed batch 54/343\n","Processed batch 36/172\n","Processed batch 55/343\n","Processed batch 56/343\n","Processed batch 57/343\n","Processed batch 58/343\n","Processed batch 37/172\n","Processed batch 59/343\n","Processed batch 60/343\n","\n","        API Calls: 61\n","        Total Tokens: 219779\n","        Estimated Cost: $0.44\n","        \n","Processed batch 38/172\n","Processed batch 61/343\n","Processed batch 62/343\n","Processed batch 63/343\n","Processed batch 64/343\n","Processed batch 39/172\n","Processed batch 65/343\n","Processed batch 66/343\n","Processed batch 40/172\n","\n","        API Calls: 41\n","        Total Tokens: 265480\n","        Estimated Cost: $0.53\n","        \n","Processed batch 67/343\n","Processed batch 41/172\n","Processed batch 68/343\n","Processed batch 69/343\n","Processed batch 70/343\n","\n","        API Calls: 71\n","        Total Tokens: 253708\n","        Estimated Cost: $0.51\n","        \n","Processed batch 42/172\n","Processed batch 71/343\n","Processed batch 72/343\n","Processed batch 43/172\n","Processed batch 73/343\n","Processed batch 74/343\n","Processed batch 44/172\n","Processed batch 75/343\n","Processed batch 45/172\n","Processed batch 76/343\n","Processed batch 77/343\n","Processed batch 78/343\n","Processed batch 46/172\n","Processed batch 79/343\n","Processed batch 47/172\n","Processed batch 80/343\n","\n","        API Calls: 81\n","        Total Tokens: 292033\n","        Estimated Cost: $0.58\n","        \n","Processed batch 81/343\n","Processed batch 48/172\n","Processed batch 82/343\n","Processed batch 83/343\n","Processed batch 84/343\n","Processed batch 49/172\n","Processed batch 50/172\n","\n","        API Calls: 51\n","        Total Tokens: 337627\n","        Estimated Cost: $0.68\n","        \n","Processed batch 85/343\n","Processed batch 86/343\n","Processed batch 51/172\n","Processed batch 87/343\n","Processed batch 52/172\n","Processed batch 88/343\n","Processed batch 89/343\n","Processed batch 90/343\n","\n","        API Calls: 90\n","        Total Tokens: 324480\n","        Estimated Cost: $0.65\n","        \n","Processed batch 53/172\n","Processed batch 91/343\n","Processed batch 92/343\n","Processed batch 54/172\n","Processed batch 55/172\n","Processed batch 93/343\n","Processed batch 94/343\n","Processed batch 56/172\n","Processed batch 95/343\n","Processed batch 96/343\n","Processed batch 97/343\n","Processed batch 98/343\n","Processed batch 57/172\n","Processed batch 99/343\n","Processed batch 100/343\n","\n","        API Calls: 100\n","        Total Tokens: 361588\n","        Estimated Cost: $0.72\n","        \n","Processed batch 58/172\n","Processed batch 101/343\n","Processed batch 102/343\n","Processed batch 59/172\n","Processed batch 103/343\n","Processed batch 104/343\n","Processed batch 60/172\n","\n","        API Calls: 61\n","        Total Tokens: 405808\n","        Estimated Cost: $0.81\n","        \n","Processed batch 61/172\n","Processed batch 62/172\n","Processed batch 105/343\n","Processed batch 106/343\n","Processed batch 107/343\n","Processed batch 108/343\n","Processed batch 63/172\n","Processed batch 109/343\n","Processed batch 110/343\n","\n","        API Calls: 110\n","        Total Tokens: 399138\n","        Estimated Cost: $0.80\n","        \n","Processed batch 64/172\n","Processed batch 111/343\n","Processed batch 112/343\n","Processed batch 65/172\n","Processed batch 66/172\n","Processed batch 113/343\n","Processed batch 114/343\n","Processed batch 67/172\n","Processed batch 115/343\n","Processed batch 116/343\n","Processed batch 68/172\n","Processed batch 117/343\n","Processed batch 118/343\n","Processed batch 119/343\n","Processed batch 69/172\n","Processed batch 70/172\n","\n","        API Calls: 71\n","        Total Tokens: 468025\n","        Estimated Cost: $0.94\n","        \n","Processed batch 120/343\n","\n","        API Calls: 120\n","        Total Tokens: 435532\n","        Estimated Cost: $0.87\n","        \n","Processed batch 71/172\n","Processed batch 121/343\n","Processed batch 122/343\n","Processed batch 72/172\n","Processed batch 123/343\n","Processed batch 124/343\n","Processed batch 125/343\n","Processed batch 73/172\n","Processed batch 126/343\n","Processed batch 127/343\n","Processed batch 128/343\n","Processed batch 74/172\n","Processed batch 129/343\n","Processed batch 130/343\n","\n","        API Calls: 130\n","        Total Tokens: 471201\n","        Estimated Cost: $0.94\n","        \n","Processed batch 75/172\n","Processed batch 131/343\n","Processed batch 76/172\n","Processed batch 132/343\n","Processed batch 77/172\n","Processed batch 133/343\n","Processed batch 134/343\n","Processed batch 78/172\n","Processed batch 79/172\n","Processed batch 80/172\n","\n","        API Calls: 81\n","        Total Tokens: 532175\n","        Estimated Cost: $1.06\n","        \n","Processed batch 135/343\n","Processed batch 136/343\n","Processed batch 137/343\n","Processed batch 81/172\n","Processed batch 138/343\n","Processed batch 139/343\n","Processed batch 140/343\n","\n","        API Calls: 140\n","        Total Tokens: 507152\n","        Estimated Cost: $1.01\n","        \n","Processed batch 82/172\n","Processed batch 141/343\n","Processed batch 142/343\n","Processed batch 143/343\n","Processed batch 144/343\n","Processed batch 145/343\n","Processed batch 83/172\n","Processed batch 84/172\n","Processed batch 85/172\n","Processed batch 146/343\n","Processed batch 86/172\n","Processed batch 147/343\n","Processed batch 148/343\n","Processed batch 87/172\n","Processed batch 149/343\n","Processed batch 150/343\n","\n","        API Calls: 150\n","        Total Tokens: 539586\n","        Estimated Cost: $1.08\n","        \n","Processed batch 151/343\n","Processed batch 88/172\n","Processed batch 152/343\n","Processed batch 89/172\n","Processed batch 153/343\n","Processed batch 154/343\n","Processed batch 155/343\n","Processed batch 156/343\n","Processed batch 90/172\n","\n","        API Calls: 91\n","        Total Tokens: 596889\n","        Estimated Cost: $1.19\n","        \n","Processed batch 91/172\n","Processed batch 157/343\n","Processed batch 92/172\n","Processed batch 158/343\n","Processed batch 93/172\n","Processed batch 159/343\n","Processed batch 160/343\n","\n","        API Calls: 159\n","        Total Tokens: 571100\n","        Estimated Cost: $1.14\n","        \n","Processed batch 94/172\n","Processed batch 161/343\n","Processed batch 162/343\n","Processed batch 95/172\n","Processed batch 163/343\n","Processed batch 96/172\n","Processed batch 164/343\n","Processed batch 165/343\n","Processed batch 97/172\n","Processed batch 166/343\n","Processed batch 98/172\n","Processed batch 167/343\n","Processed batch 168/343\n","Processed batch 99/172\n","Processed batch 169/343\n","Processed batch 170/343\n","\n","        API Calls: 169\n","        Total Tokens: 608469\n","        Estimated Cost: $1.22\n","        \n","Processed batch 100/172\n","\n","        API Calls: 101\n","        Total Tokens: 657743\n","        Estimated Cost: $1.32\n","        \n","Processed batch 171/343\n","Processed batch 172/343\n","Processed batch 173/343\n","Processed batch 101/172\n","Processed batch 174/343\n","Processed batch 175/343\n","Processed batch 102/172\n","Processed batch 176/343\n","Processed batch 177/343\n","Processed batch 103/172\n","Processed batch 178/343\n","Processed batch 179/343\n","Processed batch 180/343\n","\n","        API Calls: 179\n","        Total Tokens: 640217\n","        Estimated Cost: $1.28\n","        \n","Processed batch 104/172\n","Processed batch 181/343\n","Processed batch 182/343\n","Processed batch 183/343\n","Processed batch 105/172\n","Processed batch 106/172\n","Processed batch 184/343\n","Processed batch 185/343\n","Processed batch 107/172\n","Processed batch 108/172\n","Processed batch 109/172\n","Processed batch 186/343\n","Processed batch 187/343\n","Processed batch 110/172\n","\n","        API Calls: 111\n","        Total Tokens: 722939\n","        Estimated Cost: $1.45\n","        \n","Processed batch 188/343\n","Processed batch 111/172\n","Processed batch 112/172\n","Processed batch 113/172\n","Processed batch 189/343\n","Processed batch 190/343\n","\n","        API Calls: 189\n","        Total Tokens: 677766\n","        Estimated Cost: $1.36\n","        \n","Processed batch 114/172\n","Processed batch 115/172\n","Processed batch 191/343\n","Processed batch 192/343\n","Processed batch 116/172\n","Processed batch 193/343\n","Processed batch 194/343\n","Processed batch 117/172\n","Processed batch 195/343\n","Processed batch 196/343\n","Processed batch 118/172\n","Processed batch 197/343\n","Processed batch 119/172\n","Processed batch 198/343\n","Processed batch 199/343\n","Processed batch 120/172\n","\n","        API Calls: 121\n","        Total Tokens: 781706\n","        Estimated Cost: $1.56\n","        \n","Processed batch 200/343\n","\n","        API Calls: 199\n","        Total Tokens: 712923\n","        Estimated Cost: $1.43\n","        \n","Processed batch 201/343\n","Processed batch 202/343\n","Processed batch 121/172\n","Processed batch 122/172\n","Processed batch 203/343\n","Processed batch 204/343\n","Processed batch 123/172\n","Processed batch 205/343\n","Processed batch 206/343\n","Processed batch 207/343\n","Processed batch 208/343\n","Processed batch 124/172\n","Processed batch 209/343\n","Processed batch 210/343\n","\n","        API Calls: 209\n","        Total Tokens: 745741\n","        Estimated Cost: $1.49\n","        \n","Processed batch 211/343\n","Processed batch 125/172\n","Processed batch 126/172\n","Processed batch 212/343\n","Processed batch 213/343\n","Processed batch 127/172\n","Processed batch 214/343\n","Processed batch 215/343\n","Processed batch 216/343\n","Processed batch 217/343\n","Processed batch 128/172\n","Processed batch 218/343\n","Processed batch 129/172\n","Processed batch 219/343\n","Processed batch 220/343\n","\n","        API Calls: 219\n","        Total Tokens: 779082\n","        Estimated Cost: $1.56\n","        \n","Processed batch 221/343\n","Processed batch 222/343\n","Processed batch 130/172\n","\n","        API Calls: 131\n","        Total Tokens: 848475\n","        Estimated Cost: $1.70\n","        \n","Processed batch 223/343\n","Processed batch 224/343\n","Processed batch 131/172\n","Processed batch 225/343\n","Processed batch 226/343\n","Processed batch 227/343\n","Processed batch 132/172\n","Processed batch 228/343\n","Processed batch 133/172\n","Processed batch 229/343\n","Processed batch 230/343\n","\n","        API Calls: 229\n","        Total Tokens: 812829\n","        Estimated Cost: $1.63\n","        \n","Processed batch 231/343\n","Processed batch 134/172\n","Processed batch 232/343\n","Processed batch 135/172\n","Processed batch 233/343\n","Processed batch 136/172\n","Processed batch 234/343\n","Processed batch 235/343\n","Processed batch 137/172\n","Processed batch 138/172\n","Processed batch 236/343\n","Processed batch 237/343\n","Processed batch 139/172\n","Processed batch 238/343\n","Processed batch 239/343\n","Processed batch 140/172\n","\n","        API Calls: 141\n","        Total Tokens: 912265\n","        Estimated Cost: $1.82\n","        \n","Processed batch 240/343\n","\n","        API Calls: 238\n","        Total Tokens: 848298\n","        Estimated Cost: $1.70\n","        \n","Processed batch 241/343\n","Processed batch 141/172\n","Processed batch 142/172\n","Processed batch 242/343\n","Processed batch 143/172\n","Processed batch 243/343\n","Processed batch 144/172\n","Processed batch 244/343\n","Processed batch 145/172\n","Processed batch 245/343\n","Processed batch 246/343\n","Processed batch 146/172\n","Processed batch 247/343\n","Processed batch 248/343\n","Processed batch 249/343\n","Processed batch 147/172\n","Processed batch 250/343\n","\n","        API Calls: 248\n","        Total Tokens: 885864\n","        Estimated Cost: $1.77\n","        \n","Processed batch 148/172\n","Processed batch 251/343\n","Processed batch 252/343\n","Processed batch 149/172\n","Processed batch 253/343\n","Processed batch 150/172\n","\n","        API Calls: 151\n","        Total Tokens: 972907\n","        Estimated Cost: $1.95\n","        \n","Processed batch 254/343\n","Processed batch 255/343\n","Processed batch 256/343\n","Processed batch 151/172\n","Processed batch 257/343\n","Processed batch 152/172\n","Processed batch 258/343\n","Processed batch 153/172\n","Processed batch 259/343\n","Processed batch 260/343\n","\n","        API Calls: 258\n","        Total Tokens: 921875\n","        Estimated Cost: $1.84\n","        \n","Processed batch 154/172\n","Processed batch 261/343\n","Processed batch 155/172\n","Processed batch 262/343\n","Processed batch 263/343\n","Processed batch 156/172\n","Processed batch 157/172\n","Processed batch 264/343\n","Processed batch 158/172\n","Processed batch 265/343\n","Processed batch 266/343\n","Processed batch 267/343\n","Processed batch 159/172\n","Processed batch 268/343\n","Processed batch 160/172\n","\n","        API Calls: 161\n","        Total Tokens: 1034691\n","        Estimated Cost: $2.07\n","        \n","Processed batch 269/343\n","Processed batch 270/343\n","\n","        API Calls: 268\n","        Total Tokens: 956962\n","        Estimated Cost: $1.91\n","        \n","Processed batch 271/343\n","Processed batch 161/172\n","Processed batch 272/343\n","Processed batch 273/343\n","Processed batch 162/172\n","Processed batch 274/343\n","Processed batch 275/343\n","Processed batch 163/172\n","Processed batch 276/343\n","Processed batch 164/172\n","Processed batch 165/172\n","Processed batch 277/343\n","Processed batch 278/343\n","Processed batch 279/343\n","Processed batch 166/172\n","Processed batch 167/172\n","Processed batch 280/343\n","\n","        API Calls: 278\n","        Total Tokens: 989053\n","        Estimated Cost: $1.98\n","        \n","Processed batch 281/343\n","Processed batch 168/172\n","Processed batch 169/172\n","Processed batch 282/343\n","Processed batch 283/343\n","Processed batch 170/172\n","\n","        API Calls: 171\n","        Total Tokens: 1094214\n","        Estimated Cost: $2.19\n","        \n","Processed batch 171/172\n","\n","Final Statistics:\n","\n","        API Calls: 172\n","        Total Tokens: 1094704\n","        Estimated Cost: $2.19\n","        \n"]}],"source":["# running for asyncronous function, which is not in use at the moment\n","\n","if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n","    dataset_name = \"msr_paraphrase\"\n","\n","    # Get the OpenAI API key using userdata\n","    from google.colab import userdata\n","    api_key = userdata.get('OPENAI_API_KEY')\n","\n","    if api_key is None:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets.\")\n","\n","    # Use nest_asyncio to integrate with the existing loop\n","    import nest_asyncio\n","    nest_asyncio.apply()\n","\n","    # Get the current event loop\n","    loop = asyncio.get_event_loop()\n","\n","    # Run the process_dataset function using the existing loop\n","    loop.run_until_complete(process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=64  # Adjust batch size as needed\n","    ))"]},{"cell_type":"markdown","metadata":{"id":"7teb07y3BkXj"},"source":["## **para-nmt-50m-small.txt**"]},{"cell_type":"markdown","metadata":{"id":"znolnMDHBkXk"},"source":["#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ypEpd4DGBkXk"},"outputs":[],"source":["# new code for more memory-efficient approach using iterators to save the matching pairs\n","\n","import torch\n","import numpy as np\n","from transformers import pipeline\n","\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        total_lines = sum(1 for _ in f)\n","\n","    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))\n","    selected_texts = []\n","\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        for i, line in enumerate(f):\n","            if i in selected_indices:\n","                selected_texts.append(line.strip())\n","                if len(selected_texts) == batch_size:\n","                    try:\n","                        translations = translate_batch(selected_texts, translator)\n","                        save_pairs(selected_texts, translations, dataset_name)\n","                        print(f\"Processed batch: {len(selected_texts)} lines\")\n","                        selected_texts = []\n","                        if torch.cuda.is_available():\n","                            torch.cuda.empty_cache()\n","                    except Exception as e:\n","                        print(f\"Error processing batch: {str(e)}\")\n","                        continue\n","\n","    if selected_texts:\n","        try:\n","            translations = translate_batch(selected_texts, translator)\n","            save_pairs(selected_texts, translations)\n","            print(f\"Processed remaining {len(selected_texts)} lines\")\n","        except Exception as e:\n","            print(f\"Error processing final batch: {str(e)}\")\n","\n","def save_pairs(originals, translations, dataset_name):\n","    with open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt\", \"a\", encoding=\"utf-8\") as f1, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt\", \"a\", encoding=\"utf-8\") as f2, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt\", \"a\", encoding=\"utf-8\") as f3:\n","        for orig, trans in zip(originals, translations):\n","            f1.write(orig + \"\\n\")\n","            f2.write(trans + \"\\n\")\n","            f3.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")"]},{"cell_type":"markdown","metadata":{"id":"fLQIEANsBkXl"},"source":["#### Translating para-nmt-50m-small dataset - executing for 59m 40s"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["30b77ea0da62446582596ae92d972d50","5e0c6bc44aba4ec9bae1f53af8066c48","e4c09fd264e84828aea20aff6014f9e4","b392e5e08a16406d9858aef7a8e1ca2d","66cb186581554c4ca021d9f7cf0895ab","b420d55a600a426699b86d852d6010ec","3880607120a54e55b03853d13da7cbbf","7ef590e67f8a48ad8405031da8bbc0e5","c5fa31106ac443eba526ac05ac9bbde2","0baf4216ec334550905c01076386eb74","6127a90a81d24211b53cee45b25d08d4","871ef377f0bb475ea5942f397fcf171f","3a70636325ac46c8aef83adb8d2ab9c6","ca9c3d4b81184ca3b676dba06445ffc9","c3ba586a868040bcb34c718d69c6b6d5","69a978aae40740dd930fc2e5c100870f","04155872a82e474da6e61cd55e8a4e8b","5382b29abdff414d9aafd9f1394e1437","1e5d98847309441bb7246a82b5adfa1b","5cf99a06606d452192b3c3e2d27afcc7","34b94a0f80b04cd69a57bc6860a1cca9","9dca3607e9104e0184cd4b4c3223fb33","233f8b8b0676425ca4c182fe75e43cb1","b11248ee8d864146b64382943612fb56","7431bd15c0e64d049b5184b7f28e4676","201b094b6cf34494ba0c7c22771ce001","3661b742dc0e4c9b871fc73102600bb2","0e33e97a5c194ab4a42e610e5187c0e7","b5faa536110643b8b6b898cd1375957a","49b55e097580407b86e35bfc27cc3aeb","47e7d007e2a14ddca24f3b98045af790","965d8a94bf564bc5880f3de8cb49a83c","c0f9afeea14a42ba88f7abee5fa7b4d8","9a995d0b11a84827a2066aa33243a57d","990eae43d46843d088a4ad7413004aa1","674af803faeb4d6fbb62c0c03ed37359","1b18fd49deef40609ea0f4b25ab8cebd","afcbf7a7e6e949d3b01435049cc40e33","129cb1f20217474ebe050340e66272be","fe57d589a51c4c0ea5f9d3b5f887b843","50f0edd9e11945ba98a6c6c89971784e","ca4617ae273a417ebaa3f63177eb197a","cd3c42b44caa49409e36c7290e3e98bd","8f98d24a9e374ec198f2d38db9fb7818","050a59b327314eca8bc3a4a64e675a5c","69facca08ed1490eab1d1336ecde8371","15b92aea3001497eb2d3a89d050da56c","10b10cb472ab4d058ccecf25be2f3249","3d4de7a807d64585818e9627f43d962b","558d582e6979484184888b4c078f255e","368737d8471943a2a7b9c0aeea0054fc","3dbd8142b3e44a8a9496c16e44445846","e4d9b41232c542fd982f1e01e4568834","8441616b5a4d4ddca5ce72c254507399","0d6db61ca20947bca5b9164772410ffb","3d1864f015e246089758b3e446e44693","826f15c454af4ce297eb1a3e38ca53d5","69d241da292544338b1a385ce4b4830a","08481755c5aa4fe9a0d7b0336fe43c1e","0cccef2924f24d2a8609505dacc46251","6de7207c43b741629034db29f621ea02","2167113a4f6d44b59c17bdd3393f95af","7bf12fd3c37f44b3976dcec840305b3a","ec58c4ea7f884a75bb568845df8e64c1","918ca344b76d4c75accfd6857129427b","afce05e70f4f4324829f79fcfee94845","aa6a974cf1c64a829deb2c72c9dc2cfa","6e16f7363541446ea186319dfaf7e8ef","2e8321d2816a49dcbc84bf0ba4b0dcc5","c7ede456e99b43cbad4fd001ff4365d7","f707f917ee614bb5980e9a0490afe361","b5698aee625143ef8cd05bc78fad04d6","c51f0161c0bd477c8a130fa316340d03","4364a5000f174c8cbd53cca2304ff3e3","00aee409da1644f8b4115eaa4ce69c52","b23882c4a5f74d74bb0f09ec6ecedfbd","779fd5065dab4535b84e5cee501f8a8e","a41c3fefa9e646dcaf8af7ee6c65cbab","553b8beaa2ac450c88f63414ac2a82f0","51bb252a86094c79a65f6d68f1c57756","9615cf5194e04724b52ee4d3fdb2a0df","bc3de210a4034fc78a6ebfe4bbe1537d","95902a780de041fe98cff1feef6911b8","5f9dc424daa14ed3a174d3fa90e8714b","e1804d7b8a3942f78e39cf02641e83fc","0f385d231c3c4c5097c88c66c09c933d","e2c894d5627f44dca6d24a36b5be0d5b","57bb6d6778874920adf26e9723850798","2bc55f899f294ddb9c878e4738efa39d","f0ef9a1f7c4e444fb5c3576295227afa","d65ead1372dd496c93a0ec62c167e3a8","f81362adf0af4ef89fcb4d55dd1f682d","0a708ec96a5144eda4731bce6402bf98","38bbdd38bfe34dcebd6545b558d853b2","2ad5b79159814d239f3d3636c18faa6a","60cbc5726d9d4195bebf8cf8f290cc66","c22feb7e6f2d4f3884c000c48f2e5d95","7789f9ed1318495a98992959bddc6d11","f329a51cc4154f60ac01cfc413ce1799","78e732a6e3ec45918db0194c38585d4e","0d4bcbb1f6664ea3b20e7d8f3e6a02d6","e4117737d72541cea1dc95f2995d1384","eb15ed2ffb604c85a6709bf01c859744","71455e5e0a0e473da374d6a77e455160","a829daf79eb64a3c9598ace0ca3fedd5","68a0e824dfaf45399a895e71a6e967b7","a49197f3ac3b4eb2a063bd78ae2a7216","0ee106af88c84f78bcc230164d33147a","667da5923ab5405a9568c41801fa3a0a","40ccd98af088495684f125adc26fcfcc"]},"outputId":"a9403ea3-d19d-4186-ad70-b4cd3ebdab55","id":"bkV0FpdFBkXl"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"30b77ea0da62446582596ae92d972d50","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"871ef377f0bb475ea5942f397fcf171f","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/33.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"233f8b8b0676425ca4c182fe75e43cb1","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9a995d0b11a84827a2066aa33243a57d","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"050a59b327314eca8bc3a4a64e675a5c","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3d1864f015e246089758b3e446e44693","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"aa6a974cf1c64a829deb2c72c9dc2cfa","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a41c3fefa9e646dcaf8af7ee6c65cbab","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2bc55f899f294ddb9c878e4738efa39d","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.19M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"78e732a6e3ec45918db0194c38585d4e","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"name":"stdout","output_type":"stream","text":["Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","translator = load_model()\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/para-nmt-50m-small.txt\"\n","select_and_translate(input_file_path, translator, \"para-nmt-50m\")"]},{"cell_type":"markdown","source":["#### Indexing the aligned pairs - additionally done ofr para-nmt-50m"],"metadata":{"id":"Ik4aBvEeBkXp"}},{"cell_type":"code","source":["def add_indices_to_pairs(aligned_pairs_path):\n","    # Read the entire file\n","    with open(aligned_pairs_path, 'r', encoding='utf-8') as file:\n","        content = file.read()\n","\n","    # Split the content into pairs based on the separator\n","    pairs = content.split('---\\n')\n","\n","    # Process each pair and add indices\n","    indexed_content = ''\n","    for idx, pair in enumerate(pairs, 1):\n","        if not pair.strip():  # Skip empty pairs\n","            continue\n","\n","        # Add index to the Original line\n","        if 'Original:' in pair:\n","            pair = pair.replace('Original:', f'Original [{idx}]:')\n","\n","        # Add the separator back except for the last pair\n","        indexed_content += pair + ('---\\n' if idx < len(pairs) else '')\n","\n","    # Write back to file\n","    with open(aligned_pairs_path + '.indexed', 'w', encoding='utf-8') as file:\n","        file.write(indexed_content)\n","\n","    print(f\"Added indices to {len(pairs)} pairs\")\n","    return aligned_pairs_path + '.indexed'\n"],"metadata":{"id":"huH2D5NhBkXp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Usage example:\n","file_path = \"/content/drive/My Drive/Colab Notebooks/para-nmt-50m_aligned_pairs.txt\"\n","indexed_file = add_indices_to_pairs(file_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1736940890897,"user_tz":-60,"elapsed":813,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"edefa63a-da1e-4d6f-8eee-5bd2f667ad79","id":"Xx0cU9D0BkXp"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Added indices to 5761 pairs\n"]}]},{"cell_type":"markdown","metadata":{"id":"iZetRXUWBkXq"},"source":["## **ppdb-1.0-s-m2o**"]},{"cell_type":"code","source":["from typing import Iterator, List, Dict, Any, Tuple, Optional\n","import json\n","import os\n","from pathlib import Path\n","from datetime import datetime\n","from openai import OpenAI\n","from openai.types.chat import ChatCompletion\n","import time\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n","        self.client = OpenAI(api_key=api_key)\n","        self.cache_dir = Path(cache_dir)\n","        self.checkpoint_dir = Path(checkpoint_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.checkpoint_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def get_checkpoint_file(self, dataset_name: str) -> Path:\n","        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n","        checkpoint_data = {\n","            \"last_batch\": batch_num,\n","            \"translations_count\": translations_count,\n","            \"timestamp\": datetime.now().isoformat()\n","        }\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","\n","    def load_checkpoint(self, dataset_name: str) -> Dict:\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        if checkpoint_file.exists():\n","            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {\"last_batch\": -1, \"translations_count\": 0}\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = [\"\"] * len(texts)  # Initialize with empty strings\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if text in cache:\n","                    translations[i] = cache[text]\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                retry_attempts = 3\n","                for attempt in range(retry_attempts):\n","                    try:\n","                        response = self.client.chat.completions.create(\n","                            model=\"gpt-3.5-turbo\",\n","                            messages=[\n","                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n","                            ],\n","                            temperature=0.3\n","                        )\n","\n","                        new_translations = [choice.message.content for choice in response.choices]\n","\n","                        # Ensure we have the same number of translations as input texts\n","                        if len(new_translations) != len(uncached_texts):\n","                            new_translations = new_translations[:len(uncached_texts)]\n","                            if len(new_translations) < len(uncached_texts):\n","                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                        for text, trans in zip(uncached_texts, new_translations):\n","                            self.batch_cache[text] = trans\n","                            self.current_cache_size += 1\n","\n","                        for idx, trans in zip(uncached_indices, new_translations):\n","                            translations[idx] = trans\n","\n","                        if self.current_cache_size >= self.max_cache_size:\n","                            self._save_batch_cache(batch_id)\n","\n","                        return translations, response\n","\n","                    except Exception as e:\n","                        print(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n","                        if attempt < retry_attempts - 1:\n","                            time.sleep(2)  # Wait before retrying\n","                        else:\n","                            # If all attempts fail, log the error and return partial results\n","                            for idx in uncached_indices:\n","                                translations[idx] = f\"ERROR: {str(e)}\"\n","                            return translations, None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 10949):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.max_sentences = max_sentences\n","        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n","        self.start_line = min(start_line, max(0, self.total_lines - 1))\n","\n","    def _count_lines(self) -> int:\n","        try:\n","            with open(self.file_path, \"rb\") as f:\n","                return sum(1 for _ in f)\n","        except Exception as e:\n","            print(f\"Error counting lines: {str(e)}\")\n","            return 0\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        processed_lines = 0\n","\n","        try:\n","            with open(self.file_path, \"rb\") as f:\n","                # Skip to start line\n","                for _ in range(self.start_line):\n","                    next(f, None)\n","\n","                for line in f:\n","                    if processed_lines >= self.max_sentences:\n","                        break\n","\n","                    try:\n","                        text = line.strip().decode('utf-8')\n","                        if text:  # Only add non-empty texts\n","                            current_batch.append(text)\n","                            processed_lines += 1\n","\n","                            if len(current_batch) == self.batch_size:\n","                                yield current_batch\n","                                current_batch = []\n","                    except Exception as e:\n","                        print(f\"Error processing line: {line.strip()}\")\n","                        print(f\"Error details: {str(e)}\")\n","                        continue\n","\n","                if current_batch:  # Don't forget last partial batch\n","                    yield current_batch\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {str(e)}\")\n","            if current_batch:  # Yield any remaining batch on error\n","                yield current_batch\n","\n","class CostTracker:\n","    def __init__(self):\n","        self.requests = 0\n","        self.total_tokens = 0\n","        self.price_per_1k_tokens = 0.002\n","\n","    def update(self, response: ChatCompletion):\n","        self.requests += 1\n","        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n","\n","    def get_cost(self) -> float:\n","        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n","\n","    def report(self) -> str:\n","        return f\"\"\"\n","        API Calls: {self.requests}\n","        Total Tokens: {self.total_tokens}\n","        Estimated Cost: ${self.get_cost():.2f}\n","        \"\"\"\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    for filename, data in [\n","        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n","        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n","    ]:\n","        try:\n","            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n","                for item in data:\n","                    f.write(f\"{item}\\n\")\n","        except Exception as e:\n","            print(f\"Error saving to {filename}: {str(e)}\")\n","\n","    try:\n","        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n","            for orig, trans in zip(originals, translations):\n","                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n","    except Exception as e:\n","        print(f\"Error saving aligned pairs: {str(e)}\")\n","\n","def process_dataset(\n","    input_file: str,\n","    dataset_name: str,\n","    api_key: str,\n","    batch_size: int = 32,\n","    checkpoint_interval: int = 5,\n","    max_sentences: int = 53315\n","):\n","    translator = TranslationManager(api_key=api_key)\n","\n","    # Load checkpoint if exists\n","    checkpoint = translator.load_checkpoint(dataset_name)\n","    start_batch = checkpoint[\"last_batch\"] + 1\n","    translations_count = checkpoint[\"translations_count\"]\n","\n","    # Calculate starting line\n","    start_line = start_batch * batch_size\n","\n","    # Create iterator with sentence limit\n","    dataset_iterator = DatasetIterator(\n","        file_path=input_file,\n","        batch_size=batch_size,\n","        start_line=start_line,\n","        max_sentences=max_sentences\n","    )\n","\n","    cost_tracker = CostTracker()\n","    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n","\n","    print(f\"Starting from batch {start_batch}, line {start_line}\")\n","    print(f\"Will process up to {max_sentences} sentences\")\n","    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n","\n","    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n","        try:\n","            batch_id = f\"{dataset_name}_{batch_num}\"\n","            translations, response = translator.translate_batch(batch, batch_id)\n","\n","            if response is not None:\n","                cost_tracker.update(response)\n","\n","            save_pairs(batch, translations, dataset_name, mode=\"a\")\n","            translations_count += len(translations)\n","\n","            if batch_num % checkpoint_interval == 0:\n","                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n","                print(f\"Checkpoint saved at batch {batch_num}\")\n","\n","            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n","            if batch_num % 10 == 0:\n","                print(cost_tracker.report())\n","\n","            # Add a small delay to avoid rate limiting\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Error processing batch {batch_num}: {str(e)}\")\n","            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n","            time.sleep(1)  # Longer delay on error\n","            continue\n","\n","    # Final checkpoint and report\n","    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n","    print(\"\\nFinal Statistics:\")\n","    print(cost_tracker.report())\n","    print(f\"Total translations: {translations_count}\")\n"],"metadata":{"id":"-_NetKpqc929"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/ppdb-1.0-s-m2o\"\n","    dataset_name = \"ppdb-1.0-s-m2o_test\"\n","    max_sentences = 53315  # Total number of sentences to process\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=32,\n","        checkpoint_interval=10,  # Increased for larger dataset\n","        max_sentences=max_sentences,\n","    )\n"],"metadata":{"id":"yeoIw578nCt3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uD21LepBkXu"},"source":["## **paws_train.tsv**"]},{"cell_type":"code","source":["# Code for translating TSV dataset with batching and checkpoints\n","\n","from typing import Iterator, List, Dict, Any, Tuple, Optional\n","import numpy as np\n","import json\n","import os\n","from pathlib import Path\n","from datetime import datetime\n","from openai import OpenAI\n","from openai.types.chat import ChatCompletion\n","import time\n","\n","class TranslationManager:\n","    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n","        self.client = OpenAI(api_key=api_key)\n","        self.cache_dir = Path(cache_dir)\n","        self.checkpoint_dir = Path(checkpoint_dir)\n","        self.cache_dir.mkdir(exist_ok=True)\n","        self.checkpoint_dir.mkdir(exist_ok=True)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","        self.max_cache_size = 1000\n","\n","    def _get_cache_file(self, batch_id: str) -> Path:\n","        return self.cache_dir / f\"cache_{batch_id}.json\"\n","\n","    def get_checkpoint_file(self, dataset_name: str) -> Path:\n","        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n","\n","    def _save_batch_cache(self, batch_id: str):\n","        cache_file = self._get_cache_file(batch_id)\n","        with open(cache_file, 'w', encoding='utf-8') as f:\n","            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n","        self.batch_cache = {}\n","        self.current_cache_size = 0\n","\n","    def _load_cache(self, batch_id: str) -> Dict:\n","        cache_file = self._get_cache_file(batch_id)\n","        if cache_file.exists():\n","            with open(cache_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {}\n","\n","    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n","        checkpoint_data = {\n","            \"last_batch\": batch_num,\n","            \"translations_count\": translations_count,\n","            \"timestamp\": datetime.now().isoformat()\n","        }\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n","            json.dump(checkpoint_data, f, indent=2)\n","\n","    def load_checkpoint(self, dataset_name: str) -> Dict:\n","        checkpoint_file = self.get_checkpoint_file(dataset_name)\n","        if checkpoint_file.exists():\n","            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n","                return json.load(f)\n","        return {\"last_batch\": -1, \"translations_count\": 0}\n","\n","    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n","        if not texts:\n","            return [], None\n","\n","        try:\n","            cache = self._load_cache(batch_id)\n","            translations = [\"\"] * len(texts)  # Initialize with empty strings\n","            uncached_texts = []\n","            uncached_indices = []\n","\n","            for i, text in enumerate(texts):\n","                text = text.strip()\n","                if text in cache:\n","                    translations[i] = cache[text]\n","                else:\n","                    uncached_texts.append(text)\n","                    uncached_indices.append(i)\n","\n","            if uncached_texts:\n","                retry_attempts = 3\n","                for attempt in range(retry_attempts):\n","                    try:\n","                        response = self.client.chat.completions.create(\n","                            model=\"gpt-3.5-turbo\",\n","                            messages=[\n","                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n","                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n","                            ],\n","                            temperature=0.3\n","                        )\n","\n","                        new_translations = [choice.message.content for choice in response.choices]\n","\n","                        # Ensure we have the same number of translations as input texts\n","                        if len(new_translations) != len(uncached_texts):\n","                            new_translations = new_translations[:len(uncached_texts)]\n","                            if len(new_translations) < len(uncached_texts):\n","                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n","\n","                        for text, trans in zip(uncached_texts, new_translations):\n","                            self.batch_cache[text] = trans\n","                            self.current_cache_size += 1\n","\n","                        for idx, trans in zip(uncached_indices, new_translations):\n","                            translations[idx] = trans\n","\n","                        if self.current_cache_size >= self.max_cache_size:\n","                            self._save_batch_cache(batch_id)\n","\n","                        return translations, response\n","\n","                    except Exception as e:\n","                        print(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n","                        if attempt < retry_attempts - 1:\n","                            time.sleep(2)  # Wait before retrying\n","                        else:\n","                            # If all attempts fail, log the error and return partial results\n","                            for idx in uncached_indices:\n","                                translations[idx] = f\"ERROR: {str(e)}\"\n","                            return translations, None\n","\n","            return translations, None\n","\n","        except Exception as e:\n","            print(f\"Error in batch processing: {str(e)}\")\n","            return [f\"ERROR: {str(e)}\"] * len(texts), None\n","\n","\n","class DatasetIterator:\n","    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1, max_sentences: int = 10949):\n","        self.file_path = file_path\n","        self.batch_size = batch_size\n","        self.text_column_index = text_column_index  # For TSV files, specify which column contains the text\n","        self.max_sentences = max_sentences\n","        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n","        # Add 1 to account for header in TSV\n","        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n","\n","    def _count_lines(self) -> int:\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                return sum(1 for _ in f)\n","        except Exception as e:\n","            print(f\"Error counting lines: {str(e)}\")\n","            return 0\n","\n","    def __iter__(self) -> Iterator[List[str]]:\n","        current_batch = []\n","        processed_lines = 0\n","\n","        try:\n","            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n","                # Skip header\n","                header = next(f, None)\n","                if not header:\n","                    raise ValueError(\"Empty file or no header found\")\n","\n","                # Skip to start line (accounting for already skipped header)\n","                for _ in range(self.start_line - 1):\n","                    next(f, None)\n","\n","                for line in f:\n","                    if processed_lines >= self.max_sentences:\n","                        break\n","\n","                    try:\n","                        columns = line.strip().split('\\t')\n","                        if len(columns) > self.text_column_index:\n","                            text = columns[self.text_column_index].strip()\n","                            if text:  # Only add non-empty texts\n","                                current_batch.append(text)\n","                                processed_lines += 1\n","\n","                                if len(current_batch) == self.batch_size:\n","                                    yield current_batch\n","                                    current_batch = []\n","                    except Exception as e:\n","                        print(f\"Error processing line: {line.strip()}\")\n","                        print(f\"Error details: {str(e)}\")\n","                        continue\n","\n","                if current_batch:  # Don't forget last partial batch\n","                    yield current_batch\n","\n","        except Exception as e:\n","            print(f\"Error reading file: {str(e)}\")\n","            if current_batch:  # Yield any remaining batch on error\n","                yield current_batch\n","\n","class CostTracker:\n","    def __init__(self):\n","        self.requests = 0\n","        self.total_tokens = 0\n","        self.price_per_1k_tokens = 0.002\n","\n","    def update(self, response: ChatCompletion):\n","        self.requests += 1\n","        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n","\n","    def get_cost(self) -> float:\n","        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n","\n","    def report(self) -> str:\n","        return f\"\"\"\n","        API Calls: {self.requests}\n","        Total Tokens: {self.total_tokens}\n","        Estimated Cost: ${self.get_cost():.2f}\n","        \"\"\"\n","\n","def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n","    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n","\n","    for filename, data in [\n","        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n","        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n","    ]:\n","        try:\n","            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n","                for item in data:\n","                    f.write(f\"{item}\\n\")\n","        except Exception as e:\n","            print(f\"Error saving to {filename}: {str(e)}\")\n","\n","    try:\n","        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n","            for orig, trans in zip(originals, translations):\n","                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n","    except Exception as e:\n","        print(f\"Error saving aligned pairs: {str(e)}\")\n","\n","def process_dataset(\n","    input_file: str,\n","    dataset_name: str,\n","    api_key: str,\n","    batch_size: int = 32,\n","    checkpoint_interval: int = 5,\n","    max_sentences: int = 49402,\n","    text_column_index: int = 1\n","):\n","    translator = TranslationManager(api_key=api_key)\n","\n","    # Load checkpoint if exists\n","    checkpoint = translator.load_checkpoint(dataset_name)\n","    start_batch = checkpoint[\"last_batch\"] + 1\n","    translations_count = checkpoint[\"translations_count\"]\n","\n","    # Calculate starting line\n","    start_line = start_batch * batch_size\n","\n","    # Create iterator with sentence limit and TSV column index\n","    dataset_iterator = DatasetIterator(\n","        file_path=input_file,\n","        batch_size=batch_size,\n","        start_line=start_line,\n","        text_column_index=text_column_index,\n","        max_sentences=max_sentences\n","    )\n","\n","    cost_tracker = CostTracker()\n","    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n","\n","    print(f\"Starting from batch {start_batch}, line {start_line}\")\n","    print(f\"Will process up to {max_sentences} sentences\")\n","    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n","    print(f\"Reading text from column index: {text_column_index}\")\n","\n","    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n","        try:\n","            batch_id = f\"{dataset_name}_{batch_num}\"\n","            translations, response = translator.translate_batch(batch, batch_id)\n","\n","            if response is not None:\n","                cost_tracker.update(response)\n","\n","            save_pairs(batch, translations, dataset_name, mode=\"a\")\n","            translations_count += len(translations)\n","\n","            if batch_num % checkpoint_interval == 0:\n","                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n","                print(f\"Checkpoint saved at batch {batch_num}\")\n","\n","            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n","            if batch_num % 10 == 0:\n","                print(cost_tracker.report())\n","\n","            # Add a small delay to avoid rate limiting\n","            time.sleep(0.5)\n","\n","        except Exception as e:\n","            print(f\"Error processing batch {batch_num}: {str(e)}\")\n","            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n","            time.sleep(1)  # Longer delay on error\n","            continue\n","\n","    # Final checkpoint and report\n","    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n","    print(\"\\nFinal Statistics:\")\n","    print(cost_tracker.report())\n","    print(f\"Total translations: {translations_count}\")\n"],"metadata":{"id":"fo_M6FGuBgLR","executionInfo":{"status":"ok","timestamp":1739351745736,"user_tz":-60,"elapsed":1035,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}}},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":["#### Execution of the code"],"metadata":{"id":"2F5tExbJ_kNs"}},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","    input_file = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","    dataset_name = \"paws_paraphrase_test\"\n","    max_sentences = 49402  # Total number of sentences to process\n","\n","    try:\n","        from google.colab import userdata\n","        api_key = userdata.get('OPENAI_API_KEY')\n","    except ImportError:\n","        api_key = os.getenv('OPENAI_API_KEY')\n","\n","    if not api_key:\n","        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n","\n","    process_dataset(\n","        input_file=input_file,\n","        dataset_name=dataset_name,\n","        api_key=api_key,\n","        batch_size=32,\n","        checkpoint_interval=10,  # Increased for larger dataset\n","        max_sentences=max_sentences,\n","        text_column_index=1  # Specify which column contains the text to translate\n","    )"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":634},"id":"Pvsa3wqq6LwP","executionInfo":{"status":"error","timestamp":1739351849619,"user_tz":-60,"elapsed":89366,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"48c15dd7-d070-43e7-9c43-5ee55e8d666f"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Starting from batch 0, line 0\n","Will process up to 49402 sentences\n","Total lines to process: 49402\n","Reading text from column index: 1\n","Checkpoint saved at batch 0\n","Processed batch 0/1543 (32 items)\n","\n","        API Calls: 1\n","        Total Tokens: 2173\n","        Estimated Cost: $0.00\n","        \n","Processed batch 1/1543 (32 items)\n","Processed batch 2/1543 (32 items)\n","Processed batch 3/1543 (32 items)\n","Processed batch 4/1543 (32 items)\n","Processed batch 5/1543 (32 items)\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-4-04133e259b2e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     process_dataset(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-d0d9c58f879c>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(input_file, dataset_name, api_key, batch_size, checkpoint_interval, max_sentences, text_column_index)\u001b[0m\n\u001b[1;32m    268\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    269\u001b[0m             \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset_name}_{batch_num}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 270\u001b[0;31m             \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    271\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    272\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-d0d9c58f879c>\u001b[0m in \u001b[0;36mtranslate_batch\u001b[0;34m(self, texts, batch_id)\u001b[0m\n\u001b[1;32m     81\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_attempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                         response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     84\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m                             messages=[\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    997\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"markdown","metadata":{"id":"faVNT2NCBkXu"},"source":["#### Check the number of lines in the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4976,"status":"ok","timestamp":1734602901137,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"},"user_tz":-60},"outputId":"53d36e7d-e4e4-4048-bf6d-b1ca44406f92","id":"sFJknMiJBkXu"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File found!\n","The file has 49402 rows.\n"]}],"source":["# Chech the size of the file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","\n","# Check if file exists\n","import os\n","if os.path.exists(file_path):\n","    print(\"File found!\")\n","else:\n","    print(\"File not found. Check the path!\")\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","\n","# Count lines in the file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n","\n","print(f\"The file has {line_count} rows.\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":208,"referenced_widgets":["827270af9be049789f715f568bf33333","39ebd284abe7428aa1da678f4f1311b4","b1028910c80a42c0858883b35e5fb018","86725f3bfc48474bb1cdf15687311c4f","f1a36aa40f2643488effee44205905d3","f44bbe4484e0485bacef6bae4c896738","4e9587f23fb841e583adbec831b0e9b9","58e243e3242f4de6b94d850964e44a66","046cf7e2bba24d76981ac3c8ac5adf63","17ccb71754574c299ccbb536b929bc83","4e23cd19624042b3827737dd8d567937"]},"executionInfo":{"elapsed":111895,"status":"ok","timestamp":1734613731945,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"},"user_tz":-60},"outputId":"6a6eecb0-1091-4f1b-a004-91241420f527","id":"IpQAwpsaBkXv"},"outputs":[{"name":"stdout","output_type":"stream","text":["Using device: cuda\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"827270af9be049789f715f568bf33333","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n"]},{"name":"stdout","output_type":"stream","text":["Model loaded successfully\n","Reading file and selecting samples...\n","Total lines in file: 49402\n","Selected 10000 random lines for translation\n"]},{"name":"stderr","output_type":"stream","text":["Translating: 100%|| 313/313 [2:07:43<00:00, 24.49s/it]"]},{"name":"stdout","output_type":"stream","text":["Translation complete! Translated file saved to: /content/drive/My Drive/Colab Notebooks/translated_paws_train.tsv\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["import torch\n","from transformers import pipeline\n","import random\n","from tqdm import tqdm  # Adding progress bar\n","\n","# Function to load model\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","# Batch translation function\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","# File paths\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n","output_file_path = \"/content/drive/My Drive/Colab Notebooks/translated_paws_train.tsv\"\n","\n","# Setup\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","# Load model\n","translator = load_model()\n","print(\"Model loaded successfully\")\n","\n","# Parameters\n","num_samples = 10000\n","batch_size = 32\n","\n","try:\n","    # First, read all lines and select random samples\n","    print(\"Reading file and selecting samples...\")\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as infile:\n","        all_lines = [line.strip() for line in infile if line.strip()]\n","\n","    # Select random samples\n","    total_lines = len(all_lines)\n","    print(f\"Total lines in file: {total_lines}\")\n","\n","    selected_lines = random.sample(all_lines, num_samples)\n","    print(f\"Selected {num_samples} random lines for translation\")\n","\n","    # Process selected lines in batches\n","    batches = [selected_lines[i:i + batch_size] for i in range(0, len(selected_lines), batch_size)]\n","\n","    # Translate and save\n","    with open(output_file_path, \"w\", encoding=\"utf-8\") as outfile:\n","        for batch_num, batch in enumerate(tqdm(batches, desc=\"Translating\")):\n","            try:\n","                translated_batch = translate_batch(batch, translator)\n","                outfile.write(\"\\n\".join(translated_batch) + \"\\n\")\n","\n","                # Clear CUDA cache periodically\n","                if torch.cuda.is_available() and batch_num % 10 == 0:\n","                    torch.cuda.empty_cache()\n","\n","            except Exception as e:\n","                print(f\"Error processing batch {batch_num}: {str(e)}\")\n","                continue\n","\n","    print(f\"Translation complete! Translated file saved to: {output_file_path}\")\n","\n","except Exception as e:\n","    print(f\"An error occurred: {str(e)}\")\n","\n","# Optional: Save indices of selected lines for reproducibility\n","with open(output_file_path + \".indices\", \"w\", encoding=\"utf-8\") as f:\n","    for i, line in enumerate(selected_lines):\n","        f.write(f\"{i}\\t{line}\\n\")"]},{"cell_type":"markdown","metadata":{"id":"E3utXiV_BkXv"},"source":[]},{"cell_type":"markdown","metadata":{"id":"fdUr6j_0BkXz"},"source":["## **Quora Duplicate Questions**"]},{"cell_type":"markdown","metadata":{"id":"xVsiYu9eBkXz"},"source":["#### Checking the size of the file"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1737021941636,"user_tz":-60,"elapsed":5154,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"outputId":"241791b7-a107-4389-aac8-baa2ca1f5e7e","id":"w1RGOCsPBkXz"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","File found!\n","The file has 404302 rows.\n"]}],"source":["# Check the size of the file\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n","\n","# Check if file exists\n","import os\n","if os.path.exists(file_path):\n","    print(\"File found!\")\n","else:\n","    print(\"File not found. Check the path!\")\n","\n","file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n","\n","# Count lines in the file\n","with open(file_path, \"r\", encoding=\"utf-8\") as file:\n","    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n","\n","print(f\"The file has {line_count} rows.\")\n"]},{"cell_type":"markdown","metadata":{"id":"lxKAjCS6BkX0"},"source":["#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"MVXaiqFIBkX0"},"outputs":[],"source":["# new code for more memory-efficient approach using iterators to save the matching pairs\n","\n","import torch\n","import numpy as np\n","from transformers import pipeline\n","\n","def load_model():\n","    model_id = \"dvres/GaMS-1B-Translator_0.1\"\n","    translator = pipeline(\n","        \"text-generation\",\n","        model=model_id,\n","        device_map=\"auto\",\n","        token=\"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\"\n","    )\n","    return translator\n","\n","def translate_batch(texts, translator):\n","    translations = []\n","    for text in texts:\n","        messages = [{\"role\": \"english\", \"content\": text}]\n","        response = translator(\n","            messages,\n","            max_new_tokens=512\n","        )[0][\"generated_text\"][-1][\"content\"]\n","        translations.append(response)\n","    return translations\n","\n","def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        total_lines = sum(1 for _ in f)\n","\n","    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))\n","    selected_texts = []\n","\n","    with open(input_file_path, \"r\", encoding=\"utf-8\") as f:\n","        for i, line in enumerate(f):\n","            if i in selected_indices:\n","                selected_texts.append(line.strip())\n","                if len(selected_texts) == batch_size:\n","                    try:\n","                        translations = translate_batch(selected_texts, translator)\n","                        save_pairs(selected_texts, translations, dataset_name)\n","                        print(f\"Processed batch: {len(selected_texts)} lines\")\n","                        selected_texts = []\n","                        if torch.cuda.is_available():\n","                            torch.cuda.empty_cache()\n","                    except Exception as e:\n","                        print(f\"Error processing batch: {str(e)}\")\n","                        continue\n","\n","    if selected_texts:\n","        try:\n","            translations = translate_batch(selected_texts, translator)\n","            save_pairs(selected_texts, translations)\n","            print(f\"Processed remaining {len(selected_texts)} lines\")\n","        except Exception as e:\n","            print(f\"Error processing final batch: {str(e)}\")\n","\n","def save_pairs(originals, translations, dataset_name):\n","    with open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt\", \"a\", encoding=\"utf-8\") as f1, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt\", \"a\", encoding=\"utf-8\") as f2, \\\n","         open(f\"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt\", \"a\", encoding=\"utf-8\") as f3:\n","        for orig, trans in zip(originals, translations):\n","            f1.write(orig + \"\\n\")\n","            f2.write(trans + \"\\n\")\n","            f3.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")"]},{"cell_type":"markdown","metadata":{"id":"_av7NrelBkX0"},"source":["#### Translating quora duplicate questions - executing for\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["d396a1f97a9b402aa1e9932295ac07ae","17ce541421814b9dba563683f2c5d3f6","6e7807eb3031485b9c5860ac49ce188f","801612be699a415382733c1bc9f9cbbc","9cfbff66c74b4630ac11cc7d9c2672f7","a1d62f8fe66d4ba68cace96c022ef611","c40dad2b3dbd4cef8f72d75b4cac51be","4e9c16911076461fa4a55e155d271b7a","0cbfce6c64ee4beb81c2194243a347d4","c0146e088fe64e86aa64757c310e8ddb","72c92dacce26432c93b16705090b3a82","9c14c296fcd842aeaa4756087672fe17","70f860bd97924bb699f4095afd2f7359","468d60d8d7f44713a4c40f8cc0dea802","ee5ed91618694e0fb50f93405676b715","190a05e36f1a4d138dd723607cd172e5","dd94e079abd74d11a75f1cb37d7771c7","39f6c8ae33264721b3537aa5fd435469","e6a1739732774b7399d7dabb734f44ab","ceaab7d73eb843fcb0fbb4e0d924c813","b8f775941848479b8f6bf06de49ff423","963ac02d34f946a0ad8301db079ae4db","b9dfdd45c6db48ab91782f925ed05891","e0c79d53a9574f81a890c5bfcab2cc97","5923cb6ffa48450b9ff0f559d5ec886e","ec0294adb73a41c3a25ba751cfa2763a","c2285cf091774e5997c71811d59b5429","ed4d6b8342914b6fa165f420cdd535a2","d368061f36d0411a90412019ce14a341","79f9151374514624a80be7f82a61a834","13373ab855584ddcb3531fc04d6b9b7c","1cf7ae8f6ae14c00bb1fd1e7539f0345","0fc2bf50f6ac4c6db34e09098a9c654c","a533c0fb3de7442a88ea5cb0c469b648","1c3ecc6b086741d5b14939c7320d6fc2","eb17751102bb4885957236a1a16953fd","aabc468c224d42da812f29a7a10da868","a13f7b074af341f989d73414513d84b6","38f55f40c7304f60bbac27c1e035e905","0aff301f3b5e485ab80d18c49d0f96a9","a83cdc340d1b4a3e96c2876497f05707","f9c9ce81ab2849339499e9d9d8ba364b","1d27adea7cc941319a5b8b10890b7b80","a9a0a3ea36744328aa74c74ded39edd8","fa31365072da404eac5643961c03852e","99e47288464e4ff591f3fb9c1e8e49d0","49532c48e5534dd794f15d95a669680b","ae7209517112467aba76b688ff4f777b","d2216956e4ea4ef1992adeeb7893353b","0afae8bbb1aa4bf59a9dc5a1ee4afcd8","cd3f12c1b8fa4d4e8bff5ed7ea19a31f","b4a0d96174b245339f176af680e4ed97","48832ff395d049cf9563b7efb44833bc","ee45cf497eda4aee99573e5189db4958","239035ee78dd4f08994a97bebed48ed9","1ba315f2b6a6470aa250368f2ce3347e","7708fde1cd0148539dd8747d3e9e2b00","a149dc0a071f450ca9e7a8edf1714beb","473a646a5f67443e9be38c05a92b4d33","d2ece034a964418abae8e4dcdd54cddc","ddd7a828f3d74dfe953d37971384287f","f361f9154fd34c03ae00dc997dc6ed44","2761bd917d1b4c96b6f98d08fcdc58ae","d41bdd7e965447eaabbe81835aa3d3bd","8e62e4eb20f0412b8f8ab1b407507778","926b7374470b4a31a5202cbdf48c6bbb","0a76a9b3ca154682ba42052e84b5aee2","259171e08aa345d5b45b1c65c72db023","e955496bbda042e7aaf6ada08643dbb2","96d974751c7546558ecc5655e923f5bc","974ad51b88b44ed091b45696a564b5e9","97982ebafb5640bfa8858b066568f37b","0b63519fd2dd45eaa23a67c36ded740b","dfbb3d5045a24818a1b41bde950b2d9c","724465cd5a5a46548a05945e7c5c5980","302e7e442d3a4151b5125784534c97a6","a3983dad77eb4173998f2f8aac3408e4","2a224c5745ac4e3ea30b1fd6c32202b8","799906dc3425492096cccc0f75199324","75024ada41614cbd91461cf6a01605cf","529d7eb8d84846718b387c8915c32c6b","f281cae9a2e740c480746137d1990e39","332f8313fe5e4f54a65f6a0be11f1c1a","ec86805562dc4db89600d41fe43a2293","be119df5ecfd4c6897b7beced51b37ea","0767f2af1a3b4d4f832cf6eada9b549e","f6547ba510f649b482d1c554d3e42ced","5ff36b5bb08c4292a46cc4eb4b3774c3","f28b0c420709475e88b3efefaab477d0","91acfac934f2420694602a19fc027ec8","22c3f365869144f4b6485e5ef3307698","e16127b9de5c4e8087e304d4da81e393","fcb96432a7e24fe6890feaa5ff3a833a","d8da9f51758f4cb5bb8c72b898ecfe77","2fd90a78cc474d8e80f875ee644c210b","11dc513db76840c8a7e5c738763ac5aa","6705b79663304da583c498ae6c43959b","d6bd1b287d114e0c92c15782dd0c148b","da209107cc324c2bbdfdd96475fe99a1","6b187b24016c4f3698c2b390084da669","2a071d1bfb3e44a0a4db08e0324ac5af","482f7c16e5054b44bb08659d64b9e787","1d71bc0ef6d24ff7aa90c5cfb5d5c403","a408e8202d0046fe9505701b0251b4d8","accb502740634c389fa140fcd5c500b7","7c599bc10da7497da6b68f6d70235b04","2551e2b1e3624198a268c396c5895bac","10e4b6b260624b4e8fda7ffbe48570e2","4c220ef243e3434286fe57e4ba30f369","65399957dc654a9988a2a1e72be7fcda"]},"outputId":"677ecba5-386f-4d10-f291-31c8bc0d3659","executionInfo":{"status":"ok","timestamp":1737025258465,"user_tz":-60,"elapsed":876219,"user":{"displayName":"Alenka Zumer","userId":"03523294138110394084"}},"id":"fcOY6aaCBkX0"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d396a1f97a9b402aa1e9932295ac07ae","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/835 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"9c14c296fcd842aeaa4756087672fe17","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/33.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9dfdd45c6db48ab91782f925ed05891","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a533c0fb3de7442a88ea5cb0c469b648","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00002.safetensors:   0%|          | 0.00/4.97G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fa31365072da404eac5643961c03852e","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00002.safetensors:   0%|          | 0.00/1.19G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"1ba315f2b6a6470aa250368f2ce3347e","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a76a9b3ca154682ba42052e84b5aee2","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a224c5745ac4e3ea30b1fd6c32202b8","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/2.22k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f28b0c420709475e88b3efefaab477d0","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.19M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6b187b24016c4f3698c2b390084da669","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/964 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Device set to use cuda:0\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n"]},{"output_type":"stream","name":"stdout","text":["Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Processed batch: 32 lines\n","Error processing final batch: save_pairs() missing 1 required positional argument: 'dataset_name'\n"]}],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","translator = load_model()\n","input_file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n","select_and_translate(input_file_path, translator, \"quora_duplicate_questions.tsv\")"]}]}