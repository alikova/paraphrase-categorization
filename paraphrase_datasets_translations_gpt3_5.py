# -*- coding: utf-8 -*-
"""Paraphrase_Datasets_Translations_GPT3.5.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1EmXWiqcv6slfTZS5F5GMwZM1Ua8coTo7

#### Optimized code for translation (*test_code_Paraphrase Datasets Translations - GaMS.ipynb*) with OpenAI model. This script uses cache patterns and batch processing inside of pipeline programming, enabling parallel processing of translation batches. As a result, the API calls are optimized, and if an error occurs during the translation of a single sentence, the entire batch is not lost. We are using ChatGPT-3.5 via the OpenAI API key.

## Mount Google Drive
"""

from google.colab import drive
drive.mount('/content/drive')

"""## Git add and commit"""

!git clone https://alikova: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@github.com/alikova/paraphrase-categorization.git

!ls /content

# Commented out IPython magic to ensure Python compatibility.
# %cd /content/paraphrase-categorization
!pwd

!find /content -name "Paraphrase_Datasets_Translations_GPT3.5.ipynb"

!ls /content/paraphrase-categorization/

!ls /content/drive/MyDrive/

!find /content/drive/ -name "Paraphrase_Datasets_Translations_GPT3.5.ipynb"

# Commented out IPython magic to ensure Python compatibility.
!cp "/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb" /content/paraphrase-categorization/

!ls /content/paraphrase-categorization/
# %cd /content/paraphrase-categorization

!ls -a

!git add Paraphrase_Datasets_Translations_GPT3.5.ipynb
!git status

!git config --global user.email "z.alenka7@gmail.com"
!git config --global user.name "alikova"

!git commit -m "Update Paraphrase_Datasets_Translations_GPT3.5.ipynb"

!git push origin main  # or the appropriate branch name if it's not 'main'

"""## Installations"""

pip uninstall openai

!pip uninstall -y openai
!pip install openai>=1.0.0

#!pip uninstall -y openai
!pip install openai>=1.0.0
!python -c "import openai; print(openai.__version__)"  # Should print 1.x.x

"""## Connect to OpenAI with an API key"""

import os
import openai

from google.colab import userdata
#userdata.get('OPENAI_API_KEY')

# Retrieve the API key
api_key = userdata.get("OPENAI_API_KEY")

# Verify if the key exists (good practice)
if api_key is None:
    raise ValueError("API key not found in environment variables")

print("API key successfully loaded")

# This will retrieve the key you configured in Colab's secrets
api_key = userdata.get('OPENAI_API_KEY')

# Create an OpenAI client (New API format)
from openai import OpenAI  # We'll use synchronous version instead

# Instead of AsyncOpenAI, we'll modify our code to use the synchronous version:
client = OpenAI(api_key=api_key)

# Example OpenAI API request
response = client.chat.completions.create(
    model="gpt-3.5-turbo",
    messages=[{"role": "user", "content": "Tell me a joke!"}]
)

print(response.choices[0].message.content)
print(response.usage.total_tokens)

"""# Translate with Pipeline Programming

#####           Lower Temperature (closer to 0): The model will be more deterministic and predictable, choosing the words with the highest probability. It will tend to produce more repetitive and safe outputs. This is often preferred for tasks where accuracy and consistency are paramount, such as translation or factual question answering. Higher Temperature (closer to 1 or above): The model becomes more creative and unpredictable. It's more likely to sample from less probable words, leading to more diverse and unexpected outputs. This is suitable for tasks where creativity and novelty are desired, such as creative writing or brainstorming.

#### Code with asynchronous function - not working
"""

# Throving an error

from typing import Iterator, List, Dict, Any, Tuple, Optional
from openai import OpenAI
import json
from pathlib import Path
from datetime import datetime
import time
import os

class TranslationManager:
    def __init__(self, api_key: str, cache_dir: str = "translation_cache", checkpoint_dir: str = "checkpoints"):
        #self.api_key = api_key  # Store the API key
        self.client = OpenAI(api_key=api_key)  # Initialize client properly
        self.cache_dir = Path(cache_dir)
        self.checkpoint_dir = Path(checkpoint_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.batch_cache = {}
        self.current_cache_size = 0
        self.max_cache_size = 1000

    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:
        if not texts:
            return [], None

        try:
            cache = self._load_cache(batch_id)
            translations = []
            uncached_texts = []
            uncached_indices = []

            for i, text in enumerate(texts):
                text = text.strip()
                if not text:  # Skip empty strings
                    translations.append("")
                    continue
                if text in cache:
                    translations.append(cache[text])
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)

            if uncached_texts:
                try:
                    response = self.client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "Translate the following English texts to Slovenian:"},
                            {"role": "user", "content": "\n---\n".join(uncached_texts)}
                        ],
                        temperature=0.3
                    )

                    # Get translations from response
                    new_translations = [choice.message.content for choice in response.choices]

                    # Ensure we have the same number of translations as input texts
                    if len(new_translations) != len(uncached_texts):
                        new_translations = new_translations[:len(uncached_texts)]
                        if len(new_translations) < len(uncached_texts):
                            new_translations.extend([""] * (len(uncached_texts) - len(new_translations)))

                    # Update cache
                    for text, trans in zip(uncached_texts, new_translations):
                        self.batch_cache[text] = trans
                        self.current_cache_size += 1

                    # Insert translations at correct positions
                    for idx, trans in zip(uncached_indices, new_translations):
                        translations.insert(idx, trans)

                    if self.current_cache_size >= self.max_cache_size:
                        self._save_batch_cache(batch_id)

                    return translations, response

                except Exception as e:
                    print(f"Error in API call: {str(e)}")
                    return [f"ERROR: {str(e)}"] * len(texts), None

            return translations, None

        except Exception as e:
            print(f"Error in batch processing: {str(e)}")
            return [f"ERROR: {str(e)}"] * len(texts), None

    def _get_cache_file(self, batch_id: str) -> Path:
        return self.cache_dir / f"cache_{batch_id}.json"

    def get_checkpoint_file(self, dataset_name: str) -> Path:
        return self.checkpoint_dir / f"{dataset_name}_checkpoint.json"

    def _save_batch_cache(self, batch_id: str):
        cache_file = self._get_cache_file(batch_id)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)
        self.batch_cache = {}
        self.current_cache_size = 0

    def _load_cache(self, batch_id: str) -> Dict:
        cache_file = self._get_cache_file(batch_id)
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):
        checkpoint_data = {
            "last_batch": batch_num,
            "translations_count": translations_count,
            "timestamp": datetime.now().isoformat()
        }
        checkpoint_file = self.get_checkpoint_file(dataset_name)
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2)

    def load_checkpoint(self, dataset_name: str) -> Dict:
        checkpoint_file = self.get_checkpoint_file(dataset_name)
        if checkpoint_file.exists():
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"last_batch": -1, "translations_count": 0}

    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:
        if not texts:
            return [], None

        try:
            cache = self._load_cache(batch_id)
            translations = []
            uncached_texts = []
            uncached_indices = []

            for i, text in enumerate(texts):
                text = text.strip()
                if not text:  # Skip empty strings
                    translations.append("")
                    continue
                if text in cache:
                    translations.append(cache[text])
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)

            if uncached_texts:
                try:
                    response = self.client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "Translate the following English texts to Slovenian:"},
                            {"role": "user", "content": "\n---\n".join(uncached_texts)}
                        ],
                        temperature=0.3
                    )

                    # new_translations = response.choices[0].message.content.split("\n---\n") # for version 0.28
                    new_translations = [choice.message.content for choice in response.choices]

                    # Ensure we have the same number of translations as input texts
                    if len(new_translations) != len(uncached_texts):
                        new_translations = new_translations[:len(uncached_texts)]
                        if len(new_translations) < len(uncached_texts):
                            new_translations.extend([""] * (len(uncached_texts) - len(new_translations)))

                    for text, trans in zip(uncached_texts, new_translations):
                        self.batch_cache[text] = trans
                        self.current_cache_size += 1

                    for idx, trans in zip(uncached_indices, new_translations):
                        translations.insert(idx, trans)

                    if self.current_cache_size >= self.max_cache_size:
                        self._save_batch_cache(batch_id)

                    return translations, response

                except Exception as e:
                    print(f"Error in API call: {str(e)}")
                    return [f"ERROR: {str(e)}"] * len(texts), None

            return translations, None

        except Exception as e:
            print(f"Error in batch processing: {str(e)}")
            return [f"ERROR: {str(e)}"] * len(texts), None

class DatasetIterator:
    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1):
        self.file_path = file_path
        self.batch_size = batch_size
        self.text_column_index = text_column_index  # Default to second column (index 1)
        self.total_lines = self._count_lines()
        # Add 1 to account for header
        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))

    def _count_lines(self) -> int:
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                return sum(1 for _ in f)
        except Exception as e:
            print(f"Error counting lines: {str(e)}")
            return 0

    def __iter__(self) -> Iterator[List[str]]:
        current_batch = []
        current_line = 0

        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                # Skip header
                header = next(f, None)
                if not header:
                    raise ValueError("Empty file or no header found")

                # Skip to start line (accounting for already skipped header)
                for _ in range(self.start_line - 1):
                    next(f, None)
                    current_line += 1

                for line in f:
                    try:
                        columns = line.strip().split('\t')
                        if len(columns) > self.text_column_index:
                            text = columns[self.text_column_index].strip()
                            if text:  # Only add non-empty texts
                                current_batch.append(text)
                                if len(current_batch) == self.batch_size:
                                    yield current_batch
                                    current_batch = []
                    except Exception as e:
                        print(f"Error processing line: {line.strip()}")
                        print(f"Error details: {str(e)}")
                        continue

                if current_batch:  # Don't forget last partial batch
                    yield current_batch

        except Exception as e:
            print(f"Error reading file: {str(e)}")
            if current_batch:  # Yield any remaining batch on error
                yield current_batch

class CostTracker:
    def __init__(self):
        self.requests = 0
        self.total_tokens = 0
        self.price_per_1k_tokens = 0.002

    def update(self, response: ChatCompletion):
        self.requests += 1
        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens

    def get_cost(self) -> float:
        return (self.total_tokens / 1000) * self.price_per_1k_tokens

    def report(self) -> str:
        return f"""
        API Calls: {self.requests}
        Total Tokens: {self.total_tokens}
        Estimated Cost: ${self.get_cost():.2f}
        """

def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = "a"):
    base_path = Path("/content/drive/My Drive/Colab Notebooks")

    for filename, data in [
        (f"{dataset_name}_originals_GPT3.5.txt", originals),
        (f"{dataset_name}_translations_GPT3.5.txt", translations),
    ]:
        try:
            with open(base_path / filename, mode, encoding="utf-8") as f:
                for item in data:
                    f.write(f"{item}\n")
        except Exception as e:
            print(f"Error saving to {filename}: {str(e)}")

    try:
        with open(base_path / f"{dataset_name}_aligned_pairs_GPT3.5.txt", mode, encoding="utf-8") as f:
            for orig, trans in zip(originals, translations):
                f.write(f"Original: {orig}\nTranslation: {trans}\n---\n")
    except Exception as e:
        print(f"Error saving aligned pairs: {str(e)}")

def process_dataset(
    input_file: str,
    dataset_name: str,
    api_key: str,
    batch_size: int = 32,
    checkpoint_interval: int = 5
):
    translator = TranslationManager(api_key=api_key)

    # Load checkpoint if exists
    checkpoint = translator.load_checkpoint(dataset_name)
    start_batch = checkpoint["last_batch"] + 1
    translations_count = checkpoint["translations_count"]

    # Calculate starting line
    start_line = start_batch * batch_size

    dataset_iterator = DatasetIterator(input_file, batch_size, start_line)
    cost_tracker = CostTracker()

    total_batches = max(1, dataset_iterator.total_lines // batch_size)

    print(f"Starting from batch {start_batch}, line {start_line}")
    print(f"Total lines in file: {dataset_iterator.total_lines}")

    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):
        try:
            batch_id = f"{dataset_name}_{batch_num}"
            translations, response = translator.translate_batch(batch, batch_id)

            if response is not None:
                cost_tracker.update(response)

            # Only save if we got valid translations
            if translations:
                save_pairs(batch, translations, dataset_name, mode="a")
                translations_count += len(translations)

            if batch_num % checkpoint_interval == 0:
                translator.save_checkpoint(dataset_name, batch_num, translations_count)
                print(f"Checkpoint saved at batch {batch_num}")

            print(f"Processed batch {batch_num}/{total_batches} ({len(batch)} items)")
            if batch_num % 10 == 0:
                print(cost_tracker.report())

            # Add a small delay to avoid rate limiting
            time.sleep(0.5)

        except Exception as e:
            print(f"Error processing batch {batch_num}: {str(e)}")
            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)
            time.sleep(5)  # Longer delay on error
            continue

    # Final checkpoint and report
    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)
    print("\nFinal Statistics:")
    print(cost_tracker.report())
    print(f"Total translations: {translations_count}")

"""#### Code without asynchronous function - Testing the translation on the first 100 or 10 or n sentences"""

# Code is working, it fecthes first n sentences

from typing import Iterator, List, Dict, Any, Tuple, Optional
import numpy as np
import json
import os
from pathlib import Path
from datetime import datetime
from openai import OpenAI
from openai.types.chat import ChatCompletion
import time

class TranslationManager:
    def __init__(self, api_key: str, cache_dir: str = "translation_cache", checkpoint_dir: str = "checkpoints"):
        self.client = OpenAI(api_key=api_key)
        self.cache_dir = Path(cache_dir)
        self.checkpoint_dir = Path(checkpoint_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.batch_cache = {}
        self.current_cache_size = 0
        self.max_cache_size = 1000

    def _get_cache_file(self, batch_id: str) -> Path:
        return self.cache_dir / f"cache_{batch_id}.json"

    def get_checkpoint_file(self, dataset_name: str) -> Path:
        return self.checkpoint_dir / f"{dataset_name}_checkpoint.json"

    def _save_batch_cache(self, batch_id: str):
        cache_file = self._get_cache_file(batch_id)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)
        self.batch_cache = {}
        self.current_cache_size = 0

    def _load_cache(self, batch_id: str) -> Dict:
        cache_file = self._get_cache_file(batch_id)
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):
        checkpoint_data = {
            "last_batch": batch_num,
            "translations_count": translations_count,
            "timestamp": datetime.now().isoformat()
        }
        checkpoint_file = self.get_checkpoint_file(dataset_name)
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2)

    def load_checkpoint(self, dataset_name: str) -> Dict:
        checkpoint_file = self.get_checkpoint_file(dataset_name)
        if checkpoint_file.exists():
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"last_batch": -1, "translations_count": 0}

    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:
        if not texts:
            return [], None

        try:
            cache = self._load_cache(batch_id)
            translations = []
            uncached_texts = []
            uncached_indices = []

            for i, text in enumerate(texts):
                text = text.strip()
                if not text:  # Skip empty strings
                    translations.append("")
                    continue
                if text in cache:
                    translations.append(cache[text])
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)

            if uncached_texts:
                try:
                    response = self.client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "Translate the following English texts to Slovenian:"},
                            {"role": "user", "content": "\n-\n".join(uncached_texts)}
                        ],
                        temperature=0.3
                    )

                    new_translations = [choice.message.content for choice in response.choices]

                    # Ensure we have the same number of translations as input texts
                    if len(new_translations) != len(uncached_texts):
                        new_translations = new_translations[:len(uncached_texts)]
                        if len(new_translations) < len(uncached_texts):
                            new_translations.extend([""] * (len(uncached_texts) - len(new_translations)))

                    for text, trans in zip(uncached_texts, new_translations):
                        self.batch_cache[text] = trans
                        self.current_cache_size += 1

                    for idx, trans in zip(uncached_indices, new_translations):
                        translations.insert(idx, trans)

                    if self.current_cache_size >= self.max_cache_size:
                        self._save_batch_cache(batch_id)

                    return translations, response

                except Exception as e:
                    print(f"Error in API call: {str(e)}")
                    return [f"ERROR: {str(e)}"] * len(texts), None

            return translations, None

        except Exception as e:
            print(f"Error in batch processing: {str(e)}")
            return [f"ERROR: {str(e)}"] * len(texts), None

class DatasetIterator:
    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 100):
        self.file_path = file_path
        self.batch_size = batch_size
        self.max_sentences = max_sentences
        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines
        self.start_line = min(start_line, max(0, self.total_lines - 1))

    def _count_lines(self) -> int:
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                return sum(1 for _ in f)
        except Exception as e:
            print(f"Error counting lines: {str(e)}")
            return 0

    def __iter__(self) -> Iterator[List[str]]:
        current_batch = []
        processed_lines = 0

        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                # Skip to start line
                for _ in range(self.start_line):
                    next(f, None)

                for line in f:
                    if processed_lines >= self.max_sentences:
                        break

                    line = line.strip()
                    if line:  # Only add non-empty lines
                        current_batch.append(line)
                        processed_lines += 1

                        if len(current_batch) == self.batch_size:
                            yield current_batch
                            current_batch = []

                    if processed_lines >= self.max_sentences:
                        break

                if current_batch:  # Don't forget last partial batch
                    yield current_batch

        except Exception as e:
            print(f"Error reading file: {str(e)}")
            if current_batch:  # Yield any remaining batch on error
                yield current_batch

class CostTracker:
    def __init__(self):
        self.requests = 0
        self.total_tokens = 0
        self.price_per_1k_tokens = 0.002

    def update(self, response: ChatCompletion):
        self.requests += 1
        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens

    def get_cost(self) -> float:
        return (self.total_tokens / 1000) * self.price_per_1k_tokens

    def report(self) -> str:
        return f"""
        API Calls: {self.requests}
        Total Tokens: {self.total_tokens}
        Estimated Cost: ${self.get_cost():.2f}
        """

def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = "a"):
    base_path = Path("/content/drive/My Drive/Colab Notebooks")

    for filename, data in [
        (f"{dataset_name}_originals_GPT3.5.txt", originals),
        (f"{dataset_name}_translations_GPT3.5.txt", translations),
    ]:
        try:
            with open(base_path / filename, mode, encoding="utf-8") as f:
                for item in data:
                    f.write(f"{item}\n")
        except Exception as e:
            print(f"Error saving to {filename}: {str(e)}")

    try:
        with open(base_path / f"{dataset_name}_aligned_pairs_GPT3.5.txt", mode, encoding="utf-8") as f:
            for orig, trans in zip(originals, translations):
                f.write(f"Original: {orig}\nTranslation: {trans}\n---\n")
    except Exception as e:
        print(f"Error saving aligned pairs: {str(e)}")

def process_dataset(
    input_file: str,
    dataset_name: str,
    api_key: str,
    batch_size: int = 32,
    checkpoint_interval: int = 5,
    max_sentences: int = 10949 # Parameter to limit number of sentences in the dataset - write manually since this code is working okay
):
    translator = TranslationManager(api_key=api_key)

    # Load checkpoint if exists
    checkpoint = translator.load_checkpoint(dataset_name)
    start_batch = checkpoint["last_batch"] + 1
    translations_count = checkpoint["translations_count"]

    # Calculate starting line
    start_line = start_batch * batch_size

    # Create iterator with sentence limit
    dataset_iterator = DatasetIterator(
        file_path=input_file,
        batch_size=batch_size,
        start_line=start_line,
        max_sentences=max_sentences
    )

    cost_tracker = CostTracker()
    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)

    print(f"Starting from batch {start_batch}, line {start_line}")
    print(f"Will process up to {max_sentences} sentences")
    print(f"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}")

    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):
        try:
            batch_id = f"{dataset_name}_{batch_num}"
            translations, response = translator.translate_batch(batch, batch_id)

            if response is not None:
                cost_tracker.update(response)

            save_pairs(batch, translations, dataset_name, mode="a")
            translations_count += len(translations)

            if batch_num % checkpoint_interval == 0:
                translator.save_checkpoint(dataset_name, batch_num, translations_count)
                print(f"Checkpoint saved at batch {batch_num}")

            print(f"Processed batch {batch_num}/{total_batches} ({len(batch)} items)")
            if batch_num % 10 == 0:
                print(cost_tracker.report())

            # Add a small delay to avoid rate limiting
            time.sleep(0.5)

        except Exception as e:
            print(f"Error processing batch {batch_num}: {str(e)}")
            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)
            time.sleep(1)  # Longer delay on error
            continue

    # Final checkpoint and report
    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)
    print("\nFinal Statistics:")
    print(cost_tracker.report())
    print(f"Total translations: {translations_count}")

# try

if __name__ == "__main__":
    input_file = "/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt"
    dataset_name = "msr_paraphrase_test"  # Using test suffix to distinguish from full runs
    max_sentences = 10949  # Parameter to limit number of sentences in the dataset - write manually since this code is working okay

    try:
        from google.colab import userdata
        api_key = userdata.get('OPENAI_API_KEY')
    except ImportError:
        api_key = os.getenv('OPENAI_API_KEY')

    if not api_key:
        raise ValueError("API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.")

    process_dataset(
        input_file=input_file,
        dataset_name=dataset_name,
        api_key=api_key,
        batch_size=32,  # Keeping original batch size
        checkpoint_interval=10,  # Frequent checkpoints for testing
        max_sentences=10949  # Parameter to limit number of sentences in the dataset - write manually since this code is working okay
    )

"""## msr_paraphrase_data.txt

#### Checking the size of the file
"""

# Check the size of the file

from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt"

# Check if file exists
import os
if os.path.exists(file_path):
    print("File found!")
else:
    print("File not found. Check the path!")

file_path = "/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt"

# Count lines in the file
with open(file_path, "r", encoding="utf-8") as file:
    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines

print(f"The file has {line_count} rows.")

"""### MSR translation

Final Statistics:

        API Calls: 342
        Total Tokens: 1248224
        Estimated Cost: $2.50
        Time: 52min
        
Total translations: 10927
"""

# Successful

if __name__ == "__main__":
    input_file = "/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt"
    dataset_name = "msr_paraphrase_test"  # Using test suffix to distinguish from full runs
    max_sentences = 10949  # Limit to first 100 sentences

    try:
        from google.colab import userdata
        api_key = userdata.get('OPENAI_API_KEY')
    except ImportError:
        api_key = os.getenv('OPENAI_API_KEY')

    if not api_key:
        raise ValueError("API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.")

    process_dataset(
        input_file=input_file,
        dataset_name=dataset_name,
        api_key=api_key,
        batch_size=32,  # Keeping original batch size
        checkpoint_interval=10,  # Frequent checkpoints for testing
        max_sentences=10949  # Parameter to limit number of sentences
    )

"""### Few tries of translation of msr paraphrase dataset - Executing the code for 1h

Final Statistics:

        API Calls: 172
        Total Tokens: 1094704
        Estimated Cost: $2.19

"""

# new, more robust and for non-asynchronous function

if __name__ == "__main__":
    input_file = "/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt"
    dataset_name = "msr_paraphrase"

    try:
        from google.colab import userdata
        api_key = userdata.get('OPENAI_API_KEY')
    except ImportError:
        api_key = os.getenv('OPENAI_API_KEY')

    if not api_key:
        raise ValueError("API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.")

    process_dataset(
        input_file=input_file,
        dataset_name=dataset_name,
        api_key=api_key,
        batch_size=64,
        checkpoint_interval=5
    )

# running for asyncronous function, which is not in use at the moment

if __name__ == "__main__":
    input_file = "/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt"
    dataset_name = "msr_paraphrase"

    # Get the OpenAI API key using userdata
    from google.colab import userdata
    api_key = userdata.get('OPENAI_API_KEY')

    if api_key is None:
        raise ValueError("API key not found. Please set the OPENAI_API_KEY in Colab secrets.")

    # Use nest_asyncio to integrate with the existing loop
    import nest_asyncio
    nest_asyncio.apply()

    # Get the current event loop
    loop = asyncio.get_event_loop()

    # Run the process_dataset function using the existing loop
    loop.run_until_complete(process_dataset(
        input_file=input_file,
        dataset_name=dataset_name,
        api_key=api_key,
        batch_size=64  # Adjust batch size as needed
    ))

"""## para-nmt-50m-small.txt

#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases
"""

# new code for more memory-efficient approach using iterators to save the matching pairs

import torch
import numpy as np
from transformers import pipeline

def load_model():
    model_id = "dvres/GaMS-1B-Translator_0.1"
    translator = pipeline(
        "text-generation",
        model=model_id,
        device_map="auto",
        token="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"
    )
    return translator

def translate_batch(texts, translator):
    translations = []
    for text in texts:
        messages = [{"role": "english", "content": text}]
        response = translator(
            messages,
            max_new_tokens=512
        )[0]["generated_text"][-1]["content"]
        translations.append(response)
    return translations

def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):
    with open(input_file_path, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)

    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))
    selected_texts = []

    with open(input_file_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i in selected_indices:
                selected_texts.append(line.strip())
                if len(selected_texts) == batch_size:
                    try:
                        translations = translate_batch(selected_texts, translator)
                        save_pairs(selected_texts, translations, dataset_name)
                        print(f"Processed batch: {len(selected_texts)} lines")
                        selected_texts = []
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e:
                        print(f"Error processing batch: {str(e)}")
                        continue

    if selected_texts:
        try:
            translations = translate_batch(selected_texts, translator)
            save_pairs(selected_texts, translations)
            print(f"Processed remaining {len(selected_texts)} lines")
        except Exception as e:
            print(f"Error processing final batch: {str(e)}")

def save_pairs(originals, translations, dataset_name):
    with open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt", "a", encoding="utf-8") as f1, \
         open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt", "a", encoding="utf-8") as f2, \
         open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt", "a", encoding="utf-8") as f3:
        for orig, trans in zip(originals, translations):
            f1.write(orig + "\n")
            f2.write(trans + "\n")
            f3.write(f"Original: {orig}\nTranslation: {trans}\n---\n")

"""#### Translating para-nmt-50m-small dataset - executing for 59m 40s"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translator = load_model()
input_file_path = "/content/drive/My Drive/Colab Notebooks/para-nmt-50m-small.txt"
select_and_translate(input_file_path, translator, "para-nmt-50m")

"""#### Indexing the aligned pairs - additionally done ofr para-nmt-50m"""

def add_indices_to_pairs(aligned_pairs_path):
    # Read the entire file
    with open(aligned_pairs_path, 'r', encoding='utf-8') as file:
        content = file.read()

    # Split the content into pairs based on the separator
    pairs = content.split('---\n')

    # Process each pair and add indices
    indexed_content = ''
    for idx, pair in enumerate(pairs, 1):
        if not pair.strip():  # Skip empty pairs
            continue

        # Add index to the Original line
        if 'Original:' in pair:
            pair = pair.replace('Original:', f'Original [{idx}]:')

        # Add the separator back except for the last pair
        indexed_content += pair + ('---\n' if idx < len(pairs) else '')

    # Write back to file
    with open(aligned_pairs_path + '.indexed', 'w', encoding='utf-8') as file:
        file.write(indexed_content)

    print(f"Added indices to {len(pairs)} pairs")
    return aligned_pairs_path + '.indexed'

# Usage example:
file_path = "/content/drive/My Drive/Colab Notebooks/para-nmt-50m_aligned_pairs.txt"
indexed_file = add_indices_to_pairs(file_path)

"""## ppdb-1.0-s-m2o

#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases
"""

# new code for more memory-efficient approach using iterators to save the matching pairs

import torch
import numpy as np
from transformers import pipeline

def load_model():
    model_id = "dvres/GaMS-1B-Translator_0.1"
    translator = pipeline(
        "text-generation",
        model=model_id,
        device_map="auto",
        token="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"
    )
    return translator

def translate_batch(texts, translator):
    translations = []
    for text in texts:
        messages = [{"role": "english", "content": text}]
        response = translator(
            messages,
            max_new_tokens=512
        )[0]["generated_text"][-1]["content"]
        translations.append(response)
    return translations

def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):
    with open(input_file_path, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)

    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))
    selected_texts = []

    with open(input_file_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i in selected_indices:
                selected_texts.append(line.strip())
                if len(selected_texts) == batch_size:
                    try:
                        translations = translate_batch(selected_texts, translator)
                        save_pairs(selected_texts, translations, dataset_name)
                        print(f"Processed batch: {len(selected_texts)} lines")
                        selected_texts = []
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e:
                        print(f"Error processing batch: {str(e)}")
                        continue

    if selected_texts:
        try:
            translations = translate_batch(selected_texts, translator)
            save_pairs(selected_texts, translations)
            print(f"Processed remaining {len(selected_texts)} lines")
        except Exception as e:
            print(f"Error processing final batch: {str(e)}")

def save_pairs(originals, translations, dataset_name):
    with open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt", "a", encoding="utf-8") as f1, \
         open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt", "a", encoding="utf-8") as f2, \
         open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt", "a", encoding="utf-8") as f3:
        for orig, trans in zip(originals, translations):
            f1.write(orig + "\n")
            f2.write(trans + "\n")
            f3.write(f"Original: {orig}\nTranslation: {trans}\n---\n")

"""#### Translating ppdb-1.0-s-m2o - executing for 1h 9m 30s



"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translator = load_model()
input_file_path = "/content/drive/My Drive/Colab Notebooks/ppdb-1.0-s-m2o"
select_and_translate(input_file_path, translator, "ppdb-1.0-s-m2o")

"""## paws_train.tsv"""

# Code for translating TSV dataset with batching and checkpoints

from typing import Iterator, List, Dict, Any, Tuple, Optional
import numpy as np
import json
import os
from pathlib import Path
from datetime import datetime
from openai import OpenAI
from openai.types.chat import ChatCompletion
import time

class TranslationManager:
    def __init__(self, api_key: str, cache_dir: str = "translation_cache", checkpoint_dir: str = "checkpoints"):
        self.client = OpenAI(api_key=api_key)
        self.cache_dir = Path(cache_dir)
        self.checkpoint_dir = Path(checkpoint_dir)
        self.cache_dir.mkdir(exist_ok=True)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.batch_cache = {}
        self.current_cache_size = 0
        self.max_cache_size = 1000

    def _get_cache_file(self, batch_id: str) -> Path:
        return self.cache_dir / f"cache_{batch_id}.json"

    def get_checkpoint_file(self, dataset_name: str) -> Path:
        return self.checkpoint_dir / f"{dataset_name}_checkpoint.json"

    def _save_batch_cache(self, batch_id: str):
        cache_file = self._get_cache_file(batch_id)
        with open(cache_file, 'w', encoding='utf-8') as f:
            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)
        self.batch_cache = {}
        self.current_cache_size = 0

    def _load_cache(self, batch_id: str) -> Dict:
        cache_file = self._get_cache_file(batch_id)
        if cache_file.exists():
            with open(cache_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {}

    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):
        checkpoint_data = {
            "last_batch": batch_num,
            "translations_count": translations_count,
            "timestamp": datetime.now().isoformat()
        }
        checkpoint_file = self.get_checkpoint_file(dataset_name)
        with open(checkpoint_file, 'w', encoding='utf-8') as f:
            json.dump(checkpoint_data, f, indent=2)

    def load_checkpoint(self, dataset_name: str) -> Dict:
        checkpoint_file = self.get_checkpoint_file(dataset_name)
        if checkpoint_file.exists():
            with open(checkpoint_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {"last_batch": -1, "translations_count": 0}

    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:
        if not texts:
            return [], None

        try:
            cache = self._load_cache(batch_id)
            translations = []
            uncached_texts = []
            uncached_indices = []

            for i, text in enumerate(texts):
                text = text.strip()
                if not text:  # Skip empty strings
                    translations.append("")
                    continue
                if text in cache:
                    translations.append(cache[text])
                else:
                    uncached_texts.append(text)
                    uncached_indices.append(i)

            if uncached_texts:
                try:
                    response = self.client.chat.completions.create(
                        model="gpt-3.5-turbo",
                        messages=[
                            {"role": "system", "content": "Translate the following English texts to Slovenian:"},
                            {"role": "user", "content": "\n-\n".join(uncached_texts)}
                        ],
                        temperature=0.3
                    )

                    new_translations = [choice.message.content for choice in response.choices]

                    # Ensure we have the same number of translations as input texts
                    if len(new_translations) != len(uncached_texts):
                        new_translations = new_translations[:len(uncached_texts)]
                        if len(new_translations) < len(uncached_texts):
                            new_translations.extend([""] * (len(uncached_texts) - len(new_translations)))

                    for text, trans in zip(uncached_texts, new_translations):
                        self.batch_cache[text] = trans
                        self.current_cache_size += 1

                    for idx, trans in zip(uncached_indices, new_translations):
                        translations.insert(idx, trans)

                    if self.current_cache_size >= self.max_cache_size:
                        self._save_batch_cache(batch_id)

                    return translations, response

                except Exception as e:
                    print(f"Error in API call: {str(e)}")
                    return [f"ERROR: {str(e)}"] * len(texts), None

            return translations, None

        except Exception as e:
            print(f"Error in batch processing: {str(e)}")
            return [f"ERROR: {str(e)}"] * len(texts), None

class DatasetIterator:
    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1, max_sentences: int = 10949):
        self.file_path = file_path
        self.batch_size = batch_size
        self.text_column_index = text_column_index  # For TSV files, specify which column contains the text
        self.max_sentences = max_sentences
        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines
        # Add 1 to account for header in TSV
        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))

    def _count_lines(self) -> int:
        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                return sum(1 for _ in f)
        except Exception as e:
            print(f"Error counting lines: {str(e)}")
            return 0

    def __iter__(self) -> Iterator[List[str]]:
        current_batch = []
        processed_lines = 0

        try:
            with open(self.file_path, "r", encoding="utf-8") as f:
                # Skip header
                header = next(f, None)
                if not header:
                    raise ValueError("Empty file or no header found")

                # Skip to start line (accounting for already skipped header)
                for _ in range(self.start_line - 1):
                    next(f, None)

                for line in f:
                    if processed_lines >= self.max_sentences:
                        break

                    try:
                        columns = line.strip().split('\t')
                        if len(columns) > self.text_column_index:
                            text = columns[self.text_column_index].strip()
                            if text:  # Only add non-empty texts
                                current_batch.append(text)
                                processed_lines += 1

                                if len(current_batch) == self.batch_size:
                                    yield current_batch
                                    current_batch = []
                    except Exception as e:
                        print(f"Error processing line: {line.strip()}")
                        print(f"Error details: {str(e)}")
                        continue

                if current_batch:  # Don't forget last partial batch
                    yield current_batch

        except Exception as e:
            print(f"Error reading file: {str(e)}")
            if current_batch:  # Yield any remaining batch on error
                yield current_batch

class CostTracker:
    def __init__(self):
        self.requests = 0
        self.total_tokens = 0
        self.price_per_1k_tokens = 0.002

    def update(self, response: ChatCompletion):
        self.requests += 1
        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens

    def get_cost(self) -> float:
        return (self.total_tokens / 1000) * self.price_per_1k_tokens

    def report(self) -> str:
        return f"""
        API Calls: {self.requests}
        Total Tokens: {self.total_tokens}
        Estimated Cost: ${self.get_cost():.2f}
        """

def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = "a"):
    base_path = Path("/content/drive/My Drive/Colab Notebooks")

    for filename, data in [
        (f"{dataset_name}_originals_GPT3.5.txt", originals),
        (f"{dataset_name}_translations_GPT3.5.txt", translations),
    ]:
        try:
            with open(base_path / filename, mode, encoding="utf-8") as f:
                for item in data:
                    f.write(f"{item}\n")
        except Exception as e:
            print(f"Error saving to {filename}: {str(e)}")

    try:
        with open(base_path / f"{dataset_name}_aligned_pairs_GPT3.5.txt", mode, encoding="utf-8") as f:
            for orig, trans in zip(originals, translations):
                f.write(f"Original: {orig}\nTranslation: {trans}\n---\n")
    except Exception as e:
        print(f"Error saving aligned pairs: {str(e)}")

def process_dataset(
    input_file: str,
    dataset_name: str,
    api_key: str,
    batch_size: int = 32,
    checkpoint_interval: int = 5,
    max_sentences: int = 49402,
    text_column_index: int = 1
):
    translator = TranslationManager(api_key=api_key)

    # Load checkpoint if exists
    checkpoint = translator.load_checkpoint(dataset_name)
    start_batch = checkpoint["last_batch"] + 1
    translations_count = checkpoint["translations_count"]

    # Calculate starting line
    start_line = start_batch * batch_size

    # Create iterator with sentence limit and TSV column index
    dataset_iterator = DatasetIterator(
        file_path=input_file,
        batch_size=batch_size,
        start_line=start_line,
        text_column_index=text_column_index,
        max_sentences=max_sentences
    )

    cost_tracker = CostTracker()
    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)

    print(f"Starting from batch {start_batch}, line {start_line}")
    print(f"Will process up to {max_sentences} sentences")
    print(f"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}")
    print(f"Reading text from column index: {text_column_index}")

    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):
        try:
            batch_id = f"{dataset_name}_{batch_num}"
            translations, response = translator.translate_batch(batch, batch_id)

            if response is not None:
                cost_tracker.update(response)

            save_pairs(batch, translations, dataset_name, mode="a")
            translations_count += len(translations)

            if batch_num % checkpoint_interval == 0:
                translator.save_checkpoint(dataset_name, batch_num, translations_count)
                print(f"Checkpoint saved at batch {batch_num}")

            print(f"Processed batch {batch_num}/{total_batches} ({len(batch)} items)")
            if batch_num % 10 == 0:
                print(cost_tracker.report())

            # Add a small delay to avoid rate limiting
            time.sleep(0.5)

        except Exception as e:
            print(f"Error processing batch {batch_num}: {str(e)}")
            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)
            time.sleep(1)  # Longer delay on error
            continue

    # Final checkpoint and report
    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)
    print("\nFinal Statistics:")
    print(cost_tracker.report())
    print(f"Total translations: {translations_count}")

"""#### Execution of the code"""

if __name__ == "__main__":
    input_file = "/content/drive/My Drive/Colab Notebooks/paws_train.tsv"
    dataset_name = "paws_paraphrase_test"
    max_sentences = 49402  # Total number of sentences to process

    try:
        from google.colab import userdata
        api_key = userdata.get('OPENAI_API_KEY')
    except ImportError:
        api_key = os.getenv('OPENAI_API_KEY')

    if not api_key:
        raise ValueError("API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.")

    process_dataset(
        input_file=input_file,
        dataset_name=dataset_name,
        api_key=api_key,
        batch_size=32,
        checkpoint_interval=10,  # Increased for larger dataset
        max_sentences=max_sentences,
        text_column_index=1  # Specify which column contains the text to translate
    )

"""#### Check the number of lines in the file"""

# Chech the size of the file

from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/My Drive/Colab Notebooks/paws_train.tsv"

# Check if file exists
import os
if os.path.exists(file_path):
    print("File found!")
else:
    print("File not found. Check the path!")

file_path = "/content/drive/My Drive/Colab Notebooks/paws_train.tsv"

# Count lines in the file
with open(file_path, "r", encoding="utf-8") as file:
    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines

print(f"The file has {line_count} rows.")

import torch
from transformers import pipeline
import random
from tqdm import tqdm  # Adding progress bar

# Function to load model
def load_model():
    model_id = "dvres/GaMS-1B-Translator_0.1"
    translator = pipeline(
        "text-generation",
        model=model_id,
        device_map="auto",
        token="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"
    )
    return translator

# Batch translation function
def translate_batch(texts, translator):
    translations = []
    for text in texts:
        messages = [{"role": "english", "content": text}]
        response = translator(
            messages,
            max_new_tokens=512
        )[0]["generated_text"][-1]["content"]
        translations.append(response)
    return translations

# File paths
input_file_path = "/content/drive/My Drive/Colab Notebooks/paws_train.tsv"
output_file_path = "/content/drive/My Drive/Colab Notebooks/translated_paws_train.tsv"

# Setup
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Load model
translator = load_model()
print("Model loaded successfully")

# Parameters
num_samples = 10000
batch_size = 32

try:
    # First, read all lines and select random samples
    print("Reading file and selecting samples...")
    with open(input_file_path, "r", encoding="utf-8") as infile:
        all_lines = [line.strip() for line in infile if line.strip()]

    # Select random samples
    total_lines = len(all_lines)
    print(f"Total lines in file: {total_lines}")

    selected_lines = random.sample(all_lines, num_samples)
    print(f"Selected {num_samples} random lines for translation")

    # Process selected lines in batches
    batches = [selected_lines[i:i + batch_size] for i in range(0, len(selected_lines), batch_size)]

    # Translate and save
    with open(output_file_path, "w", encoding="utf-8") as outfile:
        for batch_num, batch in enumerate(tqdm(batches, desc="Translating")):
            try:
                translated_batch = translate_batch(batch, translator)
                outfile.write("\n".join(translated_batch) + "\n")

                # Clear CUDA cache periodically
                if torch.cuda.is_available() and batch_num % 10 == 0:
                    torch.cuda.empty_cache()

            except Exception as e:
                print(f"Error processing batch {batch_num}: {str(e)}")
                continue

    print(f"Translation complete! Translated file saved to: {output_file_path}")

except Exception as e:
    print(f"An error occurred: {str(e)}")

# Optional: Save indices of selected lines for reproducibility
with open(output_file_path + ".indices", "w", encoding="utf-8") as f:
    for i, line in enumerate(selected_lines):
        f.write(f"{i}\t{line}\n")

"""# Quora Duplicate Questions

#### Checking the size of the file
"""

# Check the size of the file

from google.colab import drive
drive.mount('/content/drive')

file_path = "/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv"

# Check if file exists
import os
if os.path.exists(file_path):
    print("File found!")
else:
    print("File not found. Check the path!")

file_path = "/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv"

# Count lines in the file
with open(file_path, "r", encoding="utf-8") as file:
    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines

print(f"The file has {line_count} rows.")

"""#### New code - optimizing memory-efficient random sampling, and aligning pairs of original and translated cases"""

# new code for more memory-efficient approach using iterators to save the matching pairs

import torch
import numpy as np
from transformers import pipeline

def load_model():
    model_id = "dvres/GaMS-1B-Translator_0.1"
    translator = pipeline(
        "text-generation",
        model=model_id,
        device_map="auto",
        token="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"
    )
    return translator

def translate_batch(texts, translator):
    translations = []
    for text in texts:
        messages = [{"role": "english", "content": text}]
        response = translator(
            messages,
            max_new_tokens=512
        )[0]["generated_text"][-1]["content"]
        translations.append(response)
    return translations

def select_and_translate(input_file_path, translator, dataset_name, n_samples=10000, batch_size=32):
    with open(input_file_path, "r", encoding="utf-8") as f:
        total_lines = sum(1 for _ in f)

    selected_indices = set(np.random.choice(total_lines, n_samples, replace=False))
    selected_texts = []

    with open(input_file_path, "r", encoding="utf-8") as f:
        for i, line in enumerate(f):
            if i in selected_indices:
                selected_texts.append(line.strip())
                if len(selected_texts) == batch_size:
                    try:
                        translations = translate_batch(selected_texts, translator)
                        save_pairs(selected_texts, translations, dataset_name)
                        print(f"Processed batch: {len(selected_texts)} lines")
                        selected_texts = []
                        if torch.cuda.is_available():
                            torch.cuda.empty_cache()
                    except Exception as e:
                        print(f"Error processing batch: {str(e)}")
                        continue

    if selected_texts:
        try:
            translations = translate_batch(selected_texts, translator)
            save_pairs(selected_texts, translations)
            print(f"Processed remaining {len(selected_texts)} lines")
        except Exception as e:
            print(f"Error processing final batch: {str(e)}")

def save_pairs(originals, translations, dataset_name):
    with open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_originals.txt", "a", encoding="utf-8") as f1, \
         open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_selected_translations.txt", "a", encoding="utf-8") as f2, \
         open(f"/content/drive/My Drive/Colab Notebooks/{dataset_name}_aligned_pairs.txt", "a", encoding="utf-8") as f3:
        for orig, trans in zip(originals, translations):
            f1.write(orig + "\n")
            f2.write(trans + "\n")
            f3.write(f"Original: {orig}\nTranslation: {trans}\n---\n")

"""#### Translating quora duplicate questions - executing for



"""

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
translator = load_model()
input_file_path = "/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv"
select_and_translate(input_file_path, translator, "quora_duplicate_questions.tsv")