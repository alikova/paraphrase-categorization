{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7cJnBTxBa3h"
      },
      "source": [
        "#### Optimized code for translation (*test_code_Paraphrase Datasets Translations - GaMS.ipynb*) with OpenAI model. This script uses cache patterns and batch processing inside of pipeline programming, enabling parallel processing of translation batches. As a result, the API calls are optimized, and if an error occurs during the translation of a single sentence, the entire batch is not lost. We are using ChatGPT-3.5 via the OpenAI API key."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9NLS82cGq5F"
      },
      "source": [
        "## Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msAJWILyGttj",
        "outputId": "fdd86bd1-b88b-41eb-ca53-2ef72087c796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rUN7eCZBkXU"
      },
      "source": [
        "## Git add and commit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9s71E83_BkXU",
        "outputId": "9006744d-0ddc-46c5-fe74-4113503c0063"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'paraphrase-categorization'...\n",
            "remote: Enumerating objects: 164, done.\u001b[K\n",
            "remote: Counting objects: 100% (21/21), done.\u001b[K\n",
            "remote: Compressing objects: 100% (16/16), done.\u001b[K\n",
            "remote: Total 164 (delta 6), reused 18 (delta 5), pack-reused 143 (from 1)\u001b[K\n",
            "Receiving objects: 100% (164/164), 67.99 MiB | 14.12 MiB/s, done.\n",
            "Resolving deltas: 100% (67/67), done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://alikova: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@github.com/alikova/paraphrase-categorization.git\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tAfkbrc3BkXV",
        "outputId": "42ebb6ca-7e12-4271-c0f6-272ff5c35fd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "drive  paraphrase-categorization  sample_data\n"
          ]
        }
      ],
      "source": [
        "!ls /content\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tcdAMSyBkXV",
        "outputId": "361d94f1-3fcc-47bb-8d40-e674a680a1af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/paraphrase-categorization\n",
            "/content/paraphrase-categorization\n"
          ]
        }
      ],
      "source": [
        "%cd /content/paraphrase-categorization\n",
        "!pwd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uVLQ8AlpBkXW",
        "outputId": "a3e17f82-d583-4d69-a1a2-c9ae71bd1f57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/paraphrase-categorization/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
            "/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
            " kaggle_prevajajanje_zbirk.html\n",
            " kategorije_parafraz_draft\n",
            " library_ontology\n",
            "'Load and translate Paraphrases with GaMS _ Kaggle.html'\n",
            " paraphrase_categorization\n",
            " paraphrase_datasets_translation_GaMS_gpu\n",
            " Paraphrase_Datasets_Translations_GaMS.ipynb\n",
            " Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n",
            " Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
            " paraphrase_datasets_translations_gpt3_5.py\n",
            " paraphrase_read-and-translate.py\n",
            " README.md\n",
            "'Colab Notebooks'   Translated_Datasets\n"
          ]
        }
      ],
      "source": [
        "!find /content -name \"Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n",
        "\n",
        "!ls /content/paraphrase-categorization/\n",
        "\n",
        "!ls /content/drive/MyDrive/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLpfCqYYHay-",
        "outputId": "5cf0b735-a48d-4853-fe71-d11e2238d1ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\n"
          ]
        }
      ],
      "source": [
        "!find /content/drive/ -name \"Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HlEcO_pxBkXW",
        "outputId": "a882faa2-f085-4ba3-f4e4-82132c0a2693"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " kaggle_prevajajanje_zbirk.html\n",
            " kategorije_parafraz_draft\n",
            " library_ontology\n",
            "'Load and translate Paraphrases with GaMS _ Kaggle.html'\n",
            " paraphrase_categorization\n",
            " paraphrase_datasets_translation_GaMS_gpu\n",
            " Paraphrase_Datasets_Translations_GaMS.ipynb\n",
            " Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n",
            " Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
            " paraphrase_datasets_translations_gpt3_5.py\n",
            " paraphrase_read-and-translate.py\n",
            " README.md\n",
            "/content/paraphrase-categorization\n"
          ]
        }
      ],
      "source": [
        "!cp \"/content/drive/MyDrive/Colab Notebooks/Paraphrase_Datasets_Translations_GPT3.5.ipynb\" /content/paraphrase-categorization/\n",
        "\n",
        "!ls /content/paraphrase-categorization/\n",
        "%cd /content/paraphrase-categorization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sZYez0CLBkXX",
        "outputId": "5a42df00-7312-457d-ecb1-89b27ee0b8e1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " .\n",
            " ..\n",
            " .DS_Store\n",
            " .git\n",
            " kaggle_prevajajanje_zbirk.html\n",
            " kategorije_parafraz_draft\n",
            " library_ontology\n",
            "'Load and translate Paraphrases with GaMS _ Kaggle.html'\n",
            " paraphrase_categorization\n",
            " paraphrase_datasets_translation_GaMS_gpu\n",
            " Paraphrase_Datasets_Translations_GaMS.ipynb\n",
            " Paraphrase_Datasets_Translations_GPT3.5_GPU.ipynb\n",
            " Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
            " paraphrase_datasets_translations_gpt3_5.py\n",
            " paraphrase_read-and-translate.py\n",
            " README.md\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "Changes to be committed:\n",
            "  (use \"git restore --staged <file>...\" to unstage)\n",
            "\t\u001b[32mmodified:   Paraphrase_Datasets_Translations_GPT3.5.ipynb\u001b[m\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!ls -a\n",
        "\n",
        "!git add Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
        "!git status\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "95QXRLcUBkXX"
      },
      "outputs": [],
      "source": [
        "!git config --global user.email \"z.alenka7@gmail.com\"\n",
        "!git config --global user.name \"alikova\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GcR-ZHLjBkXX",
        "outputId": "2106a49c-d340-42ae-cbc4-82258287bbd8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 64dbae0] Update Paraphrase_Datasets_Translations_GPT3.5.ipynb\n",
            " 1 file changed, 1 insertion(+), 1 deletion(-)\n"
          ]
        }
      ],
      "source": [
        "!git commit -m \"Update Paraphrase_Datasets_Translations_GPT3.5.ipynb\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sj0U_6hoBkXY",
        "outputId": "1b26b42f-8f38-4eb6-ba6f-ff5680e3cd31"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enumerating objects: 5, done.\n",
            "Counting objects:  20% (1/5)\rCounting objects:  40% (2/5)\rCounting objects:  60% (3/5)\rCounting objects:  80% (4/5)\rCounting objects: 100% (5/5)\rCounting objects: 100% (5/5), done.\n",
            "Delta compression using up to 8 threads\n",
            "Compressing objects:  33% (1/3)\rCompressing objects:  66% (2/3)\rCompressing objects: 100% (3/3)\rCompressing objects: 100% (3/3), done.\n",
            "Writing objects:  33% (1/3)\rWriting objects:  66% (2/3)\rWriting objects: 100% (3/3)\rWriting objects: 100% (3/3), 3.59 KiB | 612.00 KiB/s, done.\n",
            "Total 3 (delta 2), reused 0 (delta 0), pack-reused 0\n",
            "remote: Resolving deltas: 100% (2/2), completed with 2 local objects.\u001b[K\n",
            "To https://github.com/alikova/paraphrase-categorization.git\n",
            "   d4dd1c1..64dbae0  main -> main\n"
          ]
        }
      ],
      "source": [
        "!git push origin main  # or the appropriate branch name if it's not 'main'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-BPcvwxZBkXY"
      },
      "source": [
        "## Installations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cftOsVoceHhc",
        "outputId": "3d166a23-57c1-4b2b-9b30-15fed7ed3e54"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: openai 1.61.1\n",
            "Uninstalling openai-1.61.1:\n",
            "  Would remove:\n",
            "    /usr/local/bin/openai\n",
            "    /usr/local/lib/python3.11/dist-packages/openai-1.61.1.dist-info/*\n",
            "    /usr/local/lib/python3.11/dist-packages/openai/*\n",
            "Proceed (Y/n)? Y\n",
            "  Successfully uninstalled openai-1.61.1\n"
          ]
        }
      ],
      "source": [
        "pip uninstall openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "umygU4wve8r0",
        "outputId": "202d4756-e6c7-45cb-c832-525e4fb6c194"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found existing installation: openai 1.61.1\n",
            "Uninstalling openai-1.61.1:\n",
            "  Successfully uninstalled openai-1.61.1\n"
          ]
        }
      ],
      "source": [
        "!pip uninstall -y openai\n",
        "!pip install openai>=1.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lqjl4jP7_R7t",
        "outputId": "426e3c3b-dd3e-4520-a067-60e9af424082"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.61.1\n"
          ]
        }
      ],
      "source": [
        "#!pip uninstall -y openai\n",
        "!pip install openai>=1.0.0\n",
        "!python -c \"import openai; print(openai.__version__)\"  # Should print 1.x.x"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sX-obQ9KBkXZ"
      },
      "source": [
        "## Connect to OpenAI with an API key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rZdz89fScAR7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import openai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaQuw2A9BSvb"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "#userdata.get('OPENAI_API_KEY')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wGD1XIodUgT",
        "outputId": "ddcedc92-07b3-4c44-8f4c-6e4e5cc59af9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "API key successfully loaded\n"
          ]
        }
      ],
      "source": [
        "# Retrieve the API key\n",
        "api_key = userdata.get(\"OPENAI_API_KEY\")\n",
        "\n",
        "# Verify if the key exists (good practice)\n",
        "if api_key is None:\n",
        "    raise ValueError(\"API key not found in environment variables\")\n",
        "\n",
        "print(\"API key successfully loaded\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "evqZmL5YhEIk"
      },
      "outputs": [],
      "source": [
        "# This will retrieve the key you configured in Colab's secrets\n",
        "api_key = userdata.get('OPENAI_API_KEY')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o3fsR0bCmXa5"
      },
      "outputs": [],
      "source": [
        "# Create an OpenAI client (New API format)\n",
        "from openai import OpenAI  # We'll use synchronous version instead\n",
        "\n",
        "# Instead of AsyncOpenAI, we'll modify our code to use the synchronous version:\n",
        "client = OpenAI(api_key=api_key)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yBzGPkvFgr2w",
        "outputId": "fe35311e-4649-4b4f-9b4d-36d39bad57db"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Why was the math book sad?\n",
            "\n",
            "Because it had too many problems.\n",
            "27\n"
          ]
        }
      ],
      "source": [
        "# Example OpenAI API request\n",
        "response = client.chat.completions.create(\n",
        "    model=\"gpt-3.5-turbo\",\n",
        "    messages=[{\"role\": \"user\", \"content\": \"Tell me a joke!\"}]\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)\n",
        "print(response.usage.total_tokens)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9_PrDfecBkXb"
      },
      "source": [
        "# Translate with Pipeline Programming"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lq-c_WA97e72"
      },
      "source": [
        "#####           Lower Temperature (closer to 0): The model will be more deterministic and predictable, choosing the words with the highest probability. It will tend to produce more repetitive and safe outputs. This is often preferred for tasks where accuracy and consistency are paramount, such as translation or factual question answering. Higher Temperature (closer to 1 or above): The model becomes more creative and unpredictable. It's more likely to sample from less probable words, leading to more diverse and unexpected outputs. This is suitable for tasks where creativity and novelty are desired, such as creative writing or brainstorming.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4FhHiV9ewezJ"
      },
      "source": [
        "#### Code without asynchronous function - testing the translation on the first n sentences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pj2w8X4eDJnm"
      },
      "outputs": [],
      "source": [
        "# Code is working, it fecthes first n sentences\n",
        "\n",
        "from typing import Iterator, List, Dict, Any, Tuple, Optional\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = []\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if not text:  # Skip empty strings\n",
        "                    translations.append(\"\")\n",
        "                    continue\n",
        "                if text in cache:\n",
        "                    translations.append(cache[text])\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                try:\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                            {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n",
        "                        ],\n",
        "                        temperature=0.3\n",
        "                    )\n",
        "\n",
        "                    new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                    # Ensure we have the same number of translations as input texts\n",
        "                    if len(new_translations) != len(uncached_texts):\n",
        "                        new_translations = new_translations[:len(uncached_texts)]\n",
        "                        if len(new_translations) < len(uncached_texts):\n",
        "                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                    for text, trans in zip(uncached_texts, new_translations):\n",
        "                        self.batch_cache[text] = trans\n",
        "                        self.current_cache_size += 1\n",
        "\n",
        "                    for idx, trans in zip(uncached_indices, new_translations):\n",
        "                        translations.insert(idx, trans)\n",
        "\n",
        "                    if self.current_cache_size >= self.max_cache_size:\n",
        "                        self._save_batch_cache(batch_id)\n",
        "\n",
        "                    return translations, response\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in API call: {str(e)}\")\n",
        "                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 100):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[str]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip to start line\n",
        "                for _ in range(self.start_line):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    line = line.strip()\n",
        "                    if line:  # Only add non-empty lines\n",
        "                        current_batch.append(line)\n",
        "                        processed_lines += 1\n",
        "\n",
        "                        if len(current_batch) == self.batch_size:\n",
        "                            yield current_batch\n",
        "                            current_batch = []\n",
        "\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 10949 # Parameter to limit number of sentences in the dataset - write manually since this code is working okay\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translations_count = checkpoint[\"translations_count\"]\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    # Create iterator with sentence limit\n",
        "    dataset_iterator = DatasetIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_line=start_line,\n",
        "        max_sentences=max_sentences\n",
        "    )\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    print(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    print(f\"Will process up to {max_sentences} sentences\")\n",
        "    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                print(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                print(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    print(\"\\nFinal Statistics:\")\n",
        "    print(cost_tracker.report())\n",
        "    print(f\"Total translations: {translations_count}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gLFdSOLe2D1F"
      },
      "outputs": [],
      "source": [
        "# try\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n",
        "    dataset_name = \"msr_paraphrase_test\"  # Using test suffix to distinguish from full runs\n",
        "    max_sentences = 10949  # Parameter to limit number of sentences in the dataset - write manually since this code is working okay\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,  # Keeping original batch size\n",
        "        checkpoint_interval=10,  # Frequent checkpoints for testing\n",
        "        max_sentences=10949  # Parameter to limit number of sentences in the dataset - write manually since this code is working okay\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jCSpfR0I_0zc"
      },
      "source": [
        "#### Code with asynchronous function - not working"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5De-X5UnN-8"
      },
      "outputs": [],
      "source": [
        "# Throving an error\n",
        "\n",
        "from typing import Iterator, List, Dict, Any, Tuple, Optional\n",
        "from openai import OpenAI\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import time\n",
        "import os\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        #self.api_key = api_key  # Store the API key\n",
        "        self.client = OpenAI(api_key=api_key)  # Initialize client properly\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = []\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if not text:  # Skip empty strings\n",
        "                    translations.append(\"\")\n",
        "                    continue\n",
        "                if text in cache:\n",
        "                    translations.append(cache[text])\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                try:\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                            {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n",
        "                        ],\n",
        "                        temperature=0.3\n",
        "                    )\n",
        "\n",
        "                    # Get translations from response\n",
        "                    new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                    # Ensure we have the same number of translations as input texts\n",
        "                    if len(new_translations) != len(uncached_texts):\n",
        "                        new_translations = new_translations[:len(uncached_texts)]\n",
        "                        if len(new_translations) < len(uncached_texts):\n",
        "                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                    # Update cache\n",
        "                    for text, trans in zip(uncached_texts, new_translations):\n",
        "                        self.batch_cache[text] = trans\n",
        "                        self.current_cache_size += 1\n",
        "\n",
        "                    # Insert translations at correct positions\n",
        "                    for idx, trans in zip(uncached_indices, new_translations):\n",
        "                        translations.insert(idx, trans)\n",
        "\n",
        "                    if self.current_cache_size >= self.max_cache_size:\n",
        "                        self._save_batch_cache(batch_id)\n",
        "\n",
        "                    return translations, response\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in API call: {str(e)}\")\n",
        "                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = []\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if not text:  # Skip empty strings\n",
        "                    translations.append(\"\")\n",
        "                    continue\n",
        "                if text in cache:\n",
        "                    translations.append(cache[text])\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                try:\n",
        "                    response = self.client.chat.completions.create(\n",
        "                        model=\"gpt-3.5-turbo\",\n",
        "                        messages=[\n",
        "                            {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                            {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n",
        "                        ],\n",
        "                        temperature=0.3\n",
        "                    )\n",
        "\n",
        "                    # new_translations = response.choices[0].message.content.split(\"\\n---\\n\") # for version 0.28\n",
        "                    new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                    # Ensure we have the same number of translations as input texts\n",
        "                    if len(new_translations) != len(uncached_texts):\n",
        "                        new_translations = new_translations[:len(uncached_texts)]\n",
        "                        if len(new_translations) < len(uncached_texts):\n",
        "                            new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                    for text, trans in zip(uncached_texts, new_translations):\n",
        "                        self.batch_cache[text] = trans\n",
        "                        self.current_cache_size += 1\n",
        "\n",
        "                    for idx, trans in zip(uncached_indices, new_translations):\n",
        "                        translations.insert(idx, trans)\n",
        "\n",
        "                    if self.current_cache_size >= self.max_cache_size:\n",
        "                        self._save_batch_cache(batch_id)\n",
        "\n",
        "                    return translations, response\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error in API call: {str(e)}\")\n",
        "                    return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.text_column_index = text_column_index  # Default to second column (index 1)\n",
        "        self.total_lines = self._count_lines()\n",
        "        # Add 1 to account for header\n",
        "        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[str]]:\n",
        "        current_batch = []\n",
        "        current_line = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip header\n",
        "                header = next(f, None)\n",
        "                if not header:\n",
        "                    raise ValueError(\"Empty file or no header found\")\n",
        "\n",
        "                # Skip to start line (accounting for already skipped header)\n",
        "                for _ in range(self.start_line - 1):\n",
        "                    next(f, None)\n",
        "                    current_line += 1\n",
        "\n",
        "                for line in f:\n",
        "                    try:\n",
        "                        columns = line.strip().split('\\t')\n",
        "                        if len(columns) > self.text_column_index:\n",
        "                            text = columns[self.text_column_index].strip()\n",
        "                            if text:  # Only add non-empty texts\n",
        "                                current_batch.append(text)\n",
        "                                if len(current_batch) == self.batch_size:\n",
        "                                    yield current_batch\n",
        "                                    current_batch = []\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing line: {line.strip()}\")\n",
        "                        print(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translations_count = checkpoint[\"translations_count\"]\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    dataset_iterator = DatasetIterator(input_file, batch_size, start_line)\n",
        "    cost_tracker = CostTracker()\n",
        "\n",
        "    total_batches = max(1, dataset_iterator.total_lines // batch_size)\n",
        "\n",
        "    print(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    print(f\"Total lines in file: {dataset_iterator.total_lines}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            # Only save if we got valid translations\n",
        "            if translations:\n",
        "                save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "                translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                print(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                print(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(5)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    print(\"\\nFinal Statistics:\")\n",
        "    print(cost_tracker.report())\n",
        "    print(f\"Total translations: {translations_count}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BCcetVRwBkXc"
      },
      "source": [
        "## **msr_paraphrase_data.txt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7Li0568BkXc"
      },
      "source": [
        "#### Checking the size of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HVE4UDXYBkXc",
        "outputId": "0e863ca1-6db2-418a-a059-f16b50fe1efc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found!\n",
            "The file has 10949 rows.\n"
          ]
        }
      ],
      "source": [
        "# Check the size of the file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n",
        "\n",
        "# Check if file exists\n",
        "import os\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found!\")\n",
        "else:\n",
        "    print(\"File not found. Check the path!\")\n",
        "\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n",
        "\n",
        "# Count lines in the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n",
        "\n",
        "print(f\"The file has {line_count} rows.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOjH7N941z9z"
      },
      "source": [
        "### MSR translation\n",
        "\n",
        "Final Statistics:\n",
        "\n",
        "        API Calls: 342\n",
        "        Total Tokens: 1248224\n",
        "        Estimated Cost: $2.50\n",
        "        Time: 52min\n",
        "        \n",
        "Total translations: 10927"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_5tHrPhDDvMM",
        "outputId": "e8e7c91a-552e-45f7-fa54-792b510426ae"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting from batch 1, line 32\n",
            "Will process up to 10949 sentences\n",
            "Total lines to process: 10949\n",
            "Processed batch 1/342 (32 items)\n",
            "Processed batch 2/342 (32 items)\n",
            "Processed batch 3/342 (32 items)\n",
            "Processed batch 4/342 (32 items)\n",
            "Processed batch 5/342 (32 items)\n",
            "Processed batch 6/342 (32 items)\n",
            "Processed batch 7/342 (32 items)\n",
            "Processed batch 8/342 (32 items)\n",
            "Processed batch 9/342 (32 items)\n",
            "Checkpoint saved at batch 10\n",
            "Processed batch 10/342 (32 items)\n",
            "\n",
            "        API Calls: 10\n",
            "        Total Tokens: 37928\n",
            "        Estimated Cost: $0.08\n",
            "        \n",
            "Processed batch 11/342 (32 items)\n",
            "Processed batch 12/342 (32 items)\n",
            "Processed batch 13/342 (32 items)\n",
            "Processed batch 14/342 (32 items)\n",
            "Processed batch 15/342 (32 items)\n",
            "Processed batch 16/342 (32 items)\n",
            "Processed batch 17/342 (32 items)\n",
            "Processed batch 18/342 (32 items)\n",
            "Processed batch 19/342 (32 items)\n",
            "Checkpoint saved at batch 20\n",
            "Processed batch 20/342 (32 items)\n",
            "\n",
            "        API Calls: 20\n",
            "        Total Tokens: 73322\n",
            "        Estimated Cost: $0.15\n",
            "        \n",
            "Processed batch 21/342 (32 items)\n",
            "Processed batch 22/342 (32 items)\n",
            "Processed batch 23/342 (32 items)\n",
            "Processed batch 24/342 (32 items)\n",
            "Processed batch 25/342 (32 items)\n",
            "Processed batch 26/342 (32 items)\n",
            "Processed batch 27/342 (32 items)\n",
            "Processed batch 28/342 (32 items)\n",
            "Processed batch 29/342 (32 items)\n",
            "Checkpoint saved at batch 30\n",
            "Processed batch 30/342 (32 items)\n",
            "\n",
            "        API Calls: 30\n",
            "        Total Tokens: 109329\n",
            "        Estimated Cost: $0.22\n",
            "        \n",
            "Processed batch 31/342 (32 items)\n",
            "Processed batch 32/342 (32 items)\n",
            "Processed batch 33/342 (32 items)\n",
            "Processed batch 34/342 (32 items)\n",
            "Processed batch 35/342 (32 items)\n",
            "Processed batch 36/342 (32 items)\n",
            "Processed batch 37/342 (32 items)\n",
            "Processed batch 38/342 (32 items)\n",
            "Processed batch 39/342 (32 items)\n",
            "Checkpoint saved at batch 40\n",
            "Processed batch 40/342 (32 items)\n",
            "\n",
            "        API Calls: 40\n",
            "        Total Tokens: 146025\n",
            "        Estimated Cost: $0.29\n",
            "        \n",
            "Processed batch 41/342 (32 items)\n",
            "Processed batch 42/342 (32 items)\n",
            "Processed batch 43/342 (32 items)\n",
            "Processed batch 44/342 (32 items)\n",
            "Processed batch 45/342 (32 items)\n",
            "Processed batch 46/342 (32 items)\n",
            "Processed batch 47/342 (32 items)\n",
            "Processed batch 48/342 (32 items)\n",
            "Processed batch 49/342 (32 items)\n",
            "Checkpoint saved at batch 50\n",
            "Processed batch 50/342 (32 items)\n",
            "\n",
            "        API Calls: 50\n",
            "        Total Tokens: 182819\n",
            "        Estimated Cost: $0.37\n",
            "        \n",
            "Processed batch 51/342 (32 items)\n",
            "Processed batch 52/342 (32 items)\n",
            "Processed batch 53/342 (32 items)\n",
            "Processed batch 54/342 (32 items)\n",
            "Processed batch 55/342 (32 items)\n",
            "Processed batch 56/342 (32 items)\n",
            "Processed batch 57/342 (32 items)\n",
            "Processed batch 58/342 (32 items)\n",
            "Processed batch 59/342 (32 items)\n",
            "Checkpoint saved at batch 60\n",
            "Processed batch 60/342 (32 items)\n",
            "\n",
            "        API Calls: 60\n",
            "        Total Tokens: 217586\n",
            "        Estimated Cost: $0.44\n",
            "        \n",
            "Processed batch 61/342 (32 items)\n",
            "Processed batch 62/342 (32 items)\n",
            "Processed batch 63/342 (32 items)\n",
            "Processed batch 64/342 (32 items)\n",
            "Processed batch 65/342 (32 items)\n",
            "Processed batch 66/342 (32 items)\n",
            "Processed batch 67/342 (32 items)\n",
            "Processed batch 68/342 (32 items)\n",
            "Processed batch 69/342 (32 items)\n",
            "Checkpoint saved at batch 70\n",
            "Processed batch 70/342 (32 items)\n",
            "\n",
            "        API Calls: 70\n",
            "        Total Tokens: 248324\n",
            "        Estimated Cost: $0.50\n",
            "        \n",
            "Processed batch 71/342 (32 items)\n",
            "Processed batch 72/342 (32 items)\n",
            "Processed batch 73/342 (32 items)\n",
            "Processed batch 74/342 (32 items)\n",
            "Processed batch 75/342 (32 items)\n",
            "Processed batch 76/342 (32 items)\n",
            "Processed batch 77/342 (32 items)\n",
            "Processed batch 78/342 (32 items)\n",
            "Processed batch 79/342 (32 items)\n",
            "Checkpoint saved at batch 80\n",
            "Processed batch 80/342 (32 items)\n",
            "\n",
            "        API Calls: 80\n",
            "        Total Tokens: 284445\n",
            "        Estimated Cost: $0.57\n",
            "        \n",
            "Processed batch 81/342 (32 items)\n",
            "Processed batch 82/342 (32 items)\n",
            "Processed batch 83/342 (32 items)\n",
            "Processed batch 84/342 (32 items)\n",
            "Processed batch 85/342 (32 items)\n",
            "Processed batch 86/342 (32 items)\n",
            "Processed batch 87/342 (32 items)\n",
            "Processed batch 88/342 (32 items)\n",
            "Processed batch 89/342 (32 items)\n",
            "Checkpoint saved at batch 90\n",
            "Processed batch 90/342 (32 items)\n",
            "\n",
            "        API Calls: 90\n",
            "        Total Tokens: 320208\n",
            "        Estimated Cost: $0.64\n",
            "        \n",
            "Processed batch 91/342 (32 items)\n",
            "Processed batch 92/342 (32 items)\n",
            "Processed batch 93/342 (32 items)\n",
            "Processed batch 94/342 (32 items)\n",
            "Processed batch 95/342 (32 items)\n",
            "Processed batch 96/342 (32 items)\n",
            "Processed batch 97/342 (32 items)\n",
            "Processed batch 98/342 (32 items)\n",
            "Processed batch 99/342 (32 items)\n",
            "Checkpoint saved at batch 100\n",
            "Processed batch 100/342 (32 items)\n",
            "\n",
            "        API Calls: 100\n",
            "        Total Tokens: 357002\n",
            "        Estimated Cost: $0.71\n",
            "        \n",
            "Processed batch 101/342 (32 items)\n",
            "Processed batch 102/342 (32 items)\n",
            "Processed batch 103/342 (32 items)\n",
            "Processed batch 104/342 (32 items)\n",
            "Processed batch 105/342 (32 items)\n",
            "Processed batch 106/342 (32 items)\n",
            "Processed batch 107/342 (32 items)\n",
            "Processed batch 108/342 (32 items)\n",
            "Processed batch 109/342 (32 items)\n",
            "Checkpoint saved at batch 110\n",
            "Processed batch 110/342 (32 items)\n",
            "\n",
            "        API Calls: 110\n",
            "        Total Tokens: 393340\n",
            "        Estimated Cost: $0.79\n",
            "        \n",
            "Processed batch 111/342 (32 items)\n",
            "Processed batch 112/342 (32 items)\n",
            "Processed batch 113/342 (32 items)\n",
            "Processed batch 114/342 (32 items)\n",
            "Processed batch 115/342 (32 items)\n",
            "Processed batch 116/342 (32 items)\n",
            "Processed batch 117/342 (32 items)\n",
            "Processed batch 118/342 (32 items)\n",
            "Processed batch 119/342 (32 items)\n",
            "Checkpoint saved at batch 120\n",
            "Processed batch 120/342 (32 items)\n",
            "\n",
            "        API Calls: 120\n",
            "        Total Tokens: 430714\n",
            "        Estimated Cost: $0.86\n",
            "        \n",
            "Processed batch 121/342 (32 items)\n",
            "Processed batch 122/342 (32 items)\n",
            "Processed batch 123/342 (32 items)\n",
            "Processed batch 124/342 (32 items)\n",
            "Processed batch 125/342 (32 items)\n",
            "Processed batch 126/342 (32 items)\n",
            "Processed batch 127/342 (32 items)\n",
            "Processed batch 128/342 (32 items)\n",
            "Processed batch 129/342 (32 items)\n",
            "Checkpoint saved at batch 130\n",
            "Processed batch 130/342 (32 items)\n",
            "\n",
            "        API Calls: 130\n",
            "        Total Tokens: 465011\n",
            "        Estimated Cost: $0.93\n",
            "        \n",
            "Processed batch 131/342 (32 items)\n",
            "Processed batch 132/342 (32 items)\n",
            "Processed batch 133/342 (32 items)\n",
            "Processed batch 134/342 (32 items)\n",
            "Processed batch 135/342 (32 items)\n",
            "Processed batch 136/342 (32 items)\n",
            "Processed batch 137/342 (32 items)\n",
            "Processed batch 138/342 (32 items)\n",
            "Processed batch 139/342 (32 items)\n",
            "Checkpoint saved at batch 140\n",
            "Processed batch 140/342 (32 items)\n",
            "\n",
            "        API Calls: 140\n",
            "        Total Tokens: 505688\n",
            "        Estimated Cost: $1.01\n",
            "        \n",
            "Processed batch 141/342 (32 items)\n",
            "Processed batch 142/342 (32 items)\n",
            "Processed batch 143/342 (32 items)\n",
            "Processed batch 144/342 (32 items)\n",
            "Processed batch 145/342 (32 items)\n",
            "Processed batch 146/342 (32 items)\n",
            "Processed batch 147/342 (32 items)\n",
            "Processed batch 148/342 (32 items)\n",
            "Processed batch 149/342 (32 items)\n",
            "Checkpoint saved at batch 150\n",
            "Processed batch 150/342 (32 items)\n",
            "\n",
            "        API Calls: 150\n",
            "        Total Tokens: 544197\n",
            "        Estimated Cost: $1.09\n",
            "        \n",
            "Processed batch 151/342 (32 items)\n",
            "Processed batch 152/342 (32 items)\n",
            "Processed batch 153/342 (32 items)\n",
            "Processed batch 154/342 (32 items)\n",
            "Processed batch 155/342 (32 items)\n",
            "Processed batch 156/342 (32 items)\n",
            "Processed batch 157/342 (32 items)\n",
            "Processed batch 158/342 (32 items)\n",
            "Processed batch 159/342 (32 items)\n",
            "Checkpoint saved at batch 160\n",
            "Processed batch 160/342 (32 items)\n",
            "\n",
            "        API Calls: 160\n",
            "        Total Tokens: 579162\n",
            "        Estimated Cost: $1.16\n",
            "        \n",
            "Processed batch 161/342 (32 items)\n",
            "Processed batch 162/342 (32 items)\n",
            "Processed batch 163/342 (32 items)\n",
            "Processed batch 164/342 (32 items)\n",
            "Processed batch 165/342 (32 items)\n",
            "Processed batch 166/342 (32 items)\n",
            "Processed batch 167/342 (32 items)\n",
            "Processed batch 168/342 (32 items)\n",
            "Processed batch 169/342 (32 items)\n",
            "Checkpoint saved at batch 170\n",
            "Processed batch 170/342 (32 items)\n",
            "\n",
            "        API Calls: 170\n",
            "        Total Tokens: 616551\n",
            "        Estimated Cost: $1.23\n",
            "        \n",
            "Processed batch 171/342 (32 items)\n",
            "Processed batch 172/342 (32 items)\n",
            "Processed batch 173/342 (32 items)\n",
            "Processed batch 174/342 (32 items)\n",
            "Processed batch 175/342 (32 items)\n",
            "Processed batch 176/342 (32 items)\n",
            "Processed batch 177/342 (32 items)\n",
            "Processed batch 178/342 (32 items)\n",
            "Processed batch 179/342 (32 items)\n",
            "Checkpoint saved at batch 180\n",
            "Processed batch 180/342 (32 items)\n",
            "\n",
            "        API Calls: 180\n",
            "        Total Tokens: 649967\n",
            "        Estimated Cost: $1.30\n",
            "        \n",
            "Processed batch 181/342 (32 items)\n",
            "Processed batch 182/342 (32 items)\n",
            "Processed batch 183/342 (32 items)\n",
            "Processed batch 184/342 (32 items)\n",
            "Processed batch 185/342 (32 items)\n",
            "Processed batch 186/342 (32 items)\n",
            "Processed batch 187/342 (32 items)\n",
            "Processed batch 188/342 (32 items)\n",
            "Processed batch 189/342 (32 items)\n",
            "Checkpoint saved at batch 190\n",
            "Processed batch 190/342 (32 items)\n",
            "\n",
            "        API Calls: 190\n",
            "        Total Tokens: 687474\n",
            "        Estimated Cost: $1.37\n",
            "        \n",
            "Processed batch 191/342 (32 items)\n",
            "Processed batch 192/342 (32 items)\n",
            "Processed batch 193/342 (32 items)\n",
            "Processed batch 194/342 (32 items)\n",
            "Processed batch 195/342 (32 items)\n",
            "Processed batch 196/342 (32 items)\n",
            "Processed batch 197/342 (32 items)\n",
            "Processed batch 198/342 (32 items)\n",
            "Processed batch 199/342 (32 items)\n",
            "Checkpoint saved at batch 200\n",
            "Processed batch 200/342 (32 items)\n",
            "\n",
            "        API Calls: 200\n",
            "        Total Tokens: 726060\n",
            "        Estimated Cost: $1.45\n",
            "        \n",
            "Processed batch 201/342 (32 items)\n",
            "Processed batch 202/342 (32 items)\n",
            "Processed batch 203/342 (32 items)\n",
            "Processed batch 204/342 (32 items)\n",
            "Processed batch 205/342 (32 items)\n",
            "Processed batch 206/342 (32 items)\n",
            "Processed batch 207/342 (32 items)\n",
            "Processed batch 208/342 (32 items)\n",
            "Processed batch 209/342 (32 items)\n",
            "Checkpoint saved at batch 210\n",
            "Processed batch 210/342 (32 items)\n",
            "\n",
            "        API Calls: 210\n",
            "        Total Tokens: 760956\n",
            "        Estimated Cost: $1.52\n",
            "        \n",
            "Processed batch 211/342 (32 items)\n",
            "Processed batch 212/342 (32 items)\n",
            "Processed batch 213/342 (32 items)\n",
            "Processed batch 214/342 (32 items)\n",
            "Processed batch 215/342 (32 items)\n",
            "Processed batch 216/342 (32 items)\n",
            "Processed batch 217/342 (32 items)\n",
            "Processed batch 218/342 (32 items)\n",
            "Processed batch 219/342 (32 items)\n",
            "Checkpoint saved at batch 220\n",
            "Processed batch 220/342 (32 items)\n",
            "\n",
            "        API Calls: 220\n",
            "        Total Tokens: 800130\n",
            "        Estimated Cost: $1.60\n",
            "        \n",
            "Processed batch 221/342 (32 items)\n",
            "Processed batch 222/342 (32 items)\n",
            "Processed batch 223/342 (32 items)\n",
            "Processed batch 224/342 (32 items)\n",
            "Processed batch 225/342 (32 items)\n",
            "Processed batch 226/342 (32 items)\n",
            "Processed batch 227/342 (32 items)\n",
            "Processed batch 228/342 (32 items)\n",
            "Processed batch 229/342 (32 items)\n",
            "Checkpoint saved at batch 230\n",
            "Processed batch 230/342 (32 items)\n",
            "\n",
            "        API Calls: 230\n",
            "        Total Tokens: 836066\n",
            "        Estimated Cost: $1.67\n",
            "        \n",
            "Processed batch 231/342 (32 items)\n",
            "Processed batch 232/342 (32 items)\n",
            "Processed batch 233/342 (32 items)\n",
            "Processed batch 234/342 (32 items)\n",
            "Processed batch 235/342 (32 items)\n",
            "Processed batch 236/342 (32 items)\n",
            "Processed batch 237/342 (32 items)\n",
            "Processed batch 238/342 (32 items)\n",
            "Processed batch 239/342 (32 items)\n",
            "Checkpoint saved at batch 240\n",
            "Processed batch 240/342 (32 items)\n",
            "\n",
            "        API Calls: 240\n",
            "        Total Tokens: 874207\n",
            "        Estimated Cost: $1.75\n",
            "        \n",
            "Processed batch 241/342 (32 items)\n",
            "Processed batch 242/342 (32 items)\n",
            "Processed batch 243/342 (32 items)\n",
            "Processed batch 244/342 (32 items)\n",
            "Processed batch 245/342 (32 items)\n",
            "Processed batch 246/342 (32 items)\n",
            "Processed batch 247/342 (32 items)\n",
            "Processed batch 248/342 (32 items)\n",
            "Processed batch 249/342 (32 items)\n",
            "Checkpoint saved at batch 250\n",
            "Processed batch 250/342 (32 items)\n",
            "\n",
            "        API Calls: 250\n",
            "        Total Tokens: 911818\n",
            "        Estimated Cost: $1.82\n",
            "        \n",
            "Processed batch 251/342 (32 items)\n",
            "Processed batch 252/342 (32 items)\n",
            "Processed batch 253/342 (32 items)\n",
            "Processed batch 254/342 (32 items)\n",
            "Processed batch 255/342 (32 items)\n",
            "Processed batch 256/342 (32 items)\n",
            "Processed batch 257/342 (32 items)\n",
            "Processed batch 258/342 (32 items)\n",
            "Processed batch 259/342 (32 items)\n",
            "Checkpoint saved at batch 260\n",
            "Processed batch 260/342 (32 items)\n",
            "\n",
            "        API Calls: 260\n",
            "        Total Tokens: 949344\n",
            "        Estimated Cost: $1.90\n",
            "        \n",
            "Processed batch 261/342 (32 items)\n",
            "Processed batch 262/342 (32 items)\n",
            "Processed batch 263/342 (32 items)\n",
            "Processed batch 264/342 (32 items)\n",
            "Processed batch 265/342 (32 items)\n",
            "Processed batch 266/342 (32 items)\n",
            "Processed batch 267/342 (32 items)\n",
            "Processed batch 268/342 (32 items)\n",
            "Processed batch 269/342 (32 items)\n",
            "Checkpoint saved at batch 270\n",
            "Processed batch 270/342 (32 items)\n",
            "\n",
            "        API Calls: 270\n",
            "        Total Tokens: 987972\n",
            "        Estimated Cost: $1.98\n",
            "        \n",
            "Processed batch 271/342 (32 items)\n",
            "Processed batch 272/342 (32 items)\n",
            "Processed batch 273/342 (32 items)\n",
            "Processed batch 274/342 (32 items)\n",
            "Processed batch 275/342 (32 items)\n",
            "Processed batch 276/342 (32 items)\n",
            "Processed batch 277/342 (32 items)\n",
            "Processed batch 278/342 (32 items)\n",
            "Processed batch 279/342 (32 items)\n",
            "Checkpoint saved at batch 280\n",
            "Processed batch 280/342 (32 items)\n",
            "\n",
            "        API Calls: 280\n",
            "        Total Tokens: 1024726\n",
            "        Estimated Cost: $2.05\n",
            "        \n",
            "Processed batch 281/342 (32 items)\n",
            "Processed batch 282/342 (32 items)\n",
            "Processed batch 283/342 (32 items)\n",
            "Processed batch 284/342 (32 items)\n",
            "Processed batch 285/342 (32 items)\n",
            "Processed batch 286/342 (32 items)\n",
            "Processed batch 287/342 (32 items)\n",
            "Processed batch 288/342 (32 items)\n",
            "Processed batch 289/342 (32 items)\n",
            "Checkpoint saved at batch 290\n",
            "Processed batch 290/342 (32 items)\n",
            "\n",
            "        API Calls: 290\n",
            "        Total Tokens: 1061975\n",
            "        Estimated Cost: $2.12\n",
            "        \n",
            "Processed batch 291/342 (32 items)\n",
            "Processed batch 292/342 (32 items)\n",
            "Processed batch 293/342 (32 items)\n",
            "Processed batch 294/342 (32 items)\n",
            "Processed batch 295/342 (32 items)\n",
            "Processed batch 296/342 (32 items)\n",
            "Processed batch 297/342 (32 items)\n",
            "Processed batch 298/342 (32 items)\n",
            "Processed batch 299/342 (32 items)\n",
            "Checkpoint saved at batch 300\n",
            "Processed batch 300/342 (32 items)\n",
            "\n",
            "        API Calls: 300\n",
            "        Total Tokens: 1097615\n",
            "        Estimated Cost: $2.20\n",
            "        \n",
            "Processed batch 301/342 (32 items)\n",
            "Processed batch 302/342 (32 items)\n",
            "Processed batch 303/342 (32 items)\n",
            "Processed batch 304/342 (32 items)\n",
            "Processed batch 305/342 (32 items)\n",
            "Processed batch 306/342 (32 items)\n",
            "Processed batch 307/342 (32 items)\n",
            "Processed batch 308/342 (32 items)\n",
            "Processed batch 309/342 (32 items)\n",
            "Checkpoint saved at batch 310\n",
            "Processed batch 310/342 (32 items)\n",
            "\n",
            "        API Calls: 310\n",
            "        Total Tokens: 1133789\n",
            "        Estimated Cost: $2.27\n",
            "        \n",
            "Processed batch 311/342 (32 items)\n",
            "Processed batch 312/342 (32 items)\n",
            "Processed batch 313/342 (32 items)\n",
            "Processed batch 314/342 (32 items)\n",
            "Processed batch 315/342 (32 items)\n",
            "Processed batch 316/342 (32 items)\n",
            "Processed batch 317/342 (32 items)\n",
            "Processed batch 318/342 (32 items)\n",
            "Processed batch 319/342 (32 items)\n",
            "Checkpoint saved at batch 320\n",
            "Processed batch 320/342 (32 items)\n",
            "\n",
            "        API Calls: 320\n",
            "        Total Tokens: 1170206\n",
            "        Estimated Cost: $2.34\n",
            "        \n",
            "Processed batch 321/342 (32 items)\n",
            "Processed batch 322/342 (32 items)\n",
            "Processed batch 323/342 (32 items)\n",
            "Processed batch 324/342 (32 items)\n",
            "Processed batch 325/342 (32 items)\n",
            "Processed batch 326/342 (32 items)\n",
            "Processed batch 327/342 (32 items)\n",
            "Processed batch 328/342 (32 items)\n",
            "Processed batch 329/342 (32 items)\n",
            "Checkpoint saved at batch 330\n",
            "Processed batch 330/342 (32 items)\n",
            "\n",
            "        API Calls: 330\n",
            "        Total Tokens: 1210258\n",
            "        Estimated Cost: $2.42\n",
            "        \n",
            "Processed batch 331/342 (32 items)\n",
            "Processed batch 332/342 (32 items)\n",
            "Processed batch 333/342 (32 items)\n",
            "Processed batch 334/342 (32 items)\n",
            "Processed batch 335/342 (32 items)\n",
            "Processed batch 336/342 (32 items)\n",
            "Processed batch 337/342 (32 items)\n",
            "Processed batch 338/342 (32 items)\n",
            "Processed batch 339/342 (32 items)\n",
            "Checkpoint saved at batch 340\n",
            "Processed batch 340/342 (32 items)\n",
            "\n",
            "        API Calls: 340\n",
            "        Total Tokens: 1243900\n",
            "        Estimated Cost: $2.49\n",
            "        \n",
            "Processed batch 341/342 (32 items)\n",
            "Processed batch 342/342 (5 items)\n",
            "\n",
            "Final Statistics:\n",
            "\n",
            "        API Calls: 342\n",
            "        Total Tokens: 1248224\n",
            "        Estimated Cost: $2.50\n",
            "        \n",
            "Total translations: 10927\n"
          ]
        }
      ],
      "source": [
        "# Successful\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n",
        "    dataset_name = \"msr_paraphrase_test\"  # Using test suffix to distinguish from full runs\n",
        "    max_sentences = 10949  # Limit to first 100 sentences\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,  # Keeping original batch size\n",
        "        checkpoint_interval=10,  # Frequent checkpoints for testing\n",
        "        max_sentences=10949  # Parameter to limit number of sentences\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tnJIJ9oNBkXf"
      },
      "source": [
        "### Few tries of translation of msr paraphrase dataset - Executing the code for 1h\n",
        "\n",
        "Final Statistics:\n",
        "\n",
        "        API Calls: 172\n",
        "        Total Tokens: 1094704\n",
        "        Estimated Cost: $2.19\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GNao8vk4CdtN",
        "outputId": "e25df440-3d7f-476a-ee54-07a3eedf366c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting from batch 171, line 10944\n",
            "Total lines in file: 10949\n",
            "Processed batch 171/171 (4 items)\n",
            "\n",
            "Final Statistics:\n",
            "\n",
            "        API Calls: 1\n",
            "        Total Tokens: 308\n",
            "        Estimated Cost: $0.00\n",
            "        \n",
            "Total translations: 11016\n"
          ]
        }
      ],
      "source": [
        "# new, more robust and for non-asynchronous function\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n",
        "    dataset_name = \"msr_paraphrase\"\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=64,\n",
        "        checkpoint_interval=5\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wwQOdDo6BkXf",
        "outputId": "cfbf12d0-a925-456c-d94f-f62cdeddad9c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Processed batch 5/343\n",
            "Processed batch 6/343\n",
            "Processed batch 0/172\n",
            "\n",
            "        API Calls: 1\n",
            "        Total Tokens: 7487\n",
            "        Estimated Cost: $0.01\n",
            "        \n",
            "Processed batch 7/343\n",
            "Processed batch 1/172\n",
            "Processed batch 2/172\n",
            "Processed batch 8/343\n",
            "Processed batch 9/343\n",
            "Processed batch 3/172\n",
            "Processed batch 10/343\n",
            "\n",
            "        API Calls: 11\n",
            "        Total Tokens: 39969\n",
            "        Estimated Cost: $0.08\n",
            "        \n",
            "Processed batch 11/343\n",
            "Processed batch 4/172\n",
            "Processed batch 12/343\n",
            "Processed batch 5/172\n",
            "Processed batch 6/172\n",
            "Processed batch 13/343\n",
            "Processed batch 14/343\n",
            "Processed batch 7/172\n",
            "Processed batch 15/343\n",
            "Processed batch 16/343\n",
            "Processed batch 8/172\n",
            "Processed batch 9/172\n",
            "Processed batch 10/172\n",
            "\n",
            "        API Calls: 11\n",
            "        Total Tokens: 70814\n",
            "        Estimated Cost: $0.14\n",
            "        \n",
            "Processed batch 11/172\n",
            "Processed batch 17/343\n",
            "Processed batch 18/343\n",
            "Processed batch 19/343\n",
            "Processed batch 12/172\n",
            "Processed batch 20/343\n",
            "\n",
            "        API Calls: 21\n",
            "        Total Tokens: 78213\n",
            "        Estimated Cost: $0.16\n",
            "        \n",
            "Processed batch 21/343\n",
            "Processed batch 13/172\n",
            "Processed batch 22/343\n",
            "Processed batch 14/172\n",
            "Processed batch 23/343\n",
            "Processed batch 24/343\n",
            "Processed batch 25/343\n",
            "Processed batch 15/172\n",
            "Processed batch 16/172\n",
            "Processed batch 26/343\n",
            "Processed batch 27/343\n",
            "Processed batch 17/172\n",
            "Processed batch 28/343\n",
            "Processed batch 18/172\n",
            "Processed batch 19/172\n",
            "Processed batch 20/172\n",
            "\n",
            "        API Calls: 21\n",
            "        Total Tokens: 130578\n",
            "        Estimated Cost: $0.26\n",
            "        \n",
            "Processed batch 29/343\n",
            "Processed batch 30/343\n",
            "\n",
            "        API Calls: 31\n",
            "        Total Tokens: 115868\n",
            "        Estimated Cost: $0.23\n",
            "        \n",
            "Processed batch 31/343\n",
            "Processed batch 21/172\n",
            "Processed batch 32/343\n",
            "Processed batch 33/343\n",
            "Processed batch 22/172\n",
            "Processed batch 34/343\n",
            "Processed batch 35/343\n",
            "Processed batch 36/343\n",
            "Processed batch 23/172\n",
            "Processed batch 37/343\n",
            "Processed batch 38/343\n",
            "Processed batch 24/172\n",
            "Processed batch 39/343\n",
            "Processed batch 25/172\n",
            "Processed batch 40/343\n",
            "\n",
            "        API Calls: 41\n",
            "        Total Tokens: 150629\n",
            "        Estimated Cost: $0.30\n",
            "        \n",
            "Processed batch 41/343\n",
            "Processed batch 26/172\n",
            "Processed batch 42/343\n",
            "Processed batch 43/343\n",
            "Processed batch 44/343\n",
            "Processed batch 27/172\n",
            "Processed batch 45/343\n",
            "Processed batch 28/172\n",
            "Processed batch 46/343\n",
            "Processed batch 29/172\n",
            "Processed batch 47/343\n",
            "Processed batch 30/172\n",
            "\n",
            "        API Calls: 31\n",
            "        Total Tokens: 199562\n",
            "        Estimated Cost: $0.40\n",
            "        \n",
            "Processed batch 31/172\n",
            "Processed batch 48/343\n",
            "Processed batch 49/343\n",
            "Processed batch 50/343\n",
            "\n",
            "        API Calls: 51\n",
            "        Total Tokens: 185868\n",
            "        Estimated Cost: $0.37\n",
            "        \n",
            "Processed batch 32/172\n",
            "Processed batch 33/172\n",
            "Processed batch 51/343\n",
            "Processed batch 34/172\n",
            "Processed batch 52/343\n",
            "Processed batch 53/343\n",
            "Processed batch 35/172\n",
            "Processed batch 54/343\n",
            "Processed batch 36/172\n",
            "Processed batch 55/343\n",
            "Processed batch 56/343\n",
            "Processed batch 57/343\n",
            "Processed batch 58/343\n",
            "Processed batch 37/172\n",
            "Processed batch 59/343\n",
            "Processed batch 60/343\n",
            "\n",
            "        API Calls: 61\n",
            "        Total Tokens: 219779\n",
            "        Estimated Cost: $0.44\n",
            "        \n",
            "Processed batch 38/172\n",
            "Processed batch 61/343\n",
            "Processed batch 62/343\n",
            "Processed batch 63/343\n",
            "Processed batch 64/343\n",
            "Processed batch 39/172\n",
            "Processed batch 65/343\n",
            "Processed batch 66/343\n",
            "Processed batch 40/172\n",
            "\n",
            "        API Calls: 41\n",
            "        Total Tokens: 265480\n",
            "        Estimated Cost: $0.53\n",
            "        \n",
            "Processed batch 67/343\n",
            "Processed batch 41/172\n",
            "Processed batch 68/343\n",
            "Processed batch 69/343\n",
            "Processed batch 70/343\n",
            "\n",
            "        API Calls: 71\n",
            "        Total Tokens: 253708\n",
            "        Estimated Cost: $0.51\n",
            "        \n",
            "Processed batch 42/172\n",
            "Processed batch 71/343\n",
            "Processed batch 72/343\n",
            "Processed batch 43/172\n",
            "Processed batch 73/343\n",
            "Processed batch 74/343\n",
            "Processed batch 44/172\n",
            "Processed batch 75/343\n",
            "Processed batch 45/172\n",
            "Processed batch 76/343\n",
            "Processed batch 77/343\n",
            "Processed batch 78/343\n",
            "Processed batch 46/172\n",
            "Processed batch 79/343\n",
            "Processed batch 47/172\n",
            "Processed batch 80/343\n",
            "\n",
            "        API Calls: 81\n",
            "        Total Tokens: 292033\n",
            "        Estimated Cost: $0.58\n",
            "        \n",
            "Processed batch 81/343\n",
            "Processed batch 48/172\n",
            "Processed batch 82/343\n",
            "Processed batch 83/343\n",
            "Processed batch 84/343\n",
            "Processed batch 49/172\n",
            "Processed batch 50/172\n",
            "\n",
            "        API Calls: 51\n",
            "        Total Tokens: 337627\n",
            "        Estimated Cost: $0.68\n",
            "        \n",
            "Processed batch 85/343\n",
            "Processed batch 86/343\n",
            "Processed batch 51/172\n",
            "Processed batch 87/343\n",
            "Processed batch 52/172\n",
            "Processed batch 88/343\n",
            "Processed batch 89/343\n",
            "Processed batch 90/343\n",
            "\n",
            "        API Calls: 90\n",
            "        Total Tokens: 324480\n",
            "        Estimated Cost: $0.65\n",
            "        \n",
            "Processed batch 53/172\n",
            "Processed batch 91/343\n",
            "Processed batch 92/343\n",
            "Processed batch 54/172\n",
            "Processed batch 55/172\n",
            "Processed batch 93/343\n",
            "Processed batch 94/343\n",
            "Processed batch 56/172\n",
            "Processed batch 95/343\n",
            "Processed batch 96/343\n",
            "Processed batch 97/343\n",
            "Processed batch 98/343\n",
            "Processed batch 57/172\n",
            "Processed batch 99/343\n",
            "Processed batch 100/343\n",
            "\n",
            "        API Calls: 100\n",
            "        Total Tokens: 361588\n",
            "        Estimated Cost: $0.72\n",
            "        \n",
            "Processed batch 58/172\n",
            "Processed batch 101/343\n",
            "Processed batch 102/343\n",
            "Processed batch 59/172\n",
            "Processed batch 103/343\n",
            "Processed batch 104/343\n",
            "Processed batch 60/172\n",
            "\n",
            "        API Calls: 61\n",
            "        Total Tokens: 405808\n",
            "        Estimated Cost: $0.81\n",
            "        \n",
            "Processed batch 61/172\n",
            "Processed batch 62/172\n",
            "Processed batch 105/343\n",
            "Processed batch 106/343\n",
            "Processed batch 107/343\n",
            "Processed batch 108/343\n",
            "Processed batch 63/172\n",
            "Processed batch 109/343\n",
            "Processed batch 110/343\n",
            "\n",
            "        API Calls: 110\n",
            "        Total Tokens: 399138\n",
            "        Estimated Cost: $0.80\n",
            "        \n",
            "Processed batch 64/172\n",
            "Processed batch 111/343\n",
            "Processed batch 112/343\n",
            "Processed batch 65/172\n",
            "Processed batch 66/172\n",
            "Processed batch 113/343\n",
            "Processed batch 114/343\n",
            "Processed batch 67/172\n",
            "Processed batch 115/343\n",
            "Processed batch 116/343\n",
            "Processed batch 68/172\n",
            "Processed batch 117/343\n",
            "Processed batch 118/343\n",
            "Processed batch 119/343\n",
            "Processed batch 69/172\n",
            "Processed batch 70/172\n",
            "\n",
            "        API Calls: 71\n",
            "        Total Tokens: 468025\n",
            "        Estimated Cost: $0.94\n",
            "        \n",
            "Processed batch 120/343\n",
            "\n",
            "        API Calls: 120\n",
            "        Total Tokens: 435532\n",
            "        Estimated Cost: $0.87\n",
            "        \n",
            "Processed batch 71/172\n",
            "Processed batch 121/343\n",
            "Processed batch 122/343\n",
            "Processed batch 72/172\n",
            "Processed batch 123/343\n",
            "Processed batch 124/343\n",
            "Processed batch 125/343\n",
            "Processed batch 73/172\n",
            "Processed batch 126/343\n",
            "Processed batch 127/343\n",
            "Processed batch 128/343\n",
            "Processed batch 74/172\n",
            "Processed batch 129/343\n",
            "Processed batch 130/343\n",
            "\n",
            "        API Calls: 130\n",
            "        Total Tokens: 471201\n",
            "        Estimated Cost: $0.94\n",
            "        \n",
            "Processed batch 75/172\n",
            "Processed batch 131/343\n",
            "Processed batch 76/172\n",
            "Processed batch 132/343\n",
            "Processed batch 77/172\n",
            "Processed batch 133/343\n",
            "Processed batch 134/343\n",
            "Processed batch 78/172\n",
            "Processed batch 79/172\n",
            "Processed batch 80/172\n",
            "\n",
            "        API Calls: 81\n",
            "        Total Tokens: 532175\n",
            "        Estimated Cost: $1.06\n",
            "        \n",
            "Processed batch 135/343\n",
            "Processed batch 136/343\n",
            "Processed batch 137/343\n",
            "Processed batch 81/172\n",
            "Processed batch 138/343\n",
            "Processed batch 139/343\n",
            "Processed batch 140/343\n",
            "\n",
            "        API Calls: 140\n",
            "        Total Tokens: 507152\n",
            "        Estimated Cost: $1.01\n",
            "        \n",
            "Processed batch 82/172\n",
            "Processed batch 141/343\n",
            "Processed batch 142/343\n",
            "Processed batch 143/343\n",
            "Processed batch 144/343\n",
            "Processed batch 145/343\n",
            "Processed batch 83/172\n",
            "Processed batch 84/172\n",
            "Processed batch 85/172\n",
            "Processed batch 146/343\n",
            "Processed batch 86/172\n",
            "Processed batch 147/343\n",
            "Processed batch 148/343\n",
            "Processed batch 87/172\n",
            "Processed batch 149/343\n",
            "Processed batch 150/343\n",
            "\n",
            "        API Calls: 150\n",
            "        Total Tokens: 539586\n",
            "        Estimated Cost: $1.08\n",
            "        \n",
            "Processed batch 151/343\n",
            "Processed batch 88/172\n",
            "Processed batch 152/343\n",
            "Processed batch 89/172\n",
            "Processed batch 153/343\n",
            "Processed batch 154/343\n",
            "Processed batch 155/343\n",
            "Processed batch 156/343\n",
            "Processed batch 90/172\n",
            "\n",
            "        API Calls: 91\n",
            "        Total Tokens: 596889\n",
            "        Estimated Cost: $1.19\n",
            "        \n",
            "Processed batch 91/172\n",
            "Processed batch 157/343\n",
            "Processed batch 92/172\n",
            "Processed batch 158/343\n",
            "Processed batch 93/172\n",
            "Processed batch 159/343\n",
            "Processed batch 160/343\n",
            "\n",
            "        API Calls: 159\n",
            "        Total Tokens: 571100\n",
            "        Estimated Cost: $1.14\n",
            "        \n",
            "Processed batch 94/172\n",
            "Processed batch 161/343\n",
            "Processed batch 162/343\n",
            "Processed batch 95/172\n",
            "Processed batch 163/343\n",
            "Processed batch 96/172\n",
            "Processed batch 164/343\n",
            "Processed batch 165/343\n",
            "Processed batch 97/172\n",
            "Processed batch 166/343\n",
            "Processed batch 98/172\n",
            "Processed batch 167/343\n",
            "Processed batch 168/343\n",
            "Processed batch 99/172\n",
            "Processed batch 169/343\n",
            "Processed batch 170/343\n",
            "\n",
            "        API Calls: 169\n",
            "        Total Tokens: 608469\n",
            "        Estimated Cost: $1.22\n",
            "        \n",
            "Processed batch 100/172\n",
            "\n",
            "        API Calls: 101\n",
            "        Total Tokens: 657743\n",
            "        Estimated Cost: $1.32\n",
            "        \n",
            "Processed batch 171/343\n",
            "Processed batch 172/343\n",
            "Processed batch 173/343\n",
            "Processed batch 101/172\n",
            "Processed batch 174/343\n",
            "Processed batch 175/343\n",
            "Processed batch 102/172\n",
            "Processed batch 176/343\n",
            "Processed batch 177/343\n",
            "Processed batch 103/172\n",
            "Processed batch 178/343\n",
            "Processed batch 179/343\n",
            "Processed batch 180/343\n",
            "\n",
            "        API Calls: 179\n",
            "        Total Tokens: 640217\n",
            "        Estimated Cost: $1.28\n",
            "        \n",
            "Processed batch 104/172\n",
            "Processed batch 181/343\n",
            "Processed batch 182/343\n",
            "Processed batch 183/343\n",
            "Processed batch 105/172\n",
            "Processed batch 106/172\n",
            "Processed batch 184/343\n",
            "Processed batch 185/343\n",
            "Processed batch 107/172\n",
            "Processed batch 108/172\n",
            "Processed batch 109/172\n",
            "Processed batch 186/343\n",
            "Processed batch 187/343\n",
            "Processed batch 110/172\n",
            "\n",
            "        API Calls: 111\n",
            "        Total Tokens: 722939\n",
            "        Estimated Cost: $1.45\n",
            "        \n",
            "Processed batch 188/343\n",
            "Processed batch 111/172\n",
            "Processed batch 112/172\n",
            "Processed batch 113/172\n",
            "Processed batch 189/343\n",
            "Processed batch 190/343\n",
            "\n",
            "        API Calls: 189\n",
            "        Total Tokens: 677766\n",
            "        Estimated Cost: $1.36\n",
            "        \n",
            "Processed batch 114/172\n",
            "Processed batch 115/172\n",
            "Processed batch 191/343\n",
            "Processed batch 192/343\n",
            "Processed batch 116/172\n",
            "Processed batch 193/343\n",
            "Processed batch 194/343\n",
            "Processed batch 117/172\n",
            "Processed batch 195/343\n",
            "Processed batch 196/343\n",
            "Processed batch 118/172\n",
            "Processed batch 197/343\n",
            "Processed batch 119/172\n",
            "Processed batch 198/343\n",
            "Processed batch 199/343\n",
            "Processed batch 120/172\n",
            "\n",
            "        API Calls: 121\n",
            "        Total Tokens: 781706\n",
            "        Estimated Cost: $1.56\n",
            "        \n",
            "Processed batch 200/343\n",
            "\n",
            "        API Calls: 199\n",
            "        Total Tokens: 712923\n",
            "        Estimated Cost: $1.43\n",
            "        \n",
            "Processed batch 201/343\n",
            "Processed batch 202/343\n",
            "Processed batch 121/172\n",
            "Processed batch 122/172\n",
            "Processed batch 203/343\n",
            "Processed batch 204/343\n",
            "Processed batch 123/172\n",
            "Processed batch 205/343\n",
            "Processed batch 206/343\n",
            "Processed batch 207/343\n",
            "Processed batch 208/343\n",
            "Processed batch 124/172\n",
            "Processed batch 209/343\n",
            "Processed batch 210/343\n",
            "\n",
            "        API Calls: 209\n",
            "        Total Tokens: 745741\n",
            "        Estimated Cost: $1.49\n",
            "        \n",
            "Processed batch 211/343\n",
            "Processed batch 125/172\n",
            "Processed batch 126/172\n",
            "Processed batch 212/343\n",
            "Processed batch 213/343\n",
            "Processed batch 127/172\n",
            "Processed batch 214/343\n",
            "Processed batch 215/343\n",
            "Processed batch 216/343\n",
            "Processed batch 217/343\n",
            "Processed batch 128/172\n",
            "Processed batch 218/343\n",
            "Processed batch 129/172\n",
            "Processed batch 219/343\n",
            "Processed batch 220/343\n",
            "\n",
            "        API Calls: 219\n",
            "        Total Tokens: 779082\n",
            "        Estimated Cost: $1.56\n",
            "        \n",
            "Processed batch 221/343\n",
            "Processed batch 222/343\n",
            "Processed batch 130/172\n",
            "\n",
            "        API Calls: 131\n",
            "        Total Tokens: 848475\n",
            "        Estimated Cost: $1.70\n",
            "        \n",
            "Processed batch 223/343\n",
            "Processed batch 224/343\n",
            "Processed batch 131/172\n",
            "Processed batch 225/343\n",
            "Processed batch 226/343\n",
            "Processed batch 227/343\n",
            "Processed batch 132/172\n",
            "Processed batch 228/343\n",
            "Processed batch 133/172\n",
            "Processed batch 229/343\n",
            "Processed batch 230/343\n",
            "\n",
            "        API Calls: 229\n",
            "        Total Tokens: 812829\n",
            "        Estimated Cost: $1.63\n",
            "        \n",
            "Processed batch 231/343\n",
            "Processed batch 134/172\n",
            "Processed batch 232/343\n",
            "Processed batch 135/172\n",
            "Processed batch 233/343\n",
            "Processed batch 136/172\n",
            "Processed batch 234/343\n",
            "Processed batch 235/343\n",
            "Processed batch 137/172\n",
            "Processed batch 138/172\n",
            "Processed batch 236/343\n",
            "Processed batch 237/343\n",
            "Processed batch 139/172\n",
            "Processed batch 238/343\n",
            "Processed batch 239/343\n",
            "Processed batch 140/172\n",
            "\n",
            "        API Calls: 141\n",
            "        Total Tokens: 912265\n",
            "        Estimated Cost: $1.82\n",
            "        \n",
            "Processed batch 240/343\n",
            "\n",
            "        API Calls: 238\n",
            "        Total Tokens: 848298\n",
            "        Estimated Cost: $1.70\n",
            "        \n",
            "Processed batch 241/343\n",
            "Processed batch 141/172\n",
            "Processed batch 142/172\n",
            "Processed batch 242/343\n",
            "Processed batch 143/172\n",
            "Processed batch 243/343\n",
            "Processed batch 144/172\n",
            "Processed batch 244/343\n",
            "Processed batch 145/172\n",
            "Processed batch 245/343\n",
            "Processed batch 246/343\n",
            "Processed batch 146/172\n",
            "Processed batch 247/343\n",
            "Processed batch 248/343\n",
            "Processed batch 249/343\n",
            "Processed batch 147/172\n",
            "Processed batch 250/343\n",
            "\n",
            "        API Calls: 248\n",
            "        Total Tokens: 885864\n",
            "        Estimated Cost: $1.77\n",
            "        \n",
            "Processed batch 148/172\n",
            "Processed batch 251/343\n",
            "Processed batch 252/343\n",
            "Processed batch 149/172\n",
            "Processed batch 253/343\n",
            "Processed batch 150/172\n",
            "\n",
            "        API Calls: 151\n",
            "        Total Tokens: 972907\n",
            "        Estimated Cost: $1.95\n",
            "        \n",
            "Processed batch 254/343\n",
            "Processed batch 255/343\n",
            "Processed batch 256/343\n",
            "Processed batch 151/172\n",
            "Processed batch 257/343\n",
            "Processed batch 152/172\n",
            "Processed batch 258/343\n",
            "Processed batch 153/172\n",
            "Processed batch 259/343\n",
            "Processed batch 260/343\n",
            "\n",
            "        API Calls: 258\n",
            "        Total Tokens: 921875\n",
            "        Estimated Cost: $1.84\n",
            "        \n",
            "Processed batch 154/172\n",
            "Processed batch 261/343\n",
            "Processed batch 155/172\n",
            "Processed batch 262/343\n",
            "Processed batch 263/343\n",
            "Processed batch 156/172\n",
            "Processed batch 157/172\n",
            "Processed batch 264/343\n",
            "Processed batch 158/172\n",
            "Processed batch 265/343\n",
            "Processed batch 266/343\n",
            "Processed batch 267/343\n",
            "Processed batch 159/172\n",
            "Processed batch 268/343\n",
            "Processed batch 160/172\n",
            "\n",
            "        API Calls: 161\n",
            "        Total Tokens: 1034691\n",
            "        Estimated Cost: $2.07\n",
            "        \n",
            "Processed batch 269/343\n",
            "Processed batch 270/343\n",
            "\n",
            "        API Calls: 268\n",
            "        Total Tokens: 956962\n",
            "        Estimated Cost: $1.91\n",
            "        \n",
            "Processed batch 271/343\n",
            "Processed batch 161/172\n",
            "Processed batch 272/343\n",
            "Processed batch 273/343\n",
            "Processed batch 162/172\n",
            "Processed batch 274/343\n",
            "Processed batch 275/343\n",
            "Processed batch 163/172\n",
            "Processed batch 276/343\n",
            "Processed batch 164/172\n",
            "Processed batch 165/172\n",
            "Processed batch 277/343\n",
            "Processed batch 278/343\n",
            "Processed batch 279/343\n",
            "Processed batch 166/172\n",
            "Processed batch 167/172\n",
            "Processed batch 280/343\n",
            "\n",
            "        API Calls: 278\n",
            "        Total Tokens: 989053\n",
            "        Estimated Cost: $1.98\n",
            "        \n",
            "Processed batch 281/343\n",
            "Processed batch 168/172\n",
            "Processed batch 169/172\n",
            "Processed batch 282/343\n",
            "Processed batch 283/343\n",
            "Processed batch 170/172\n",
            "\n",
            "        API Calls: 171\n",
            "        Total Tokens: 1094214\n",
            "        Estimated Cost: $2.19\n",
            "        \n",
            "Processed batch 171/172\n",
            "\n",
            "Final Statistics:\n",
            "\n",
            "        API Calls: 172\n",
            "        Total Tokens: 1094704\n",
            "        Estimated Cost: $2.19\n",
            "        \n"
          ]
        }
      ],
      "source": [
        "# running for asyncronous function, which is not in use at the moment\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/msr_paraphrase_data.txt\"\n",
        "    dataset_name = \"msr_paraphrase\"\n",
        "\n",
        "    # Get the OpenAI API key using userdata\n",
        "    from google.colab import userdata\n",
        "    api_key = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "    if api_key is None:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets.\")\n",
        "\n",
        "    # Use nest_asyncio to integrate with the existing loop\n",
        "    import nest_asyncio\n",
        "    nest_asyncio.apply()\n",
        "\n",
        "    # Get the current event loop\n",
        "    loop = asyncio.get_event_loop()\n",
        "\n",
        "    # Run the process_dataset function using the existing loop\n",
        "    loop.run_until_complete(process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=64  # Adjust batch size as needed\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7teb07y3BkXj"
      },
      "source": [
        "## **para-nmt-50m-small.txt**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVkgw0v4d1wc"
      },
      "source": [
        "#### First try, which gives un paired sentences.\n",
        "\n",
        "Processed batch 2680/3125 (32 items)\n",
        "\n",
        "        API Calls: 2681\n",
        "        Total Tokens: 4742926\n",
        "        Estimated Cost: $9.49"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gfg7kpM_w8CV"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, List, Dict, Any, Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [\"\"] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if text in cache:\n",
        "                    translations[i] = cache[text]\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for text, trans in zip(uncached_texts, new_translations):\n",
        "                            self.batch_cache[text] = trans\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = trans\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2)  # Wait before retrying\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = f\"ERROR: {str(e)}\"\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 10949):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[str]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip to start line\n",
        "                for _ in range(self.start_line):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        text = line.strip()\n",
        "                        if text:  # Only add non-empty texts\n",
        "                            current_batch.append(text)\n",
        "                            processed_lines += 1\n",
        "\n",
        "                            if len(current_batch) == self.batch_size:\n",
        "                                yield current_batch\n",
        "                                current_batch = []\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing line: {line.strip()}\")\n",
        "                        print(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 100000\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translations_count = checkpoint[\"translations_count\"]\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    # Create iterator with sentence limit\n",
        "    dataset_iterator = DatasetIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_line=start_line,\n",
        "        max_sentences=max_sentences\n",
        "    )\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    print(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    print(f\"Will process up to {max_sentences} sentences\")\n",
        "    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                print(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                print(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    print(\"\\nFinal Statistics:\")\n",
        "    print(cost_tracker.report())\n",
        "    print(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "m4OcKbcpxFv_",
        "outputId": "db7c484b-a4cf-4dde-bbd7-a7d65307bc36"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'process_dataset' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-56617cb1b9d0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     process_dataset(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'process_dataset' is not defined"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/para-nmt-50m-small.txt\"\n",
        "    dataset_name = \"para-nmt-50m-small_test\"\n",
        "    max_sentences = 100000  # Total number of sentences to process\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,\n",
        "        checkpoint_interval=10,  # Increased for larger dataset\n",
        "        max_sentences=max_sentences,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Second try, to give paired sentences"
      ],
      "metadata": {
        "id": "0NYeXsGabw-l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import time\n",
        "import json\n",
        "import re\n",
        "import csv\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from tqdm.notebook import tqdm\n",
        "from openai import OpenAI\n",
        "from google.colab import userdata\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key, cache_dir=\"translation_cache\", checkpoint_dir=\"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id):\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name):\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8-sig') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8-sig') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name, batch_num, translated_pairs_count, dataframe):\n",
        "        \"\"\"\n",
        "        Save checkpoint with progress information and the current DataFrame.\n",
        "        \"\"\"\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translated_pairs_count\": translated_pairs_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "\n",
        "        # Save checkpoint metadata\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8-sig') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "        # Save DataFrame checkpoint\n",
        "        df_checkpoint_file = self.checkpoint_dir / f\"{dataset_name}_data_checkpoint.csv\"\n",
        "        dataframe.to_csv(df_checkpoint_file, index=False, encoding='utf-8-sig')\n",
        "\n",
        "        print(f\"Checkpoint saved: {batch_num} batches, {translated_pairs_count} pairs\")\n",
        "\n",
        "    def load_checkpoint(self, dataset_name):\n",
        "        \"\"\"\n",
        "        Load checkpoint metadata and DataFrame if available.\n",
        "        \"\"\"\n",
        "        # Load checkpoint metadata\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        checkpoint_data = {\"last_batch\": -1, \"translated_pairs_count\": 0}\n",
        "\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8-sig') as f:\n",
        "                checkpoint_data = json.load(f)\n",
        "\n",
        "        # Load DataFrame checkpoint if it exists\n",
        "        df_checkpoint_file = self.checkpoint_dir / f\"{dataset_name}_data_checkpoint.csv\"\n",
        "        # Use explicit columns for sentence and paraphrase translations\n",
        "        df = pd.DataFrame(columns=['sentence_translation', 'paraphrase_translation'])\n",
        "\n",
        "        if df_checkpoint_file.exists():\n",
        "            try:\n",
        "                df = pd.read_csv(df_checkpoint_file, encoding='utf-8-sig')\n",
        "                print(f\"Loaded {len(df)} previously translated pairs from checkpoint\")\n",
        "            except Exception as e:\n",
        "                print(f\"Error loading DataFrame checkpoint: {e}\")\n",
        "\n",
        "        return checkpoint_data, df\n",
        "\n",
        "    def translate_batch(self, texts, batch_id):\n",
        "        \"\"\"\n",
        "        Translate a batch of texts from English to Slovenian.\n",
        "        Returns a list of translations and the API response if applicable.\n",
        "        \"\"\"\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [\"\"] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            # Check cache for existing translations\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if text in cache:\n",
        "                    translations[i] = cache[text]\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian. Return only the translations in the same order, one translation per line. Make sure to use proper Slovenian characters like č, š, and ž where appropriate:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n---\\n\".join(uncached_texts)}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        # Extract translations from response\n",
        "                        result_text = response.choices[0].message.content.strip()\n",
        "                        # Split by newlines to get individual translations\n",
        "                        new_translations = result_text.split('\\n')\n",
        "\n",
        "                        # Clean up translations (remove any numbering or markers)\n",
        "                        new_translations = [re.sub(r'^\\d+\\.\\s*', '', t).strip() for t in new_translations]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            if len(new_translations) > len(uncached_texts):\n",
        "                                new_translations = new_translations[:len(uncached_texts)]\n",
        "                            else:\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        # Update cache with new translations\n",
        "                        for text, trans in zip(uncached_texts, new_translations):\n",
        "                            self.batch_cache[text] = trans\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        # Update translations list\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = trans\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            sleep_time = 2 * (attempt + 1)  # Exponential backoff\n",
        "                            print(f\"Waiting {sleep_time} seconds before retrying...\")\n",
        "                            time.sleep(sleep_time)\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = f\"ERROR: {str(e)}\"\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class ParaNMTIterator:\n",
        "    def __init__(self, file_path, batch_size, start_pair=0, max_pairs=None):\n",
        "        \"\"\"\n",
        "        Iterator for para-nmt dataset that returns batches of sentence pairs.\n",
        "\n",
        "        Args:\n",
        "            file_path: Path to the para-nmt dataset file\n",
        "            batch_size: Number of pairs per batch\n",
        "            start_pair: Pair index to start from (for resuming)\n",
        "            max_pairs: Maximum number of pairs to process (None for all)\n",
        "        \"\"\"\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.start_pair = start_pair\n",
        "        self.max_pairs = max_pairs\n",
        "        self.total_pairs = self._count_pairs()\n",
        "\n",
        "        if self.max_pairs is not None:\n",
        "            self.total_pairs = min(self.total_pairs, self.max_pairs)\n",
        "\n",
        "    def _count_pairs(self):\n",
        "        \"\"\"Count the total number of sentence pairs in the file.\"\"\"\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8-sig\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting pairs: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self):\n",
        "        \"\"\"Return self as iterator.\"\"\"\n",
        "        # Initialize iterator state\n",
        "        self.file = None\n",
        "        self.current_pair = 0\n",
        "        self.pairs_processed = 0\n",
        "\n",
        "        # Open file and skip to start pair\n",
        "        self.file = open(self.file_path, \"r\", encoding=\"utf-8-sig\")\n",
        "\n",
        "        # Skip to start pair\n",
        "        for _ in range(self.start_pair):\n",
        "            next(self.file, None)\n",
        "            self.current_pair += 1\n",
        "\n",
        "        return self\n",
        "\n",
        "    def __next__(self):\n",
        "        \"\"\"Return the next batch of sentence pairs.\"\"\"\n",
        "        if (self.max_pairs is not None and self.pairs_processed >= self.max_pairs) or self.file is None:\n",
        "            if self.file:\n",
        "                self.file.close()\n",
        "                self.file = None\n",
        "            raise StopIteration\n",
        "\n",
        "        source_batch = []\n",
        "        target_batch = []\n",
        "        batch_start_idx = self.current_pair\n",
        "\n",
        "        # Read up to batch_size pairs\n",
        "        while len(source_batch) < self.batch_size:\n",
        "            try:\n",
        "                line = next(self.file)\n",
        "                self.current_pair += 1\n",
        "            except StopIteration:\n",
        "                # End of file\n",
        "                break\n",
        "\n",
        "            if self.max_pairs is not None and self.pairs_processed >= self.max_pairs:\n",
        "                break\n",
        "\n",
        "            try:\n",
        "                line = line.strip()\n",
        "\n",
        "                # Try multiple splitting approaches\n",
        "                # First try tab delimiter\n",
        "                parts = line.split('\\t')\n",
        "\n",
        "                # If that doesn't work, try 5+ spaces\n",
        "                if len(parts) != 2:\n",
        "                    parts = re.split(r'\\s{5,}', line)\n",
        "\n",
        "                # If that doesn't work, look for common patterns in the examples\n",
        "                if len(parts) != 2 and \"    \" in line:\n",
        "                    parts = line.split(\"    \", 1)  # Split on 4 spaces, but only first occurrence\n",
        "\n",
        "                # As a last resort, try to split on the first occurrence of several spaces\n",
        "                if len(parts) != 2:\n",
        "                    match = re.search(r'\\s{2,}', line)\n",
        "                    if match:\n",
        "                        split_point = match.start()\n",
        "                        parts = [line[:split_point].strip(), line[split_point:].strip()]\n",
        "\n",
        "                if len(parts) == 2:\n",
        "                    source_sentence = parts[0].strip()\n",
        "                    target_sentence = parts[1].strip()\n",
        "\n",
        "                    source_batch.append(source_sentence)\n",
        "                    target_batch.append(target_sentence)\n",
        "                    self.pairs_processed += 1\n",
        "                else:\n",
        "                    print(f\"Skipping line with unexpected format: {line}\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing line: {line}\")\n",
        "                print(f\"Error details: {str(e)}\")\n",
        "                continue\n",
        "\n",
        "        # If no pairs were read, stop iteration\n",
        "        if not source_batch:\n",
        "            if self.file:\n",
        "                self.file.close()\n",
        "                self.file = None\n",
        "            raise StopIteration\n",
        "\n",
        "        return (batch_start_idx, source_batch, target_batch)\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Close file when iterator is deleted.\"\"\"\n",
        "        if hasattr(self, 'file') and self.file:\n",
        "            self.file.close()\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002  # gpt-3.5-turbo price\n",
        "\n",
        "    def update(self, response):\n",
        "        \"\"\"Update statistics based on OpenAI API response.\"\"\"\n",
        "        if response is None:\n",
        "            return\n",
        "\n",
        "        self.requests += 1\n",
        "        if hasattr(response, 'usage'):\n",
        "            self.total_tokens += response.usage.total_tokens\n",
        "\n",
        "    def get_cost(self):\n",
        "        \"\"\"Get estimated cost in USD.\"\"\"\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self):\n",
        "        \"\"\"Get a formatted report of usage statistics.\"\"\"\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.4f}\n",
        "        \"\"\"\n",
        "\n",
        "def process_paranmt_dataset(\n",
        "    input_file,\n",
        "    dataset_name,\n",
        "    output_path=\"/content/drive/My Drive/Colab Notebooks\",\n",
        "    batch_size=10,\n",
        "    checkpoint_interval=5,\n",
        "    max_pairs=None\n",
        "):\n",
        "    \"\"\"\n",
        "    Process the para-nmt dataset, translating BOTH original English sentences\n",
        "    AND their paraphrases to Slovenian, saving as a CSV with two columns.\n",
        "\n",
        "    Args:\n",
        "        input_file: Path to the para-nmt dataset file\n",
        "        dataset_name: Name of the dataset (for filenames)\n",
        "        output_path: Directory to save output files\n",
        "        batch_size: Number of sentence pairs to process in each batch\n",
        "        checkpoint_interval: How many batches to process before saving a checkpoint\n",
        "        max_pairs: Maximum number of pairs to process (None for all)\n",
        "    \"\"\"\n",
        "    print(f\"Loading OpenAI API key from Colab secrets...\")\n",
        "    try:\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "        if not api_key:\n",
        "            raise ValueError(\"API key not found in Colab secrets\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error accessing API key: {str(e)}\")\n",
        "        print(\"Please set your OpenAI API key in Colab secrets with the name 'OPENAI_API_KEY'\")\n",
        "        return\n",
        "\n",
        "    print(f\"Initializing translation manager...\")\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint, df = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translated_pairs_count = checkpoint[\"translated_pairs_count\"]\n",
        "\n",
        "    # Calculate starting pair (for resuming)\n",
        "    start_pair = start_batch * batch_size\n",
        "\n",
        "    # Create iterator\n",
        "    dataset_iterator = ParaNMTIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_pair=start_pair,\n",
        "        max_pairs=max_pairs\n",
        "    )\n",
        "\n",
        "    # Set up output paths\n",
        "    output_path = Path(output_path)\n",
        "    output_path.mkdir(exist_ok=True, parents=True)\n",
        "    output_file = output_path / f\"{dataset_name}_translated_pairs.csv\"\n",
        "\n",
        "    # Create CSV file with headers if starting fresh\n",
        "    if start_batch == 0:\n",
        "        with open(output_file, 'w', encoding='utf-8-sig', newline='') as f:\n",
        "            writer = csv.writer(f)\n",
        "            writer.writerow(['sentence_translation', 'paraphrase_translation'])\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = (dataset_iterator.total_pairs - start_pair + batch_size - 1) // batch_size\n",
        "\n",
        "    print(f\"Starting from batch {start_batch}, pair {start_pair}\")\n",
        "    print(f\"Total pairs to process: {dataset_iterator.total_pairs}\")\n",
        "    print(f\"Expected batches: {total_batches}\")\n",
        "    print(f\"Output will be saved to: {output_file}\")\n",
        "\n",
        "    # Process batches with progress bar\n",
        "    batch_idx = start_batch - 1  # Initialize to handle the case where no batches are processed\n",
        "\n",
        "    with tqdm(total=total_batches, desc=\"Processing batches\") as pbar:\n",
        "        for batch_idx, (pair_start_idx, source_batch, paraphrase_batch) in enumerate(dataset_iterator, start=start_batch):\n",
        "            try:\n",
        "                # First, translate original sentences\n",
        "                batch_id_source = f\"{dataset_name}_source_{batch_idx}\"\n",
        "                translated_sources, response1 = translator.translate_batch(source_batch, batch_id_source)\n",
        "\n",
        "                if response1 is not None:\n",
        "                    cost_tracker.update(response1)\n",
        "\n",
        "                # Small delay between API calls\n",
        "                time.sleep(0.5)\n",
        "\n",
        "                # Then, translate paraphrases\n",
        "                batch_id_para = f\"{dataset_name}_para_{batch_idx}\"\n",
        "                translated_paraphrases, response2 = translator.translate_batch(paraphrase_batch, batch_id_para)\n",
        "\n",
        "                if response2 is not None:\n",
        "                    cost_tracker.update(response2)\n",
        "\n",
        "                # Add translations to dataframe with clear column names\n",
        "                batch_df = pd.DataFrame({\n",
        "                    'sentence_translation': translated_sources,\n",
        "                    'paraphrase_translation': translated_paraphrases\n",
        "                })\n",
        "\n",
        "                # Append to the main dataframe\n",
        "                df = pd.concat([df, batch_df], ignore_index=True)\n",
        "\n",
        "                # Append to CSV file\n",
        "                batch_df.to_csv(output_file, mode='a', header=False, index=False, encoding='utf-8-sig')\n",
        "\n",
        "                translated_pairs_count += len(translated_sources)\n",
        "\n",
        "                # Save checkpoint periodically\n",
        "                if batch_idx % checkpoint_interval == 0:\n",
        "                    translator.save_checkpoint(dataset_name, batch_idx, translated_pairs_count, df)\n",
        "                    print(f\"\\nCheckpoint saved at batch {batch_idx}\")\n",
        "                    print(f\"Processed {translated_pairs_count} pairs so far\")\n",
        "                    print(cost_tracker.report())\n",
        "\n",
        "                # Update progress bar\n",
        "                pbar.update(1)\n",
        "                pbar.set_postfix(pairs=translated_pairs_count, cost=f\"${cost_tracker.get_cost():.4f}\")\n",
        "\n",
        "                # Small delay to avoid rate limiting\n",
        "                time.sleep(0.5)\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"\\nError processing batch {batch_idx}: {str(e)}\")\n",
        "                translator.save_checkpoint(dataset_name, batch_idx-1, translated_pairs_count, df)\n",
        "                time.sleep(2)  # Longer delay on error\n",
        "                continue\n",
        "\n",
        "    # Final checkpoint and report - now batch_idx will always be defined\n",
        "    final_batch = max(start_batch - 1, batch_idx)\n",
        "    translator.save_checkpoint(dataset_name, final_batch, translated_pairs_count, df)\n",
        "    print(\"\\nTranslation completed!\")\n",
        "    print(f\"Total pairs translated: {translated_pairs_count}\")\n",
        "    print(cost_tracker.report())\n",
        "    print(f\"Output file saved to: {output_file}\")\n"
      ],
      "metadata": {
        "id": "9RD80gkXb1vO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Loading OpenAI API key from Colab secrets...\n",
        "Initializing translation manager...\n",
        "Starting from batch 0, pair 0\n",
        "Total pairs to process: 100000\n",
        "Expected batches: 10000\n",
        "Output will be saved to: /content/drive/My Drive/Colab Notebooks/paranmt_small_full_translated_pairs.csv\n",
        "Processing batches:  55%\n",
        " 5816/10000 [9:32:24<6:53:15,  5.93s/it, cost=$8.7911, pairs=58160]"
      ],
      "metadata": {
        "id": "FioThshNM5l1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if __name__ == \"__main__\":\n",
        "    # Configuration\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/para-nmt-50m-small.txt\"\n",
        "    dataset_name = \"paranmt_small_full\"\n",
        "    output_path = \"/content/drive/My Drive/Colab Notebooks\"\n",
        "    batch_size = 10\n",
        "\n",
        "    # Process the dataset\n",
        "    process_paranmt_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        output_path=output_path,\n",
        "        batch_size=batch_size,\n",
        "        max_pairs=None  # Set to 100 for testing, change to None for all pairs\n",
        "    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "93892b5a169342e38910618d132a0b6a",
            "1fc0462a6a0244baa92dbdbacdb9773d",
            "d5890d6518184bca8811c36d9042ee01",
            "5bfbf06eab5c46f6b1429f419e09038f",
            "ccc8022c90384c60942d0053e97a9720",
            "d816d48d2f5147a5b9c3a05fe525e466",
            "b54007a9878647b9b9fe9c177a247c6f",
            "6d626f686a2f4ac586910e7c5653b431",
            "994460ab9a4948ecbc1759ff9bd07aa5",
            "d8ec78ef09494eeba03f389df4081bbf",
            "8b363c624da34378b372fbacf1013442"
          ]
        },
        "id": "OhfhotmQeFjp",
        "outputId": "7aef20f9-10e7-46d8-a203-93443488dab8"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading OpenAI API key from Colab secrets...\n",
            "Initializing translation manager...\n",
            "Starting from batch 0, pair 0\n",
            "Total pairs to process: 100000\n",
            "Expected batches: 10000\n",
            "Output will be saved to: /content/drive/My Drive/Colab Notebooks/paranmt_small_full_translated_pairs.csv\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "93892b5a169342e38910618d132a0b6a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Processing batches:   0%|          | 0/10000 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "\n",
            "        API Calls: 6082\n",
            "        Total Tokens: 2308274\n",
            "        Estimated Cost: $4.6165\n",
            "        \n",
            "Checkpoint saved: 3045 batches, 30460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3045\n",
            "Processed 30460 pairs so far\n",
            "\n",
            "        API Calls: 6092\n",
            "        Total Tokens: 2312249\n",
            "        Estimated Cost: $4.6245\n",
            "        \n",
            "Checkpoint saved: 3050 batches, 30510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3050\n",
            "Processed 30510 pairs so far\n",
            "\n",
            "        API Calls: 6102\n",
            "        Total Tokens: 2316117\n",
            "        Estimated Cost: $4.6322\n",
            "        \n",
            "Checkpoint saved: 3055 batches, 30560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3055\n",
            "Processed 30560 pairs so far\n",
            "\n",
            "        API Calls: 6112\n",
            "        Total Tokens: 2319845\n",
            "        Estimated Cost: $4.6397\n",
            "        \n",
            "Checkpoint saved: 3060 batches, 30610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3060\n",
            "Processed 30610 pairs so far\n",
            "\n",
            "        API Calls: 6122\n",
            "        Total Tokens: 2323502\n",
            "        Estimated Cost: $4.6470\n",
            "        \n",
            "Checkpoint saved: 3065 batches, 30660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3065\n",
            "Processed 30660 pairs so far\n",
            "\n",
            "        API Calls: 6132\n",
            "        Total Tokens: 2327266\n",
            "        Estimated Cost: $4.6545\n",
            "        \n",
            "Checkpoint saved: 3070 batches, 30710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3070\n",
            "Processed 30710 pairs so far\n",
            "\n",
            "        API Calls: 6142\n",
            "        Total Tokens: 2330827\n",
            "        Estimated Cost: $4.6617\n",
            "        \n",
            "Checkpoint saved: 3075 batches, 30760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3075\n",
            "Processed 30760 pairs so far\n",
            "\n",
            "        API Calls: 6152\n",
            "        Total Tokens: 2334275\n",
            "        Estimated Cost: $4.6686\n",
            "        \n",
            "Checkpoint saved: 3080 batches, 30810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3080\n",
            "Processed 30810 pairs so far\n",
            "\n",
            "        API Calls: 6162\n",
            "        Total Tokens: 2338409\n",
            "        Estimated Cost: $4.6768\n",
            "        \n",
            "Checkpoint saved: 3085 batches, 30860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3085\n",
            "Processed 30860 pairs so far\n",
            "\n",
            "        API Calls: 6172\n",
            "        Total Tokens: 2341978\n",
            "        Estimated Cost: $4.6840\n",
            "        \n",
            "Checkpoint saved: 3090 batches, 30910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3090\n",
            "Processed 30910 pairs so far\n",
            "\n",
            "        API Calls: 6182\n",
            "        Total Tokens: 2345544\n",
            "        Estimated Cost: $4.6911\n",
            "        \n",
            "Checkpoint saved: 3095 batches, 30960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3095\n",
            "Processed 30960 pairs so far\n",
            "\n",
            "        API Calls: 6192\n",
            "        Total Tokens: 2349477\n",
            "        Estimated Cost: $4.6990\n",
            "        \n",
            "Checkpoint saved: 3100 batches, 31010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3100\n",
            "Processed 31010 pairs so far\n",
            "\n",
            "        API Calls: 6202\n",
            "        Total Tokens: 2353406\n",
            "        Estimated Cost: $4.7068\n",
            "        \n",
            "Checkpoint saved: 3105 batches, 31060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3105\n",
            "Processed 31060 pairs so far\n",
            "\n",
            "        API Calls: 6212\n",
            "        Total Tokens: 2357393\n",
            "        Estimated Cost: $4.7148\n",
            "        \n",
            "Checkpoint saved: 3110 batches, 31110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3110\n",
            "Processed 31110 pairs so far\n",
            "\n",
            "        API Calls: 6222\n",
            "        Total Tokens: 2361176\n",
            "        Estimated Cost: $4.7224\n",
            "        \n",
            "Checkpoint saved: 3115 batches, 31160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3115\n",
            "Processed 31160 pairs so far\n",
            "\n",
            "        API Calls: 6232\n",
            "        Total Tokens: 2364930\n",
            "        Estimated Cost: $4.7299\n",
            "        \n",
            "Checkpoint saved: 3120 batches, 31210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3120\n",
            "Processed 31210 pairs so far\n",
            "\n",
            "        API Calls: 6242\n",
            "        Total Tokens: 2368407\n",
            "        Estimated Cost: $4.7368\n",
            "        \n",
            "Checkpoint saved: 3125 batches, 31260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3125\n",
            "Processed 31260 pairs so far\n",
            "\n",
            "        API Calls: 6252\n",
            "        Total Tokens: 2372583\n",
            "        Estimated Cost: $4.7452\n",
            "        \n",
            "Checkpoint saved: 3130 batches, 31310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3130\n",
            "Processed 31310 pairs so far\n",
            "\n",
            "        API Calls: 6262\n",
            "        Total Tokens: 2376246\n",
            "        Estimated Cost: $4.7525\n",
            "        \n",
            "Checkpoint saved: 3135 batches, 31360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3135\n",
            "Processed 31360 pairs so far\n",
            "\n",
            "        API Calls: 6272\n",
            "        Total Tokens: 2380194\n",
            "        Estimated Cost: $4.7604\n",
            "        \n",
            "Checkpoint saved: 3140 batches, 31410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3140\n",
            "Processed 31410 pairs so far\n",
            "\n",
            "        API Calls: 6282\n",
            "        Total Tokens: 2384017\n",
            "        Estimated Cost: $4.7680\n",
            "        \n",
            "Checkpoint saved: 3145 batches, 31460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3145\n",
            "Processed 31460 pairs so far\n",
            "\n",
            "        API Calls: 6292\n",
            "        Total Tokens: 2387698\n",
            "        Estimated Cost: $4.7754\n",
            "        \n",
            "Checkpoint saved: 3150 batches, 31510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3150\n",
            "Processed 31510 pairs so far\n",
            "\n",
            "        API Calls: 6302\n",
            "        Total Tokens: 2391235\n",
            "        Estimated Cost: $4.7825\n",
            "        \n",
            "Checkpoint saved: 3155 batches, 31560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3155\n",
            "Processed 31560 pairs so far\n",
            "\n",
            "        API Calls: 6312\n",
            "        Total Tokens: 2395531\n",
            "        Estimated Cost: $4.7911\n",
            "        \n",
            "Checkpoint saved: 3160 batches, 31610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3160\n",
            "Processed 31610 pairs so far\n",
            "\n",
            "        API Calls: 6322\n",
            "        Total Tokens: 2398938\n",
            "        Estimated Cost: $4.7979\n",
            "        \n",
            "Checkpoint saved: 3165 batches, 31660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3165\n",
            "Processed 31660 pairs so far\n",
            "\n",
            "        API Calls: 6332\n",
            "        Total Tokens: 2402215\n",
            "        Estimated Cost: $4.8044\n",
            "        \n",
            "Checkpoint saved: 3170 batches, 31710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3170\n",
            "Processed 31710 pairs so far\n",
            "\n",
            "        API Calls: 6342\n",
            "        Total Tokens: 2406218\n",
            "        Estimated Cost: $4.8124\n",
            "        \n",
            "Checkpoint saved: 3175 batches, 31760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3175\n",
            "Processed 31760 pairs so far\n",
            "\n",
            "        API Calls: 6352\n",
            "        Total Tokens: 2410229\n",
            "        Estimated Cost: $4.8205\n",
            "        \n",
            "Checkpoint saved: 3180 batches, 31810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3180\n",
            "Processed 31810 pairs so far\n",
            "\n",
            "        API Calls: 6362\n",
            "        Total Tokens: 2413824\n",
            "        Estimated Cost: $4.8276\n",
            "        \n",
            "Checkpoint saved: 3185 batches, 31860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3185\n",
            "Processed 31860 pairs so far\n",
            "\n",
            "        API Calls: 6372\n",
            "        Total Tokens: 2417192\n",
            "        Estimated Cost: $4.8344\n",
            "        \n",
            "Checkpoint saved: 3190 batches, 31910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3190\n",
            "Processed 31910 pairs so far\n",
            "\n",
            "        API Calls: 6382\n",
            "        Total Tokens: 2420599\n",
            "        Estimated Cost: $4.8412\n",
            "        \n",
            "Checkpoint saved: 3195 batches, 31960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3195\n",
            "Processed 31960 pairs so far\n",
            "\n",
            "        API Calls: 6392\n",
            "        Total Tokens: 2424427\n",
            "        Estimated Cost: $4.8489\n",
            "        \n",
            "Checkpoint saved: 3200 batches, 32010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3200\n",
            "Processed 32010 pairs so far\n",
            "\n",
            "        API Calls: 6402\n",
            "        Total Tokens: 2428352\n",
            "        Estimated Cost: $4.8567\n",
            "        \n",
            "Checkpoint saved: 3205 batches, 32060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3205\n",
            "Processed 32060 pairs so far\n",
            "\n",
            "        API Calls: 6412\n",
            "        Total Tokens: 2432091\n",
            "        Estimated Cost: $4.8642\n",
            "        \n",
            "Checkpoint saved: 3210 batches, 32110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3210\n",
            "Processed 32110 pairs so far\n",
            "\n",
            "        API Calls: 6422\n",
            "        Total Tokens: 2436029\n",
            "        Estimated Cost: $4.8721\n",
            "        \n",
            "Checkpoint saved: 3215 batches, 32160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3215\n",
            "Processed 32160 pairs so far\n",
            "\n",
            "        API Calls: 6432\n",
            "        Total Tokens: 2439852\n",
            "        Estimated Cost: $4.8797\n",
            "        \n",
            "Checkpoint saved: 3220 batches, 32210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3220\n",
            "Processed 32210 pairs so far\n",
            "\n",
            "        API Calls: 6442\n",
            "        Total Tokens: 2443506\n",
            "        Estimated Cost: $4.8870\n",
            "        \n",
            "Checkpoint saved: 3225 batches, 32260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3225\n",
            "Processed 32260 pairs so far\n",
            "\n",
            "        API Calls: 6452\n",
            "        Total Tokens: 2447216\n",
            "        Estimated Cost: $4.8944\n",
            "        \n",
            "Checkpoint saved: 3230 batches, 32310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3230\n",
            "Processed 32310 pairs so far\n",
            "\n",
            "        API Calls: 6462\n",
            "        Total Tokens: 2450994\n",
            "        Estimated Cost: $4.9020\n",
            "        \n",
            "Checkpoint saved: 3235 batches, 32360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3235\n",
            "Processed 32360 pairs so far\n",
            "\n",
            "        API Calls: 6472\n",
            "        Total Tokens: 2454766\n",
            "        Estimated Cost: $4.9095\n",
            "        \n",
            "Checkpoint saved: 3240 batches, 32410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3240\n",
            "Processed 32410 pairs so far\n",
            "\n",
            "        API Calls: 6482\n",
            "        Total Tokens: 2458925\n",
            "        Estimated Cost: $4.9179\n",
            "        \n",
            "Checkpoint saved: 3245 batches, 32460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3245\n",
            "Processed 32460 pairs so far\n",
            "\n",
            "        API Calls: 6492\n",
            "        Total Tokens: 2462772\n",
            "        Estimated Cost: $4.9255\n",
            "        \n",
            "Checkpoint saved: 3250 batches, 32510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3250\n",
            "Processed 32510 pairs so far\n",
            "\n",
            "        API Calls: 6502\n",
            "        Total Tokens: 2466888\n",
            "        Estimated Cost: $4.9338\n",
            "        \n",
            "Checkpoint saved: 3255 batches, 32560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3255\n",
            "Processed 32560 pairs so far\n",
            "\n",
            "        API Calls: 6512\n",
            "        Total Tokens: 2470864\n",
            "        Estimated Cost: $4.9417\n",
            "        \n",
            "Checkpoint saved: 3260 batches, 32610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3260\n",
            "Processed 32610 pairs so far\n",
            "\n",
            "        API Calls: 6522\n",
            "        Total Tokens: 2474100\n",
            "        Estimated Cost: $4.9482\n",
            "        \n",
            "Checkpoint saved: 3265 batches, 32660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3265\n",
            "Processed 32660 pairs so far\n",
            "\n",
            "        API Calls: 6532\n",
            "        Total Tokens: 2477474\n",
            "        Estimated Cost: $4.9549\n",
            "        \n",
            "Checkpoint saved: 3270 batches, 32710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3270\n",
            "Processed 32710 pairs so far\n",
            "\n",
            "        API Calls: 6542\n",
            "        Total Tokens: 2481143\n",
            "        Estimated Cost: $4.9623\n",
            "        \n",
            "Checkpoint saved: 3275 batches, 32760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3275\n",
            "Processed 32760 pairs so far\n",
            "\n",
            "        API Calls: 6552\n",
            "        Total Tokens: 2484798\n",
            "        Estimated Cost: $4.9696\n",
            "        \n",
            "Checkpoint saved: 3280 batches, 32810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3280\n",
            "Processed 32810 pairs so far\n",
            "\n",
            "        API Calls: 6562\n",
            "        Total Tokens: 2488076\n",
            "        Estimated Cost: $4.9762\n",
            "        \n",
            "Checkpoint saved: 3285 batches, 32860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3285\n",
            "Processed 32860 pairs so far\n",
            "\n",
            "        API Calls: 6572\n",
            "        Total Tokens: 2491735\n",
            "        Estimated Cost: $4.9835\n",
            "        \n",
            "Checkpoint saved: 3290 batches, 32910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3290\n",
            "Processed 32910 pairs so far\n",
            "\n",
            "        API Calls: 6582\n",
            "        Total Tokens: 2495563\n",
            "        Estimated Cost: $4.9911\n",
            "        \n",
            "Checkpoint saved: 3295 batches, 32960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3295\n",
            "Processed 32960 pairs so far\n",
            "\n",
            "        API Calls: 6592\n",
            "        Total Tokens: 2499551\n",
            "        Estimated Cost: $4.9991\n",
            "        \n",
            "Checkpoint saved: 3300 batches, 33010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3300\n",
            "Processed 33010 pairs so far\n",
            "\n",
            "        API Calls: 6602\n",
            "        Total Tokens: 2503497\n",
            "        Estimated Cost: $5.0070\n",
            "        \n",
            "Checkpoint saved: 3305 batches, 33060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3305\n",
            "Processed 33060 pairs so far\n",
            "\n",
            "        API Calls: 6612\n",
            "        Total Tokens: 2507275\n",
            "        Estimated Cost: $5.0146\n",
            "        \n",
            "Checkpoint saved: 3310 batches, 33110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3310\n",
            "Processed 33110 pairs so far\n",
            "\n",
            "        API Calls: 6622\n",
            "        Total Tokens: 2511295\n",
            "        Estimated Cost: $5.0226\n",
            "        \n",
            "Checkpoint saved: 3315 batches, 33160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3315\n",
            "Processed 33160 pairs so far\n",
            "\n",
            "        API Calls: 6632\n",
            "        Total Tokens: 2515012\n",
            "        Estimated Cost: $5.0300\n",
            "        \n",
            "Checkpoint saved: 3320 batches, 33210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3320\n",
            "Processed 33210 pairs so far\n",
            "\n",
            "        API Calls: 6642\n",
            "        Total Tokens: 2518540\n",
            "        Estimated Cost: $5.0371\n",
            "        \n",
            "Checkpoint saved: 3325 batches, 33260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3325\n",
            "Processed 33260 pairs so far\n",
            "\n",
            "        API Calls: 6652\n",
            "        Total Tokens: 2522311\n",
            "        Estimated Cost: $5.0446\n",
            "        \n",
            "Checkpoint saved: 3330 batches, 33310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3330\n",
            "Processed 33310 pairs so far\n",
            "\n",
            "        API Calls: 6662\n",
            "        Total Tokens: 2526041\n",
            "        Estimated Cost: $5.0521\n",
            "        \n",
            "Checkpoint saved: 3335 batches, 33360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3335\n",
            "Processed 33360 pairs so far\n",
            "\n",
            "        API Calls: 6672\n",
            "        Total Tokens: 2529929\n",
            "        Estimated Cost: $5.0599\n",
            "        \n",
            "Checkpoint saved: 3340 batches, 33410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3340\n",
            "Processed 33410 pairs so far\n",
            "\n",
            "        API Calls: 6682\n",
            "        Total Tokens: 2533697\n",
            "        Estimated Cost: $5.0674\n",
            "        \n",
            "Checkpoint saved: 3345 batches, 33460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3345\n",
            "Processed 33460 pairs so far\n",
            "\n",
            "        API Calls: 6692\n",
            "        Total Tokens: 2537623\n",
            "        Estimated Cost: $5.0752\n",
            "        \n",
            "Checkpoint saved: 3350 batches, 33510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3350\n",
            "Processed 33510 pairs so far\n",
            "\n",
            "        API Calls: 6702\n",
            "        Total Tokens: 2541119\n",
            "        Estimated Cost: $5.0822\n",
            "        \n",
            "Checkpoint saved: 3355 batches, 33560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3355\n",
            "Processed 33560 pairs so far\n",
            "\n",
            "        API Calls: 6712\n",
            "        Total Tokens: 2545202\n",
            "        Estimated Cost: $5.0904\n",
            "        \n",
            "Checkpoint saved: 3360 batches, 33610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3360\n",
            "Processed 33610 pairs so far\n",
            "\n",
            "        API Calls: 6722\n",
            "        Total Tokens: 2548863\n",
            "        Estimated Cost: $5.0977\n",
            "        \n",
            "Checkpoint saved: 3365 batches, 33660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3365\n",
            "Processed 33660 pairs so far\n",
            "\n",
            "        API Calls: 6732\n",
            "        Total Tokens: 2552327\n",
            "        Estimated Cost: $5.1047\n",
            "        \n",
            "Checkpoint saved: 3370 batches, 33710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3370\n",
            "Processed 33710 pairs so far\n",
            "\n",
            "        API Calls: 6742\n",
            "        Total Tokens: 2556027\n",
            "        Estimated Cost: $5.1121\n",
            "        \n",
            "Checkpoint saved: 3375 batches, 33760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3375\n",
            "Processed 33760 pairs so far\n",
            "\n",
            "        API Calls: 6752\n",
            "        Total Tokens: 2559400\n",
            "        Estimated Cost: $5.1188\n",
            "        \n",
            "Checkpoint saved: 3380 batches, 33810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3380\n",
            "Processed 33810 pairs so far\n",
            "\n",
            "        API Calls: 6762\n",
            "        Total Tokens: 2563016\n",
            "        Estimated Cost: $5.1260\n",
            "        \n",
            "Checkpoint saved: 3385 batches, 33860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3385\n",
            "Processed 33860 pairs so far\n",
            "\n",
            "        API Calls: 6772\n",
            "        Total Tokens: 2566657\n",
            "        Estimated Cost: $5.1333\n",
            "        \n",
            "Checkpoint saved: 3390 batches, 33910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3390\n",
            "Processed 33910 pairs so far\n",
            "\n",
            "        API Calls: 6782\n",
            "        Total Tokens: 2570301\n",
            "        Estimated Cost: $5.1406\n",
            "        \n",
            "Checkpoint saved: 3395 batches, 33960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3395\n",
            "Processed 33960 pairs so far\n",
            "\n",
            "        API Calls: 6792\n",
            "        Total Tokens: 2573962\n",
            "        Estimated Cost: $5.1479\n",
            "        \n",
            "Checkpoint saved: 3400 batches, 34010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3400\n",
            "Processed 34010 pairs so far\n",
            "\n",
            "        API Calls: 6802\n",
            "        Total Tokens: 2577833\n",
            "        Estimated Cost: $5.1557\n",
            "        \n",
            "Checkpoint saved: 3405 batches, 34060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3405\n",
            "Processed 34060 pairs so far\n",
            "\n",
            "        API Calls: 6812\n",
            "        Total Tokens: 2581921\n",
            "        Estimated Cost: $5.1638\n",
            "        \n",
            "Checkpoint saved: 3410 batches, 34110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3410\n",
            "Processed 34110 pairs so far\n",
            "\n",
            "        API Calls: 6822\n",
            "        Total Tokens: 2586232\n",
            "        Estimated Cost: $5.1725\n",
            "        \n",
            "Checkpoint saved: 3415 batches, 34160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3415\n",
            "Processed 34160 pairs so far\n",
            "\n",
            "        API Calls: 6832\n",
            "        Total Tokens: 2589980\n",
            "        Estimated Cost: $5.1800\n",
            "        \n",
            "Checkpoint saved: 3420 batches, 34210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3420\n",
            "Processed 34210 pairs so far\n",
            "\n",
            "        API Calls: 6842\n",
            "        Total Tokens: 2593862\n",
            "        Estimated Cost: $5.1877\n",
            "        \n",
            "Checkpoint saved: 3425 batches, 34260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3425\n",
            "Processed 34260 pairs so far\n",
            "\n",
            "        API Calls: 6852\n",
            "        Total Tokens: 2597552\n",
            "        Estimated Cost: $5.1951\n",
            "        \n",
            "Checkpoint saved: 3430 batches, 34310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3430\n",
            "Processed 34310 pairs so far\n",
            "\n",
            "        API Calls: 6862\n",
            "        Total Tokens: 2601568\n",
            "        Estimated Cost: $5.2031\n",
            "        \n",
            "Checkpoint saved: 3435 batches, 34360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3435\n",
            "Processed 34360 pairs so far\n",
            "\n",
            "        API Calls: 6872\n",
            "        Total Tokens: 2605660\n",
            "        Estimated Cost: $5.2113\n",
            "        \n",
            "Checkpoint saved: 3440 batches, 34410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3440\n",
            "Processed 34410 pairs so far\n",
            "\n",
            "        API Calls: 6882\n",
            "        Total Tokens: 2609147\n",
            "        Estimated Cost: $5.2183\n",
            "        \n",
            "Checkpoint saved: 3445 batches, 34460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3445\n",
            "Processed 34460 pairs so far\n",
            "\n",
            "        API Calls: 6892\n",
            "        Total Tokens: 2612830\n",
            "        Estimated Cost: $5.2257\n",
            "        \n",
            "Checkpoint saved: 3450 batches, 34510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3450\n",
            "Processed 34510 pairs so far\n",
            "\n",
            "        API Calls: 6902\n",
            "        Total Tokens: 2616758\n",
            "        Estimated Cost: $5.2335\n",
            "        \n",
            "Checkpoint saved: 3455 batches, 34560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3455\n",
            "Processed 34560 pairs so far\n",
            "\n",
            "        API Calls: 6912\n",
            "        Total Tokens: 2620774\n",
            "        Estimated Cost: $5.2415\n",
            "        \n",
            "Checkpoint saved: 3460 batches, 34610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3460\n",
            "Processed 34610 pairs so far\n",
            "\n",
            "        API Calls: 6922\n",
            "        Total Tokens: 2624731\n",
            "        Estimated Cost: $5.2495\n",
            "        \n",
            "Checkpoint saved: 3465 batches, 34660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3465\n",
            "Processed 34660 pairs so far\n",
            "\n",
            "        API Calls: 6932\n",
            "        Total Tokens: 2628434\n",
            "        Estimated Cost: $5.2569\n",
            "        \n",
            "Checkpoint saved: 3470 batches, 34710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3470\n",
            "Processed 34710 pairs so far\n",
            "\n",
            "        API Calls: 6942\n",
            "        Total Tokens: 2631921\n",
            "        Estimated Cost: $5.2638\n",
            "        \n",
            "Checkpoint saved: 3475 batches, 34760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3475\n",
            "Processed 34760 pairs so far\n",
            "\n",
            "        API Calls: 6952\n",
            "        Total Tokens: 2635459\n",
            "        Estimated Cost: $5.2709\n",
            "        \n",
            "Checkpoint saved: 3480 batches, 34810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3480\n",
            "Processed 34810 pairs so far\n",
            "\n",
            "        API Calls: 6962\n",
            "        Total Tokens: 2639209\n",
            "        Estimated Cost: $5.2784\n",
            "        \n",
            "Checkpoint saved: 3485 batches, 34860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3485\n",
            "Processed 34860 pairs so far\n",
            "\n",
            "        API Calls: 6972\n",
            "        Total Tokens: 2643203\n",
            "        Estimated Cost: $5.2864\n",
            "        \n",
            "Checkpoint saved: 3490 batches, 34910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3490\n",
            "Processed 34910 pairs so far\n",
            "\n",
            "        API Calls: 6982\n",
            "        Total Tokens: 2646706\n",
            "        Estimated Cost: $5.2934\n",
            "        \n",
            "Checkpoint saved: 3495 batches, 34960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3495\n",
            "Processed 34960 pairs so far\n",
            "\n",
            "        API Calls: 6992\n",
            "        Total Tokens: 2650432\n",
            "        Estimated Cost: $5.3009\n",
            "        \n",
            "Checkpoint saved: 3500 batches, 35010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3500\n",
            "Processed 35010 pairs so far\n",
            "\n",
            "        API Calls: 7002\n",
            "        Total Tokens: 2654208\n",
            "        Estimated Cost: $5.3084\n",
            "        \n",
            "Checkpoint saved: 3505 batches, 35060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3505\n",
            "Processed 35060 pairs so far\n",
            "\n",
            "        API Calls: 7012\n",
            "        Total Tokens: 2658272\n",
            "        Estimated Cost: $5.3165\n",
            "        \n",
            "Checkpoint saved: 3510 batches, 35110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3510\n",
            "Processed 35110 pairs so far\n",
            "\n",
            "        API Calls: 7022\n",
            "        Total Tokens: 2662052\n",
            "        Estimated Cost: $5.3241\n",
            "        \n",
            "Checkpoint saved: 3515 batches, 35160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3515\n",
            "Processed 35160 pairs so far\n",
            "\n",
            "        API Calls: 7032\n",
            "        Total Tokens: 2665806\n",
            "        Estimated Cost: $5.3316\n",
            "        \n",
            "Checkpoint saved: 3520 batches, 35210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3520\n",
            "Processed 35210 pairs so far\n",
            "\n",
            "        API Calls: 7042\n",
            "        Total Tokens: 2669676\n",
            "        Estimated Cost: $5.3394\n",
            "        \n",
            "Checkpoint saved: 3525 batches, 35260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3525\n",
            "Processed 35260 pairs so far\n",
            "\n",
            "        API Calls: 7052\n",
            "        Total Tokens: 2673393\n",
            "        Estimated Cost: $5.3468\n",
            "        \n",
            "Checkpoint saved: 3530 batches, 35310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3530\n",
            "Processed 35310 pairs so far\n",
            "\n",
            "        API Calls: 7062\n",
            "        Total Tokens: 2677042\n",
            "        Estimated Cost: $5.3541\n",
            "        \n",
            "Checkpoint saved: 3535 batches, 35360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3535\n",
            "Processed 35360 pairs so far\n",
            "\n",
            "        API Calls: 7072\n",
            "        Total Tokens: 2680887\n",
            "        Estimated Cost: $5.3618\n",
            "        \n",
            "Checkpoint saved: 3540 batches, 35410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3540\n",
            "Processed 35410 pairs so far\n",
            "\n",
            "        API Calls: 7082\n",
            "        Total Tokens: 2684127\n",
            "        Estimated Cost: $5.3683\n",
            "        \n",
            "Checkpoint saved: 3545 batches, 35460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3545\n",
            "Processed 35460 pairs so far\n",
            "\n",
            "        API Calls: 7092\n",
            "        Total Tokens: 2688112\n",
            "        Estimated Cost: $5.3762\n",
            "        \n",
            "Checkpoint saved: 3550 batches, 35510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3550\n",
            "Processed 35510 pairs so far\n",
            "\n",
            "        API Calls: 7102\n",
            "        Total Tokens: 2691778\n",
            "        Estimated Cost: $5.3836\n",
            "        \n",
            "Checkpoint saved: 3555 batches, 35560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3555\n",
            "Processed 35560 pairs so far\n",
            "\n",
            "        API Calls: 7112\n",
            "        Total Tokens: 2695331\n",
            "        Estimated Cost: $5.3907\n",
            "        \n",
            "Checkpoint saved: 3560 batches, 35610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3560\n",
            "Processed 35610 pairs so far\n",
            "\n",
            "        API Calls: 7122\n",
            "        Total Tokens: 2699084\n",
            "        Estimated Cost: $5.3982\n",
            "        \n",
            "Checkpoint saved: 3565 batches, 35660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3565\n",
            "Processed 35660 pairs so far\n",
            "\n",
            "        API Calls: 7132\n",
            "        Total Tokens: 2702735\n",
            "        Estimated Cost: $5.4055\n",
            "        \n",
            "Checkpoint saved: 3570 batches, 35710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3570\n",
            "Processed 35710 pairs so far\n",
            "\n",
            "        API Calls: 7142\n",
            "        Total Tokens: 2706354\n",
            "        Estimated Cost: $5.4127\n",
            "        \n",
            "Checkpoint saved: 3575 batches, 35760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3575\n",
            "Processed 35760 pairs so far\n",
            "\n",
            "        API Calls: 7152\n",
            "        Total Tokens: 2710032\n",
            "        Estimated Cost: $5.4201\n",
            "        \n",
            "Checkpoint saved: 3580 batches, 35810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3580\n",
            "Processed 35810 pairs so far\n",
            "\n",
            "        API Calls: 7162\n",
            "        Total Tokens: 2713819\n",
            "        Estimated Cost: $5.4276\n",
            "        \n",
            "Checkpoint saved: 3585 batches, 35860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3585\n",
            "Processed 35860 pairs so far\n",
            "\n",
            "        API Calls: 7172\n",
            "        Total Tokens: 2717664\n",
            "        Estimated Cost: $5.4353\n",
            "        \n",
            "Checkpoint saved: 3590 batches, 35910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3590\n",
            "Processed 35910 pairs so far\n",
            "\n",
            "        API Calls: 7182\n",
            "        Total Tokens: 2721215\n",
            "        Estimated Cost: $5.4424\n",
            "        \n",
            "Checkpoint saved: 3595 batches, 35960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3595\n",
            "Processed 35960 pairs so far\n",
            "\n",
            "        API Calls: 7192\n",
            "        Total Tokens: 2725222\n",
            "        Estimated Cost: $5.4504\n",
            "        \n",
            "Checkpoint saved: 3600 batches, 36010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3600\n",
            "Processed 36010 pairs so far\n",
            "\n",
            "        API Calls: 7202\n",
            "        Total Tokens: 2729409\n",
            "        Estimated Cost: $5.4588\n",
            "        \n",
            "Checkpoint saved: 3605 batches, 36060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3605\n",
            "Processed 36060 pairs so far\n",
            "\n",
            "        API Calls: 7212\n",
            "        Total Tokens: 2732973\n",
            "        Estimated Cost: $5.4659\n",
            "        \n",
            "Checkpoint saved: 3610 batches, 36110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3610\n",
            "Processed 36110 pairs so far\n",
            "\n",
            "        API Calls: 7222\n",
            "        Total Tokens: 2736303\n",
            "        Estimated Cost: $5.4726\n",
            "        \n",
            "Checkpoint saved: 3615 batches, 36160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3615\n",
            "Processed 36160 pairs so far\n",
            "\n",
            "        API Calls: 7232\n",
            "        Total Tokens: 2740144\n",
            "        Estimated Cost: $5.4803\n",
            "        \n",
            "Checkpoint saved: 3620 batches, 36210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3620\n",
            "Processed 36210 pairs so far\n",
            "\n",
            "        API Calls: 7242\n",
            "        Total Tokens: 2743648\n",
            "        Estimated Cost: $5.4873\n",
            "        \n",
            "Checkpoint saved: 3625 batches, 36260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3625\n",
            "Processed 36260 pairs so far\n",
            "\n",
            "        API Calls: 7252\n",
            "        Total Tokens: 2747427\n",
            "        Estimated Cost: $5.4949\n",
            "        \n",
            "Checkpoint saved: 3630 batches, 36310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3630\n",
            "Processed 36310 pairs so far\n",
            "\n",
            "        API Calls: 7262\n",
            "        Total Tokens: 2751328\n",
            "        Estimated Cost: $5.5027\n",
            "        \n",
            "Checkpoint saved: 3635 batches, 36360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3635\n",
            "Processed 36360 pairs so far\n",
            "\n",
            "        API Calls: 7272\n",
            "        Total Tokens: 2755013\n",
            "        Estimated Cost: $5.5100\n",
            "        \n",
            "Checkpoint saved: 3640 batches, 36410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3640\n",
            "Processed 36410 pairs so far\n",
            "\n",
            "        API Calls: 7282\n",
            "        Total Tokens: 2758528\n",
            "        Estimated Cost: $5.5171\n",
            "        \n",
            "Checkpoint saved: 3645 batches, 36460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3645\n",
            "Processed 36460 pairs so far\n",
            "\n",
            "        API Calls: 7292\n",
            "        Total Tokens: 2762383\n",
            "        Estimated Cost: $5.5248\n",
            "        \n",
            "Checkpoint saved: 3650 batches, 36510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3650\n",
            "Processed 36510 pairs so far\n",
            "\n",
            "        API Calls: 7302\n",
            "        Total Tokens: 2765772\n",
            "        Estimated Cost: $5.5315\n",
            "        \n",
            "Checkpoint saved: 3655 batches, 36560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3655\n",
            "Processed 36560 pairs so far\n",
            "\n",
            "        API Calls: 7312\n",
            "        Total Tokens: 2769433\n",
            "        Estimated Cost: $5.5389\n",
            "        \n",
            "Checkpoint saved: 3660 batches, 36610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3660\n",
            "Processed 36610 pairs so far\n",
            "\n",
            "        API Calls: 7322\n",
            "        Total Tokens: 2773036\n",
            "        Estimated Cost: $5.5461\n",
            "        \n",
            "Checkpoint saved: 3665 batches, 36660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3665\n",
            "Processed 36660 pairs so far\n",
            "\n",
            "        API Calls: 7332\n",
            "        Total Tokens: 2776821\n",
            "        Estimated Cost: $5.5536\n",
            "        \n",
            "Checkpoint saved: 3670 batches, 36710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3670\n",
            "Processed 36710 pairs so far\n",
            "\n",
            "        API Calls: 7342\n",
            "        Total Tokens: 2780894\n",
            "        Estimated Cost: $5.5618\n",
            "        \n",
            "Checkpoint saved: 3675 batches, 36760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3675\n",
            "Processed 36760 pairs so far\n",
            "\n",
            "        API Calls: 7352\n",
            "        Total Tokens: 2784627\n",
            "        Estimated Cost: $5.5693\n",
            "        \n",
            "Checkpoint saved: 3680 batches, 36810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3680\n",
            "Processed 36810 pairs so far\n",
            "\n",
            "        API Calls: 7362\n",
            "        Total Tokens: 2788748\n",
            "        Estimated Cost: $5.5775\n",
            "        \n",
            "Checkpoint saved: 3685 batches, 36860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3685\n",
            "Processed 36860 pairs so far\n",
            "\n",
            "        API Calls: 7372\n",
            "        Total Tokens: 2792935\n",
            "        Estimated Cost: $5.5859\n",
            "        \n",
            "Checkpoint saved: 3690 batches, 36910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3690\n",
            "Processed 36910 pairs so far\n",
            "\n",
            "        API Calls: 7382\n",
            "        Total Tokens: 2796962\n",
            "        Estimated Cost: $5.5939\n",
            "        \n",
            "Checkpoint saved: 3695 batches, 36960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3695\n",
            "Processed 36960 pairs so far\n",
            "\n",
            "        API Calls: 7392\n",
            "        Total Tokens: 2800512\n",
            "        Estimated Cost: $5.6010\n",
            "        \n",
            "Checkpoint saved: 3700 batches, 37010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3700\n",
            "Processed 37010 pairs so far\n",
            "\n",
            "        API Calls: 7402\n",
            "        Total Tokens: 2804308\n",
            "        Estimated Cost: $5.6086\n",
            "        \n",
            "Checkpoint saved: 3705 batches, 37060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3705\n",
            "Processed 37060 pairs so far\n",
            "\n",
            "        API Calls: 7412\n",
            "        Total Tokens: 2808323\n",
            "        Estimated Cost: $5.6166\n",
            "        \n",
            "Checkpoint saved: 3710 batches, 37110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3710\n",
            "Processed 37110 pairs so far\n",
            "\n",
            "        API Calls: 7422\n",
            "        Total Tokens: 2811792\n",
            "        Estimated Cost: $5.6236\n",
            "        \n",
            "Checkpoint saved: 3715 batches, 37160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3715\n",
            "Processed 37160 pairs so far\n",
            "\n",
            "        API Calls: 7432\n",
            "        Total Tokens: 2815152\n",
            "        Estimated Cost: $5.6303\n",
            "        \n",
            "Checkpoint saved: 3720 batches, 37210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3720\n",
            "Processed 37210 pairs so far\n",
            "\n",
            "        API Calls: 7442\n",
            "        Total Tokens: 2818546\n",
            "        Estimated Cost: $5.6371\n",
            "        \n",
            "Checkpoint saved: 3725 batches, 37260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3725\n",
            "Processed 37260 pairs so far\n",
            "\n",
            "        API Calls: 7452\n",
            "        Total Tokens: 2822146\n",
            "        Estimated Cost: $5.6443\n",
            "        \n",
            "Checkpoint saved: 3730 batches, 37310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3730\n",
            "Processed 37310 pairs so far\n",
            "\n",
            "        API Calls: 7462\n",
            "        Total Tokens: 2825431\n",
            "        Estimated Cost: $5.6509\n",
            "        \n",
            "Checkpoint saved: 3735 batches, 37360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3735\n",
            "Processed 37360 pairs so far\n",
            "\n",
            "        API Calls: 7472\n",
            "        Total Tokens: 2829389\n",
            "        Estimated Cost: $5.6588\n",
            "        \n",
            "Checkpoint saved: 3740 batches, 37410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3740\n",
            "Processed 37410 pairs so far\n",
            "\n",
            "        API Calls: 7482\n",
            "        Total Tokens: 2832810\n",
            "        Estimated Cost: $5.6656\n",
            "        \n",
            "Checkpoint saved: 3745 batches, 37460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3745\n",
            "Processed 37460 pairs so far\n",
            "\n",
            "        API Calls: 7492\n",
            "        Total Tokens: 2836513\n",
            "        Estimated Cost: $5.6730\n",
            "        \n",
            "Checkpoint saved: 3750 batches, 37510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3750\n",
            "Processed 37510 pairs so far\n",
            "\n",
            "        API Calls: 7502\n",
            "        Total Tokens: 2840380\n",
            "        Estimated Cost: $5.6808\n",
            "        \n",
            "Checkpoint saved: 3755 batches, 37560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3755\n",
            "Processed 37560 pairs so far\n",
            "\n",
            "        API Calls: 7512\n",
            "        Total Tokens: 2844582\n",
            "        Estimated Cost: $5.6892\n",
            "        \n",
            "Checkpoint saved: 3760 batches, 37610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3760\n",
            "Processed 37610 pairs so far\n",
            "\n",
            "        API Calls: 7522\n",
            "        Total Tokens: 2848554\n",
            "        Estimated Cost: $5.6971\n",
            "        \n",
            "Checkpoint saved: 3765 batches, 37660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3765\n",
            "Processed 37660 pairs so far\n",
            "\n",
            "        API Calls: 7532\n",
            "        Total Tokens: 2852411\n",
            "        Estimated Cost: $5.7048\n",
            "        \n",
            "Checkpoint saved: 3770 batches, 37710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3770\n",
            "Processed 37710 pairs so far\n",
            "\n",
            "        API Calls: 7542\n",
            "        Total Tokens: 2856475\n",
            "        Estimated Cost: $5.7130\n",
            "        \n",
            "Checkpoint saved: 3775 batches, 37760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3775\n",
            "Processed 37760 pairs so far\n",
            "\n",
            "        API Calls: 7552\n",
            "        Total Tokens: 2859858\n",
            "        Estimated Cost: $5.7197\n",
            "        \n",
            "Checkpoint saved: 3780 batches, 37810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3780\n",
            "Processed 37810 pairs so far\n",
            "\n",
            "        API Calls: 7562\n",
            "        Total Tokens: 2863573\n",
            "        Estimated Cost: $5.7271\n",
            "        \n",
            "Checkpoint saved: 3785 batches, 37860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3785\n",
            "Processed 37860 pairs so far\n",
            "\n",
            "        API Calls: 7572\n",
            "        Total Tokens: 2867033\n",
            "        Estimated Cost: $5.7341\n",
            "        \n",
            "Checkpoint saved: 3790 batches, 37910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3790\n",
            "Processed 37910 pairs so far\n",
            "\n",
            "        API Calls: 7582\n",
            "        Total Tokens: 2870662\n",
            "        Estimated Cost: $5.7413\n",
            "        \n",
            "Checkpoint saved: 3795 batches, 37960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3795\n",
            "Processed 37960 pairs so far\n",
            "\n",
            "        API Calls: 7592\n",
            "        Total Tokens: 2874435\n",
            "        Estimated Cost: $5.7489\n",
            "        \n",
            "Checkpoint saved: 3800 batches, 38010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3800\n",
            "Processed 38010 pairs so far\n",
            "\n",
            "        API Calls: 7602\n",
            "        Total Tokens: 2877975\n",
            "        Estimated Cost: $5.7560\n",
            "        \n",
            "Checkpoint saved: 3805 batches, 38060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3805\n",
            "Processed 38060 pairs so far\n",
            "\n",
            "        API Calls: 7612\n",
            "        Total Tokens: 2881616\n",
            "        Estimated Cost: $5.7632\n",
            "        \n",
            "Checkpoint saved: 3810 batches, 38110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3810\n",
            "Processed 38110 pairs so far\n",
            "\n",
            "        API Calls: 7622\n",
            "        Total Tokens: 2885448\n",
            "        Estimated Cost: $5.7709\n",
            "        \n",
            "Checkpoint saved: 3815 batches, 38160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3815\n",
            "Processed 38160 pairs so far\n",
            "\n",
            "        API Calls: 7632\n",
            "        Total Tokens: 2889507\n",
            "        Estimated Cost: $5.7790\n",
            "        \n",
            "Checkpoint saved: 3820 batches, 38210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3820\n",
            "Processed 38210 pairs so far\n",
            "\n",
            "        API Calls: 7642\n",
            "        Total Tokens: 2893372\n",
            "        Estimated Cost: $5.7867\n",
            "        \n",
            "Checkpoint saved: 3825 batches, 38260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3825\n",
            "Processed 38260 pairs so far\n",
            "\n",
            "        API Calls: 7652\n",
            "        Total Tokens: 2897084\n",
            "        Estimated Cost: $5.7942\n",
            "        \n",
            "Checkpoint saved: 3830 batches, 38310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3830\n",
            "Processed 38310 pairs so far\n",
            "\n",
            "        API Calls: 7662\n",
            "        Total Tokens: 2900682\n",
            "        Estimated Cost: $5.8014\n",
            "        \n",
            "Checkpoint saved: 3835 batches, 38360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3835\n",
            "Processed 38360 pairs so far\n",
            "\n",
            "        API Calls: 7672\n",
            "        Total Tokens: 2904369\n",
            "        Estimated Cost: $5.8087\n",
            "        \n",
            "Checkpoint saved: 3840 batches, 38410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3840\n",
            "Processed 38410 pairs so far\n",
            "\n",
            "        API Calls: 7682\n",
            "        Total Tokens: 2908129\n",
            "        Estimated Cost: $5.8163\n",
            "        \n",
            "Checkpoint saved: 3845 batches, 38460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3845\n",
            "Processed 38460 pairs so far\n",
            "\n",
            "        API Calls: 7692\n",
            "        Total Tokens: 2912025\n",
            "        Estimated Cost: $5.8241\n",
            "        \n",
            "Checkpoint saved: 3850 batches, 38510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3850\n",
            "Processed 38510 pairs so far\n",
            "\n",
            "        API Calls: 7702\n",
            "        Total Tokens: 2916038\n",
            "        Estimated Cost: $5.8321\n",
            "        \n",
            "Checkpoint saved: 3855 batches, 38560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3855\n",
            "Processed 38560 pairs so far\n",
            "\n",
            "        API Calls: 7712\n",
            "        Total Tokens: 2919858\n",
            "        Estimated Cost: $5.8397\n",
            "        \n",
            "Checkpoint saved: 3860 batches, 38610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3860\n",
            "Processed 38610 pairs so far\n",
            "\n",
            "        API Calls: 7722\n",
            "        Total Tokens: 2923954\n",
            "        Estimated Cost: $5.8479\n",
            "        \n",
            "Checkpoint saved: 3865 batches, 38660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3865\n",
            "Processed 38660 pairs so far\n",
            "\n",
            "        API Calls: 7732\n",
            "        Total Tokens: 2927157\n",
            "        Estimated Cost: $5.8543\n",
            "        \n",
            "Checkpoint saved: 3870 batches, 38710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3870\n",
            "Processed 38710 pairs so far\n",
            "\n",
            "        API Calls: 7742\n",
            "        Total Tokens: 2930617\n",
            "        Estimated Cost: $5.8612\n",
            "        \n",
            "Checkpoint saved: 3875 batches, 38760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3875\n",
            "Processed 38760 pairs so far\n",
            "\n",
            "        API Calls: 7752\n",
            "        Total Tokens: 2934212\n",
            "        Estimated Cost: $5.8684\n",
            "        \n",
            "Checkpoint saved: 3880 batches, 38810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3880\n",
            "Processed 38810 pairs so far\n",
            "\n",
            "        API Calls: 7762\n",
            "        Total Tokens: 2937757\n",
            "        Estimated Cost: $5.8755\n",
            "        \n",
            "Checkpoint saved: 3885 batches, 38860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3885\n",
            "Processed 38860 pairs so far\n",
            "\n",
            "        API Calls: 7772\n",
            "        Total Tokens: 2941831\n",
            "        Estimated Cost: $5.8837\n",
            "        \n",
            "Checkpoint saved: 3890 batches, 38910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3890\n",
            "Processed 38910 pairs so far\n",
            "\n",
            "        API Calls: 7782\n",
            "        Total Tokens: 2945484\n",
            "        Estimated Cost: $5.8910\n",
            "        \n",
            "Checkpoint saved: 3895 batches, 38960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3895\n",
            "Processed 38960 pairs so far\n",
            "\n",
            "        API Calls: 7792\n",
            "        Total Tokens: 2949239\n",
            "        Estimated Cost: $5.8985\n",
            "        \n",
            "Checkpoint saved: 3900 batches, 39010 pairs\n",
            "\n",
            "Checkpoint saved at batch 3900\n",
            "Processed 39010 pairs so far\n",
            "\n",
            "        API Calls: 7802\n",
            "        Total Tokens: 2952798\n",
            "        Estimated Cost: $5.9056\n",
            "        \n",
            "Checkpoint saved: 3905 batches, 39060 pairs\n",
            "\n",
            "Checkpoint saved at batch 3905\n",
            "Processed 39060 pairs so far\n",
            "\n",
            "        API Calls: 7812\n",
            "        Total Tokens: 2956717\n",
            "        Estimated Cost: $5.9134\n",
            "        \n",
            "Checkpoint saved: 3910 batches, 39110 pairs\n",
            "\n",
            "Checkpoint saved at batch 3910\n",
            "Processed 39110 pairs so far\n",
            "\n",
            "        API Calls: 7822\n",
            "        Total Tokens: 2960331\n",
            "        Estimated Cost: $5.9207\n",
            "        \n",
            "Checkpoint saved: 3915 batches, 39160 pairs\n",
            "\n",
            "Checkpoint saved at batch 3915\n",
            "Processed 39160 pairs so far\n",
            "\n",
            "        API Calls: 7832\n",
            "        Total Tokens: 2964069\n",
            "        Estimated Cost: $5.9281\n",
            "        \n",
            "Checkpoint saved: 3920 batches, 39210 pairs\n",
            "\n",
            "Checkpoint saved at batch 3920\n",
            "Processed 39210 pairs so far\n",
            "\n",
            "        API Calls: 7842\n",
            "        Total Tokens: 2968104\n",
            "        Estimated Cost: $5.9362\n",
            "        \n",
            "Checkpoint saved: 3925 batches, 39260 pairs\n",
            "\n",
            "Checkpoint saved at batch 3925\n",
            "Processed 39260 pairs so far\n",
            "\n",
            "        API Calls: 7852\n",
            "        Total Tokens: 2971888\n",
            "        Estimated Cost: $5.9438\n",
            "        \n",
            "Checkpoint saved: 3930 batches, 39310 pairs\n",
            "\n",
            "Checkpoint saved at batch 3930\n",
            "Processed 39310 pairs so far\n",
            "\n",
            "        API Calls: 7862\n",
            "        Total Tokens: 2975778\n",
            "        Estimated Cost: $5.9516\n",
            "        \n",
            "Checkpoint saved: 3935 batches, 39360 pairs\n",
            "\n",
            "Checkpoint saved at batch 3935\n",
            "Processed 39360 pairs so far\n",
            "\n",
            "        API Calls: 7872\n",
            "        Total Tokens: 2979481\n",
            "        Estimated Cost: $5.9590\n",
            "        \n",
            "Checkpoint saved: 3940 batches, 39410 pairs\n",
            "\n",
            "Checkpoint saved at batch 3940\n",
            "Processed 39410 pairs so far\n",
            "\n",
            "        API Calls: 7882\n",
            "        Total Tokens: 2983048\n",
            "        Estimated Cost: $5.9661\n",
            "        \n",
            "Checkpoint saved: 3945 batches, 39460 pairs\n",
            "\n",
            "Checkpoint saved at batch 3945\n",
            "Processed 39460 pairs so far\n",
            "\n",
            "        API Calls: 7892\n",
            "        Total Tokens: 2986604\n",
            "        Estimated Cost: $5.9732\n",
            "        \n",
            "Checkpoint saved: 3950 batches, 39510 pairs\n",
            "\n",
            "Checkpoint saved at batch 3950\n",
            "Processed 39510 pairs so far\n",
            "\n",
            "        API Calls: 7902\n",
            "        Total Tokens: 2990285\n",
            "        Estimated Cost: $5.9806\n",
            "        \n",
            "Checkpoint saved: 3955 batches, 39560 pairs\n",
            "\n",
            "Checkpoint saved at batch 3955\n",
            "Processed 39560 pairs so far\n",
            "\n",
            "        API Calls: 7912\n",
            "        Total Tokens: 2994000\n",
            "        Estimated Cost: $5.9880\n",
            "        \n",
            "Checkpoint saved: 3960 batches, 39610 pairs\n",
            "\n",
            "Checkpoint saved at batch 3960\n",
            "Processed 39610 pairs so far\n",
            "\n",
            "        API Calls: 7922\n",
            "        Total Tokens: 2997679\n",
            "        Estimated Cost: $5.9954\n",
            "        \n",
            "Checkpoint saved: 3965 batches, 39660 pairs\n",
            "\n",
            "Checkpoint saved at batch 3965\n",
            "Processed 39660 pairs so far\n",
            "\n",
            "        API Calls: 7932\n",
            "        Total Tokens: 3001386\n",
            "        Estimated Cost: $6.0028\n",
            "        \n",
            "Checkpoint saved: 3970 batches, 39710 pairs\n",
            "\n",
            "Checkpoint saved at batch 3970\n",
            "Processed 39710 pairs so far\n",
            "\n",
            "        API Calls: 7942\n",
            "        Total Tokens: 3005140\n",
            "        Estimated Cost: $6.0103\n",
            "        \n",
            "Checkpoint saved: 3975 batches, 39760 pairs\n",
            "\n",
            "Checkpoint saved at batch 3975\n",
            "Processed 39760 pairs so far\n",
            "\n",
            "        API Calls: 7952\n",
            "        Total Tokens: 3009264\n",
            "        Estimated Cost: $6.0185\n",
            "        \n",
            "Checkpoint saved: 3980 batches, 39810 pairs\n",
            "\n",
            "Checkpoint saved at batch 3980\n",
            "Processed 39810 pairs so far\n",
            "\n",
            "        API Calls: 7962\n",
            "        Total Tokens: 3013312\n",
            "        Estimated Cost: $6.0266\n",
            "        \n",
            "Checkpoint saved: 3985 batches, 39860 pairs\n",
            "\n",
            "Checkpoint saved at batch 3985\n",
            "Processed 39860 pairs so far\n",
            "\n",
            "        API Calls: 7972\n",
            "        Total Tokens: 3017622\n",
            "        Estimated Cost: $6.0352\n",
            "        \n",
            "Checkpoint saved: 3990 batches, 39910 pairs\n",
            "\n",
            "Checkpoint saved at batch 3990\n",
            "Processed 39910 pairs so far\n",
            "\n",
            "        API Calls: 7982\n",
            "        Total Tokens: 3021541\n",
            "        Estimated Cost: $6.0431\n",
            "        \n",
            "Checkpoint saved: 3995 batches, 39960 pairs\n",
            "\n",
            "Checkpoint saved at batch 3995\n",
            "Processed 39960 pairs so far\n",
            "\n",
            "        API Calls: 7992\n",
            "        Total Tokens: 3025638\n",
            "        Estimated Cost: $6.0513\n",
            "        \n",
            "Checkpoint saved: 4000 batches, 40010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4000\n",
            "Processed 40010 pairs so far\n",
            "\n",
            "        API Calls: 8002\n",
            "        Total Tokens: 3029826\n",
            "        Estimated Cost: $6.0597\n",
            "        \n",
            "Checkpoint saved: 4005 batches, 40060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4005\n",
            "Processed 40060 pairs so far\n",
            "\n",
            "        API Calls: 8012\n",
            "        Total Tokens: 3033564\n",
            "        Estimated Cost: $6.0671\n",
            "        \n",
            "Checkpoint saved: 4010 batches, 40110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4010\n",
            "Processed 40110 pairs so far\n",
            "\n",
            "        API Calls: 8022\n",
            "        Total Tokens: 3037403\n",
            "        Estimated Cost: $6.0748\n",
            "        \n",
            "Checkpoint saved: 4015 batches, 40160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4015\n",
            "Processed 40160 pairs so far\n",
            "\n",
            "        API Calls: 8032\n",
            "        Total Tokens: 3041314\n",
            "        Estimated Cost: $6.0826\n",
            "        \n",
            "Checkpoint saved: 4020 batches, 40210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4020\n",
            "Processed 40210 pairs so far\n",
            "\n",
            "        API Calls: 8042\n",
            "        Total Tokens: 3044851\n",
            "        Estimated Cost: $6.0897\n",
            "        \n",
            "Checkpoint saved: 4025 batches, 40260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4025\n",
            "Processed 40260 pairs so far\n",
            "\n",
            "        API Calls: 8052\n",
            "        Total Tokens: 3048145\n",
            "        Estimated Cost: $6.0963\n",
            "        \n",
            "Checkpoint saved: 4030 batches, 40310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4030\n",
            "Processed 40310 pairs so far\n",
            "\n",
            "        API Calls: 8062\n",
            "        Total Tokens: 3052509\n",
            "        Estimated Cost: $6.1050\n",
            "        \n",
            "Checkpoint saved: 4035 batches, 40360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4035\n",
            "Processed 40360 pairs so far\n",
            "\n",
            "        API Calls: 8072\n",
            "        Total Tokens: 3056354\n",
            "        Estimated Cost: $6.1127\n",
            "        \n",
            "Checkpoint saved: 4040 batches, 40410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4040\n",
            "Processed 40410 pairs so far\n",
            "\n",
            "        API Calls: 8082\n",
            "        Total Tokens: 3059621\n",
            "        Estimated Cost: $6.1192\n",
            "        \n",
            "Checkpoint saved: 4045 batches, 40460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4045\n",
            "Processed 40460 pairs so far\n",
            "\n",
            "        API Calls: 8092\n",
            "        Total Tokens: 3062981\n",
            "        Estimated Cost: $6.1260\n",
            "        \n",
            "Checkpoint saved: 4050 batches, 40510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4050\n",
            "Processed 40510 pairs so far\n",
            "\n",
            "        API Calls: 8102\n",
            "        Total Tokens: 3066682\n",
            "        Estimated Cost: $6.1334\n",
            "        \n",
            "Checkpoint saved: 4055 batches, 40560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4055\n",
            "Processed 40560 pairs so far\n",
            "\n",
            "        API Calls: 8112\n",
            "        Total Tokens: 3070504\n",
            "        Estimated Cost: $6.1410\n",
            "        \n",
            "Checkpoint saved: 4060 batches, 40610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4060\n",
            "Processed 40610 pairs so far\n",
            "\n",
            "        API Calls: 8122\n",
            "        Total Tokens: 3074318\n",
            "        Estimated Cost: $6.1486\n",
            "        \n",
            "Checkpoint saved: 4065 batches, 40660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4065\n",
            "Processed 40660 pairs so far\n",
            "\n",
            "        API Calls: 8132\n",
            "        Total Tokens: 3078335\n",
            "        Estimated Cost: $6.1567\n",
            "        \n",
            "Checkpoint saved: 4070 batches, 40710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4070\n",
            "Processed 40710 pairs so far\n",
            "\n",
            "        API Calls: 8142\n",
            "        Total Tokens: 3081875\n",
            "        Estimated Cost: $6.1638\n",
            "        \n",
            "Checkpoint saved: 4075 batches, 40760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4075\n",
            "Processed 40760 pairs so far\n",
            "\n",
            "        API Calls: 8152\n",
            "        Total Tokens: 3086151\n",
            "        Estimated Cost: $6.1723\n",
            "        \n",
            "Checkpoint saved: 4080 batches, 40810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4080\n",
            "Processed 40810 pairs so far\n",
            "\n",
            "        API Calls: 8162\n",
            "        Total Tokens: 3090234\n",
            "        Estimated Cost: $6.1805\n",
            "        \n",
            "Checkpoint saved: 4085 batches, 40860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4085\n",
            "Processed 40860 pairs so far\n",
            "\n",
            "        API Calls: 8172\n",
            "        Total Tokens: 3093922\n",
            "        Estimated Cost: $6.1878\n",
            "        \n",
            "Checkpoint saved: 4090 batches, 40910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4090\n",
            "Processed 40910 pairs so far\n",
            "\n",
            "        API Calls: 8182\n",
            "        Total Tokens: 3097583\n",
            "        Estimated Cost: $6.1952\n",
            "        \n",
            "Checkpoint saved: 4095 batches, 40960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4095\n",
            "Processed 40960 pairs so far\n",
            "\n",
            "        API Calls: 8192\n",
            "        Total Tokens: 3101291\n",
            "        Estimated Cost: $6.2026\n",
            "        \n",
            "Checkpoint saved: 4100 batches, 41010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4100\n",
            "Processed 41010 pairs so far\n",
            "\n",
            "        API Calls: 8202\n",
            "        Total Tokens: 3104856\n",
            "        Estimated Cost: $6.2097\n",
            "        \n",
            "Checkpoint saved: 4105 batches, 41060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4105\n",
            "Processed 41060 pairs so far\n",
            "\n",
            "        API Calls: 8212\n",
            "        Total Tokens: 3108215\n",
            "        Estimated Cost: $6.2164\n",
            "        \n",
            "Checkpoint saved: 4110 batches, 41110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4110\n",
            "Processed 41110 pairs so far\n",
            "\n",
            "        API Calls: 8222\n",
            "        Total Tokens: 3111707\n",
            "        Estimated Cost: $6.2234\n",
            "        \n",
            "Checkpoint saved: 4115 batches, 41160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4115\n",
            "Processed 41160 pairs so far\n",
            "\n",
            "        API Calls: 8232\n",
            "        Total Tokens: 3115177\n",
            "        Estimated Cost: $6.2304\n",
            "        \n",
            "Checkpoint saved: 4120 batches, 41210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4120\n",
            "Processed 41210 pairs so far\n",
            "\n",
            "        API Calls: 8242\n",
            "        Total Tokens: 3119245\n",
            "        Estimated Cost: $6.2385\n",
            "        \n",
            "Checkpoint saved: 4125 batches, 41260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4125\n",
            "Processed 41260 pairs so far\n",
            "\n",
            "        API Calls: 8252\n",
            "        Total Tokens: 3122804\n",
            "        Estimated Cost: $6.2456\n",
            "        \n",
            "Checkpoint saved: 4130 batches, 41310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4130\n",
            "Processed 41310 pairs so far\n",
            "\n",
            "        API Calls: 8262\n",
            "        Total Tokens: 3126501\n",
            "        Estimated Cost: $6.2530\n",
            "        \n",
            "Checkpoint saved: 4135 batches, 41360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4135\n",
            "Processed 41360 pairs so far\n",
            "\n",
            "        API Calls: 8272\n",
            "        Total Tokens: 3130249\n",
            "        Estimated Cost: $6.2605\n",
            "        \n",
            "Checkpoint saved: 4140 batches, 41410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4140\n",
            "Processed 41410 pairs so far\n",
            "\n",
            "        API Calls: 8282\n",
            "        Total Tokens: 3133524\n",
            "        Estimated Cost: $6.2670\n",
            "        \n",
            "Checkpoint saved: 4145 batches, 41460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4145\n",
            "Processed 41460 pairs so far\n",
            "\n",
            "        API Calls: 8292\n",
            "        Total Tokens: 3137493\n",
            "        Estimated Cost: $6.2750\n",
            "        \n",
            "Checkpoint saved: 4150 batches, 41510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4150\n",
            "Processed 41510 pairs so far\n",
            "\n",
            "        API Calls: 8302\n",
            "        Total Tokens: 3141084\n",
            "        Estimated Cost: $6.2822\n",
            "        \n",
            "Checkpoint saved: 4155 batches, 41560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4155\n",
            "Processed 41560 pairs so far\n",
            "\n",
            "        API Calls: 8312\n",
            "        Total Tokens: 3145470\n",
            "        Estimated Cost: $6.2909\n",
            "        \n",
            "Checkpoint saved: 4160 batches, 41610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4160\n",
            "Processed 41610 pairs so far\n",
            "\n",
            "        API Calls: 8322\n",
            "        Total Tokens: 3149479\n",
            "        Estimated Cost: $6.2990\n",
            "        \n",
            "Checkpoint saved: 4165 batches, 41660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4165\n",
            "Processed 41660 pairs so far\n",
            "\n",
            "        API Calls: 8332\n",
            "        Total Tokens: 3153364\n",
            "        Estimated Cost: $6.3067\n",
            "        \n",
            "Checkpoint saved: 4170 batches, 41710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4170\n",
            "Processed 41710 pairs so far\n",
            "\n",
            "        API Calls: 8342\n",
            "        Total Tokens: 3156903\n",
            "        Estimated Cost: $6.3138\n",
            "        \n",
            "Checkpoint saved: 4175 batches, 41760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4175\n",
            "Processed 41760 pairs so far\n",
            "\n",
            "        API Calls: 8352\n",
            "        Total Tokens: 3160576\n",
            "        Estimated Cost: $6.3212\n",
            "        \n",
            "Checkpoint saved: 4180 batches, 41810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4180\n",
            "Processed 41810 pairs so far\n",
            "\n",
            "        API Calls: 8362\n",
            "        Total Tokens: 3164704\n",
            "        Estimated Cost: $6.3294\n",
            "        \n",
            "Checkpoint saved: 4185 batches, 41860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4185\n",
            "Processed 41860 pairs so far\n",
            "\n",
            "        API Calls: 8372\n",
            "        Total Tokens: 3168086\n",
            "        Estimated Cost: $6.3362\n",
            "        \n",
            "Checkpoint saved: 4190 batches, 41910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4190\n",
            "Processed 41910 pairs so far\n",
            "\n",
            "        API Calls: 8382\n",
            "        Total Tokens: 3171684\n",
            "        Estimated Cost: $6.3434\n",
            "        \n",
            "Checkpoint saved: 4195 batches, 41960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4195\n",
            "Processed 41960 pairs so far\n",
            "\n",
            "        API Calls: 8392\n",
            "        Total Tokens: 3175692\n",
            "        Estimated Cost: $6.3514\n",
            "        \n",
            "Checkpoint saved: 4200 batches, 42010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4200\n",
            "Processed 42010 pairs so far\n",
            "\n",
            "        API Calls: 8402\n",
            "        Total Tokens: 3179427\n",
            "        Estimated Cost: $6.3589\n",
            "        \n",
            "Checkpoint saved: 4205 batches, 42060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4205\n",
            "Processed 42060 pairs so far\n",
            "\n",
            "        API Calls: 8412\n",
            "        Total Tokens: 3183149\n",
            "        Estimated Cost: $6.3663\n",
            "        \n",
            "Checkpoint saved: 4210 batches, 42110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4210\n",
            "Processed 42110 pairs so far\n",
            "\n",
            "        API Calls: 8422\n",
            "        Total Tokens: 3186829\n",
            "        Estimated Cost: $6.3737\n",
            "        \n",
            "Checkpoint saved: 4215 batches, 42160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4215\n",
            "Processed 42160 pairs so far\n",
            "\n",
            "        API Calls: 8432\n",
            "        Total Tokens: 3190694\n",
            "        Estimated Cost: $6.3814\n",
            "        \n",
            "Checkpoint saved: 4220 batches, 42210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4220\n",
            "Processed 42210 pairs so far\n",
            "\n",
            "        API Calls: 8442\n",
            "        Total Tokens: 3194563\n",
            "        Estimated Cost: $6.3891\n",
            "        \n",
            "Checkpoint saved: 4225 batches, 42260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4225\n",
            "Processed 42260 pairs so far\n",
            "\n",
            "        API Calls: 8452\n",
            "        Total Tokens: 3198347\n",
            "        Estimated Cost: $6.3967\n",
            "        \n",
            "Checkpoint saved: 4230 batches, 42310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4230\n",
            "Processed 42310 pairs so far\n",
            "\n",
            "        API Calls: 8462\n",
            "        Total Tokens: 3201756\n",
            "        Estimated Cost: $6.4035\n",
            "        \n",
            "Checkpoint saved: 4235 batches, 42360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4235\n",
            "Processed 42360 pairs so far\n",
            "\n",
            "        API Calls: 8472\n",
            "        Total Tokens: 3205708\n",
            "        Estimated Cost: $6.4114\n",
            "        \n",
            "Checkpoint saved: 4240 batches, 42410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4240\n",
            "Processed 42410 pairs so far\n",
            "\n",
            "        API Calls: 8482\n",
            "        Total Tokens: 3209081\n",
            "        Estimated Cost: $6.4182\n",
            "        \n",
            "Checkpoint saved: 4245 batches, 42460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4245\n",
            "Processed 42460 pairs so far\n",
            "\n",
            "        API Calls: 8492\n",
            "        Total Tokens: 3212818\n",
            "        Estimated Cost: $6.4256\n",
            "        \n",
            "Checkpoint saved: 4250 batches, 42510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4250\n",
            "Processed 42510 pairs so far\n",
            "\n",
            "        API Calls: 8502\n",
            "        Total Tokens: 3216808\n",
            "        Estimated Cost: $6.4336\n",
            "        \n",
            "Checkpoint saved: 4255 batches, 42560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4255\n",
            "Processed 42560 pairs so far\n",
            "\n",
            "        API Calls: 8512\n",
            "        Total Tokens: 3220624\n",
            "        Estimated Cost: $6.4412\n",
            "        \n",
            "Checkpoint saved: 4260 batches, 42610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4260\n",
            "Processed 42610 pairs so far\n",
            "\n",
            "        API Calls: 8522\n",
            "        Total Tokens: 3224252\n",
            "        Estimated Cost: $6.4485\n",
            "        \n",
            "Checkpoint saved: 4265 batches, 42660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4265\n",
            "Processed 42660 pairs so far\n",
            "\n",
            "        API Calls: 8532\n",
            "        Total Tokens: 3228103\n",
            "        Estimated Cost: $6.4562\n",
            "        \n",
            "Checkpoint saved: 4270 batches, 42710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4270\n",
            "Processed 42710 pairs so far\n",
            "\n",
            "        API Calls: 8542\n",
            "        Total Tokens: 3231604\n",
            "        Estimated Cost: $6.4632\n",
            "        \n",
            "Checkpoint saved: 4275 batches, 42760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4275\n",
            "Processed 42760 pairs so far\n",
            "\n",
            "        API Calls: 8552\n",
            "        Total Tokens: 3235243\n",
            "        Estimated Cost: $6.4705\n",
            "        \n",
            "Checkpoint saved: 4280 batches, 42810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4280\n",
            "Processed 42810 pairs so far\n",
            "\n",
            "        API Calls: 8562\n",
            "        Total Tokens: 3239466\n",
            "        Estimated Cost: $6.4789\n",
            "        \n",
            "Checkpoint saved: 4285 batches, 42860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4285\n",
            "Processed 42860 pairs so far\n",
            "\n",
            "        API Calls: 8572\n",
            "        Total Tokens: 3243371\n",
            "        Estimated Cost: $6.4867\n",
            "        \n",
            "Checkpoint saved: 4290 batches, 42910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4290\n",
            "Processed 42910 pairs so far\n",
            "\n",
            "        API Calls: 8582\n",
            "        Total Tokens: 3246930\n",
            "        Estimated Cost: $6.4939\n",
            "        \n",
            "Checkpoint saved: 4295 batches, 42960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4295\n",
            "Processed 42960 pairs so far\n",
            "\n",
            "        API Calls: 8592\n",
            "        Total Tokens: 3250454\n",
            "        Estimated Cost: $6.5009\n",
            "        \n",
            "Checkpoint saved: 4300 batches, 43010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4300\n",
            "Processed 43010 pairs so far\n",
            "\n",
            "        API Calls: 8602\n",
            "        Total Tokens: 3254174\n",
            "        Estimated Cost: $6.5083\n",
            "        \n",
            "Checkpoint saved: 4305 batches, 43060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4305\n",
            "Processed 43060 pairs so far\n",
            "\n",
            "        API Calls: 8612\n",
            "        Total Tokens: 3258381\n",
            "        Estimated Cost: $6.5168\n",
            "        \n",
            "Checkpoint saved: 4310 batches, 43110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4310\n",
            "Processed 43110 pairs so far\n",
            "\n",
            "        API Calls: 8622\n",
            "        Total Tokens: 3262058\n",
            "        Estimated Cost: $6.5241\n",
            "        \n",
            "Checkpoint saved: 4315 batches, 43160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4315\n",
            "Processed 43160 pairs so far\n",
            "\n",
            "        API Calls: 8632\n",
            "        Total Tokens: 3266225\n",
            "        Estimated Cost: $6.5324\n",
            "        \n",
            "Checkpoint saved: 4320 batches, 43210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4320\n",
            "Processed 43210 pairs so far\n",
            "\n",
            "        API Calls: 8642\n",
            "        Total Tokens: 3269605\n",
            "        Estimated Cost: $6.5392\n",
            "        \n",
            "Checkpoint saved: 4325 batches, 43260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4325\n",
            "Processed 43260 pairs so far\n",
            "\n",
            "        API Calls: 8652\n",
            "        Total Tokens: 3273081\n",
            "        Estimated Cost: $6.5462\n",
            "        \n",
            "Checkpoint saved: 4330 batches, 43310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4330\n",
            "Processed 43310 pairs so far\n",
            "\n",
            "        API Calls: 8662\n",
            "        Total Tokens: 3276610\n",
            "        Estimated Cost: $6.5532\n",
            "        \n",
            "Checkpoint saved: 4335 batches, 43360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4335\n",
            "Processed 43360 pairs so far\n",
            "\n",
            "        API Calls: 8672\n",
            "        Total Tokens: 3280304\n",
            "        Estimated Cost: $6.5606\n",
            "        \n",
            "Checkpoint saved: 4340 batches, 43410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4340\n",
            "Processed 43410 pairs so far\n",
            "\n",
            "        API Calls: 8682\n",
            "        Total Tokens: 3283947\n",
            "        Estimated Cost: $6.5679\n",
            "        \n",
            "Checkpoint saved: 4345 batches, 43460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4345\n",
            "Processed 43460 pairs so far\n",
            "\n",
            "        API Calls: 8692\n",
            "        Total Tokens: 3288018\n",
            "        Estimated Cost: $6.5760\n",
            "        \n",
            "Checkpoint saved: 4350 batches, 43510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4350\n",
            "Processed 43510 pairs so far\n",
            "\n",
            "        API Calls: 8702\n",
            "        Total Tokens: 3292188\n",
            "        Estimated Cost: $6.5844\n",
            "        \n",
            "Checkpoint saved: 4355 batches, 43560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4355\n",
            "Processed 43560 pairs so far\n",
            "\n",
            "        API Calls: 8712\n",
            "        Total Tokens: 3296089\n",
            "        Estimated Cost: $6.5922\n",
            "        \n",
            "Checkpoint saved: 4360 batches, 43610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4360\n",
            "Processed 43610 pairs so far\n",
            "\n",
            "        API Calls: 8722\n",
            "        Total Tokens: 3299750\n",
            "        Estimated Cost: $6.5995\n",
            "        \n",
            "Checkpoint saved: 4365 batches, 43660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4365\n",
            "Processed 43660 pairs so far\n",
            "\n",
            "        API Calls: 8732\n",
            "        Total Tokens: 3303401\n",
            "        Estimated Cost: $6.6068\n",
            "        \n",
            "Checkpoint saved: 4370 batches, 43710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4370\n",
            "Processed 43710 pairs so far\n",
            "\n",
            "        API Calls: 8742\n",
            "        Total Tokens: 3307127\n",
            "        Estimated Cost: $6.6143\n",
            "        \n",
            "Checkpoint saved: 4375 batches, 43760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4375\n",
            "Processed 43760 pairs so far\n",
            "\n",
            "        API Calls: 8752\n",
            "        Total Tokens: 3310646\n",
            "        Estimated Cost: $6.6213\n",
            "        \n",
            "Checkpoint saved: 4380 batches, 43810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4380\n",
            "Processed 43810 pairs so far\n",
            "\n",
            "        API Calls: 8762\n",
            "        Total Tokens: 3314177\n",
            "        Estimated Cost: $6.6284\n",
            "        \n",
            "Checkpoint saved: 4385 batches, 43860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4385\n",
            "Processed 43860 pairs so far\n",
            "\n",
            "        API Calls: 8772\n",
            "        Total Tokens: 3317789\n",
            "        Estimated Cost: $6.6356\n",
            "        \n",
            "Checkpoint saved: 4390 batches, 43910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4390\n",
            "Processed 43910 pairs so far\n",
            "\n",
            "        API Calls: 8782\n",
            "        Total Tokens: 3321363\n",
            "        Estimated Cost: $6.6427\n",
            "        \n",
            "Checkpoint saved: 4395 batches, 43960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4395\n",
            "Processed 43960 pairs so far\n",
            "\n",
            "        API Calls: 8792\n",
            "        Total Tokens: 3325237\n",
            "        Estimated Cost: $6.6505\n",
            "        \n",
            "Checkpoint saved: 4400 batches, 44010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4400\n",
            "Processed 44010 pairs so far\n",
            "\n",
            "        API Calls: 8802\n",
            "        Total Tokens: 3328639\n",
            "        Estimated Cost: $6.6573\n",
            "        \n",
            "Checkpoint saved: 4405 batches, 44060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4405\n",
            "Processed 44060 pairs so far\n",
            "\n",
            "        API Calls: 8812\n",
            "        Total Tokens: 3332934\n",
            "        Estimated Cost: $6.6659\n",
            "        \n",
            "Checkpoint saved: 4410 batches, 44110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4410\n",
            "Processed 44110 pairs so far\n",
            "\n",
            "        API Calls: 8822\n",
            "        Total Tokens: 3336509\n",
            "        Estimated Cost: $6.6730\n",
            "        \n",
            "Checkpoint saved: 4415 batches, 44160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4415\n",
            "Processed 44160 pairs so far\n",
            "\n",
            "        API Calls: 8832\n",
            "        Total Tokens: 3340306\n",
            "        Estimated Cost: $6.6806\n",
            "        \n",
            "Checkpoint saved: 4420 batches, 44210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4420\n",
            "Processed 44210 pairs so far\n",
            "\n",
            "        API Calls: 8842\n",
            "        Total Tokens: 3343774\n",
            "        Estimated Cost: $6.6875\n",
            "        \n",
            "Checkpoint saved: 4425 batches, 44260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4425\n",
            "Processed 44260 pairs so far\n",
            "\n",
            "        API Calls: 8852\n",
            "        Total Tokens: 3348009\n",
            "        Estimated Cost: $6.6960\n",
            "        \n",
            "Checkpoint saved: 4430 batches, 44310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4430\n",
            "Processed 44310 pairs so far\n",
            "\n",
            "        API Calls: 8862\n",
            "        Total Tokens: 3351925\n",
            "        Estimated Cost: $6.7039\n",
            "        \n",
            "Checkpoint saved: 4435 batches, 44360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4435\n",
            "Processed 44360 pairs so far\n",
            "\n",
            "        API Calls: 8872\n",
            "        Total Tokens: 3355650\n",
            "        Estimated Cost: $6.7113\n",
            "        \n",
            "Checkpoint saved: 4440 batches, 44410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4440\n",
            "Processed 44410 pairs so far\n",
            "\n",
            "        API Calls: 8882\n",
            "        Total Tokens: 3359026\n",
            "        Estimated Cost: $6.7181\n",
            "        \n",
            "Checkpoint saved: 4445 batches, 44460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4445\n",
            "Processed 44460 pairs so far\n",
            "\n",
            "        API Calls: 8892\n",
            "        Total Tokens: 3362414\n",
            "        Estimated Cost: $6.7248\n",
            "        \n",
            "Checkpoint saved: 4450 batches, 44510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4450\n",
            "Processed 44510 pairs so far\n",
            "\n",
            "        API Calls: 8902\n",
            "        Total Tokens: 3366614\n",
            "        Estimated Cost: $6.7332\n",
            "        \n",
            "Checkpoint saved: 4455 batches, 44560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4455\n",
            "Processed 44560 pairs so far\n",
            "\n",
            "        API Calls: 8912\n",
            "        Total Tokens: 3370469\n",
            "        Estimated Cost: $6.7409\n",
            "        \n",
            "Checkpoint saved: 4460 batches, 44610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4460\n",
            "Processed 44610 pairs so far\n",
            "\n",
            "        API Calls: 8922\n",
            "        Total Tokens: 3374343\n",
            "        Estimated Cost: $6.7487\n",
            "        \n",
            "Checkpoint saved: 4465 batches, 44660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4465\n",
            "Processed 44660 pairs so far\n",
            "\n",
            "        API Calls: 8932\n",
            "        Total Tokens: 3377869\n",
            "        Estimated Cost: $6.7557\n",
            "        \n",
            "Checkpoint saved: 4470 batches, 44710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4470\n",
            "Processed 44710 pairs so far\n",
            "\n",
            "        API Calls: 8942\n",
            "        Total Tokens: 3381727\n",
            "        Estimated Cost: $6.7635\n",
            "        \n",
            "Checkpoint saved: 4475 batches, 44760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4475\n",
            "Processed 44760 pairs so far\n",
            "\n",
            "        API Calls: 8952\n",
            "        Total Tokens: 3385733\n",
            "        Estimated Cost: $6.7715\n",
            "        \n",
            "Checkpoint saved: 4480 batches, 44810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4480\n",
            "Processed 44810 pairs so far\n",
            "\n",
            "        API Calls: 8962\n",
            "        Total Tokens: 3389644\n",
            "        Estimated Cost: $6.7793\n",
            "        \n",
            "Checkpoint saved: 4485 batches, 44860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4485\n",
            "Processed 44860 pairs so far\n",
            "\n",
            "        API Calls: 8972\n",
            "        Total Tokens: 3393620\n",
            "        Estimated Cost: $6.7872\n",
            "        \n",
            "Checkpoint saved: 4490 batches, 44910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4490\n",
            "Processed 44910 pairs so far\n",
            "\n",
            "        API Calls: 8982\n",
            "        Total Tokens: 3397365\n",
            "        Estimated Cost: $6.7947\n",
            "        \n",
            "Checkpoint saved: 4495 batches, 44960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4495\n",
            "Processed 44960 pairs so far\n",
            "\n",
            "        API Calls: 8992\n",
            "        Total Tokens: 3400880\n",
            "        Estimated Cost: $6.8018\n",
            "        \n",
            "Checkpoint saved: 4500 batches, 45010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4500\n",
            "Processed 45010 pairs so far\n",
            "\n",
            "        API Calls: 9002\n",
            "        Total Tokens: 3404761\n",
            "        Estimated Cost: $6.8095\n",
            "        \n",
            "Checkpoint saved: 4505 batches, 45060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4505\n",
            "Processed 45060 pairs so far\n",
            "\n",
            "        API Calls: 9012\n",
            "        Total Tokens: 3408263\n",
            "        Estimated Cost: $6.8165\n",
            "        \n",
            "Checkpoint saved: 4510 batches, 45110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4510\n",
            "Processed 45110 pairs so far\n",
            "\n",
            "        API Calls: 9022\n",
            "        Total Tokens: 3411943\n",
            "        Estimated Cost: $6.8239\n",
            "        \n",
            "Checkpoint saved: 4515 batches, 45160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4515\n",
            "Processed 45160 pairs so far\n",
            "\n",
            "        API Calls: 9032\n",
            "        Total Tokens: 3415529\n",
            "        Estimated Cost: $6.8311\n",
            "        \n",
            "Checkpoint saved: 4520 batches, 45210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4520\n",
            "Processed 45210 pairs so far\n",
            "\n",
            "        API Calls: 9042\n",
            "        Total Tokens: 3419080\n",
            "        Estimated Cost: $6.8382\n",
            "        \n",
            "Checkpoint saved: 4525 batches, 45260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4525\n",
            "Processed 45260 pairs so far\n",
            "\n",
            "        API Calls: 9052\n",
            "        Total Tokens: 3423128\n",
            "        Estimated Cost: $6.8463\n",
            "        \n",
            "Checkpoint saved: 4530 batches, 45310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4530\n",
            "Processed 45310 pairs so far\n",
            "\n",
            "        API Calls: 9062\n",
            "        Total Tokens: 3426644\n",
            "        Estimated Cost: $6.8533\n",
            "        \n",
            "Checkpoint saved: 4535 batches, 45360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4535\n",
            "Processed 45360 pairs so far\n",
            "\n",
            "        API Calls: 9072\n",
            "        Total Tokens: 3430483\n",
            "        Estimated Cost: $6.8610\n",
            "        \n",
            "Checkpoint saved: 4540 batches, 45410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4540\n",
            "Processed 45410 pairs so far\n",
            "\n",
            "        API Calls: 9082\n",
            "        Total Tokens: 3434103\n",
            "        Estimated Cost: $6.8682\n",
            "        \n",
            "Checkpoint saved: 4545 batches, 45460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4545\n",
            "Processed 45460 pairs so far\n",
            "\n",
            "        API Calls: 9092\n",
            "        Total Tokens: 3437781\n",
            "        Estimated Cost: $6.8756\n",
            "        \n",
            "Checkpoint saved: 4550 batches, 45510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4550\n",
            "Processed 45510 pairs so far\n",
            "\n",
            "        API Calls: 9102\n",
            "        Total Tokens: 3441880\n",
            "        Estimated Cost: $6.8838\n",
            "        \n",
            "Checkpoint saved: 4555 batches, 45560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4555\n",
            "Processed 45560 pairs so far\n",
            "\n",
            "        API Calls: 9112\n",
            "        Total Tokens: 3445800\n",
            "        Estimated Cost: $6.8916\n",
            "        \n",
            "Checkpoint saved: 4560 batches, 45610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4560\n",
            "Processed 45610 pairs so far\n",
            "\n",
            "        API Calls: 9122\n",
            "        Total Tokens: 3449445\n",
            "        Estimated Cost: $6.8989\n",
            "        \n",
            "Checkpoint saved: 4565 batches, 45660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4565\n",
            "Processed 45660 pairs so far\n",
            "\n",
            "        API Calls: 9132\n",
            "        Total Tokens: 3453000\n",
            "        Estimated Cost: $6.9060\n",
            "        \n",
            "Checkpoint saved: 4570 batches, 45710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4570\n",
            "Processed 45710 pairs so far\n",
            "\n",
            "        API Calls: 9142\n",
            "        Total Tokens: 3456720\n",
            "        Estimated Cost: $6.9134\n",
            "        \n",
            "Checkpoint saved: 4575 batches, 45760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4575\n",
            "Processed 45760 pairs so far\n",
            "\n",
            "        API Calls: 9152\n",
            "        Total Tokens: 3460265\n",
            "        Estimated Cost: $6.9205\n",
            "        \n",
            "Checkpoint saved: 4580 batches, 45810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4580\n",
            "Processed 45810 pairs so far\n",
            "\n",
            "        API Calls: 9162\n",
            "        Total Tokens: 3464118\n",
            "        Estimated Cost: $6.9282\n",
            "        \n",
            "Checkpoint saved: 4585 batches, 45860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4585\n",
            "Processed 45860 pairs so far\n",
            "\n",
            "        API Calls: 9172\n",
            "        Total Tokens: 3468037\n",
            "        Estimated Cost: $6.9361\n",
            "        \n",
            "Checkpoint saved: 4590 batches, 45910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4590\n",
            "Processed 45910 pairs so far\n",
            "\n",
            "        API Calls: 9182\n",
            "        Total Tokens: 3471393\n",
            "        Estimated Cost: $6.9428\n",
            "        \n",
            "Checkpoint saved: 4595 batches, 45960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4595\n",
            "Processed 45960 pairs so far\n",
            "\n",
            "        API Calls: 9192\n",
            "        Total Tokens: 3475062\n",
            "        Estimated Cost: $6.9501\n",
            "        \n",
            "Checkpoint saved: 4600 batches, 46010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4600\n",
            "Processed 46010 pairs so far\n",
            "\n",
            "        API Calls: 9202\n",
            "        Total Tokens: 3478809\n",
            "        Estimated Cost: $6.9576\n",
            "        \n",
            "Checkpoint saved: 4605 batches, 46060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4605\n",
            "Processed 46060 pairs so far\n",
            "\n",
            "        API Calls: 9212\n",
            "        Total Tokens: 3482210\n",
            "        Estimated Cost: $6.9644\n",
            "        \n",
            "Checkpoint saved: 4610 batches, 46110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4610\n",
            "Processed 46110 pairs so far\n",
            "\n",
            "        API Calls: 9222\n",
            "        Total Tokens: 3486362\n",
            "        Estimated Cost: $6.9727\n",
            "        \n",
            "Checkpoint saved: 4615 batches, 46160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4615\n",
            "Processed 46160 pairs so far\n",
            "\n",
            "        API Calls: 9232\n",
            "        Total Tokens: 3490023\n",
            "        Estimated Cost: $6.9800\n",
            "        \n",
            "Checkpoint saved: 4620 batches, 46210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4620\n",
            "Processed 46210 pairs so far\n",
            "\n",
            "        API Calls: 9242\n",
            "        Total Tokens: 3493523\n",
            "        Estimated Cost: $6.9870\n",
            "        \n",
            "Checkpoint saved: 4625 batches, 46260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4625\n",
            "Processed 46260 pairs so far\n",
            "\n",
            "        API Calls: 9252\n",
            "        Total Tokens: 3497419\n",
            "        Estimated Cost: $6.9948\n",
            "        \n",
            "Checkpoint saved: 4630 batches, 46310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4630\n",
            "Processed 46310 pairs so far\n",
            "\n",
            "        API Calls: 9262\n",
            "        Total Tokens: 3501255\n",
            "        Estimated Cost: $7.0025\n",
            "        \n",
            "Checkpoint saved: 4635 batches, 46360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4635\n",
            "Processed 46360 pairs so far\n",
            "\n",
            "        API Calls: 9272\n",
            "        Total Tokens: 3505321\n",
            "        Estimated Cost: $7.0106\n",
            "        \n",
            "Checkpoint saved: 4640 batches, 46410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4640\n",
            "Processed 46410 pairs so far\n",
            "\n",
            "        API Calls: 9282\n",
            "        Total Tokens: 3509225\n",
            "        Estimated Cost: $7.0184\n",
            "        \n",
            "Checkpoint saved: 4645 batches, 46460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4645\n",
            "Processed 46460 pairs so far\n",
            "\n",
            "        API Calls: 9292\n",
            "        Total Tokens: 3513387\n",
            "        Estimated Cost: $7.0268\n",
            "        \n",
            "Checkpoint saved: 4650 batches, 46510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4650\n",
            "Processed 46510 pairs so far\n",
            "\n",
            "        API Calls: 9302\n",
            "        Total Tokens: 3516598\n",
            "        Estimated Cost: $7.0332\n",
            "        \n",
            "Checkpoint saved: 4655 batches, 46560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4655\n",
            "Processed 46560 pairs so far\n",
            "\n",
            "        API Calls: 9312\n",
            "        Total Tokens: 3520498\n",
            "        Estimated Cost: $7.0410\n",
            "        \n",
            "Checkpoint saved: 4660 batches, 46610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4660\n",
            "Processed 46610 pairs so far\n",
            "\n",
            "        API Calls: 9322\n",
            "        Total Tokens: 3524439\n",
            "        Estimated Cost: $7.0489\n",
            "        \n",
            "Checkpoint saved: 4665 batches, 46660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4665\n",
            "Processed 46660 pairs so far\n",
            "\n",
            "        API Calls: 9332\n",
            "        Total Tokens: 3528089\n",
            "        Estimated Cost: $7.0562\n",
            "        \n",
            "Checkpoint saved: 4670 batches, 46710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4670\n",
            "Processed 46710 pairs so far\n",
            "\n",
            "        API Calls: 9342\n",
            "        Total Tokens: 3531445\n",
            "        Estimated Cost: $7.0629\n",
            "        \n",
            "Checkpoint saved: 4675 batches, 46760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4675\n",
            "Processed 46760 pairs so far\n",
            "\n",
            "        API Calls: 9352\n",
            "        Total Tokens: 3534862\n",
            "        Estimated Cost: $7.0697\n",
            "        \n",
            "Checkpoint saved: 4680 batches, 46810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4680\n",
            "Processed 46810 pairs so far\n",
            "\n",
            "        API Calls: 9362\n",
            "        Total Tokens: 3538292\n",
            "        Estimated Cost: $7.0766\n",
            "        \n",
            "Checkpoint saved: 4685 batches, 46860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4685\n",
            "Processed 46860 pairs so far\n",
            "\n",
            "        API Calls: 9372\n",
            "        Total Tokens: 3541802\n",
            "        Estimated Cost: $7.0836\n",
            "        \n",
            "Checkpoint saved: 4690 batches, 46910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4690\n",
            "Processed 46910 pairs so far\n",
            "\n",
            "        API Calls: 9382\n",
            "        Total Tokens: 3545410\n",
            "        Estimated Cost: $7.0908\n",
            "        \n",
            "Checkpoint saved: 4695 batches, 46960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4695\n",
            "Processed 46960 pairs so far\n",
            "\n",
            "        API Calls: 9392\n",
            "        Total Tokens: 3549515\n",
            "        Estimated Cost: $7.0990\n",
            "        \n",
            "Checkpoint saved: 4700 batches, 47010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4700\n",
            "Processed 47010 pairs so far\n",
            "\n",
            "        API Calls: 9402\n",
            "        Total Tokens: 3553082\n",
            "        Estimated Cost: $7.1062\n",
            "        \n",
            "Checkpoint saved: 4705 batches, 47060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4705\n",
            "Processed 47060 pairs so far\n",
            "\n",
            "        API Calls: 9412\n",
            "        Total Tokens: 3556694\n",
            "        Estimated Cost: $7.1134\n",
            "        \n",
            "Checkpoint saved: 4710 batches, 47110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4710\n",
            "Processed 47110 pairs so far\n",
            "\n",
            "        API Calls: 9422\n",
            "        Total Tokens: 3560397\n",
            "        Estimated Cost: $7.1208\n",
            "        \n",
            "Checkpoint saved: 4715 batches, 47160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4715\n",
            "Processed 47160 pairs so far\n",
            "\n",
            "        API Calls: 9432\n",
            "        Total Tokens: 3564209\n",
            "        Estimated Cost: $7.1284\n",
            "        \n",
            "Checkpoint saved: 4720 batches, 47210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4720\n",
            "Processed 47210 pairs so far\n",
            "\n",
            "        API Calls: 9442\n",
            "        Total Tokens: 3567495\n",
            "        Estimated Cost: $7.1350\n",
            "        \n",
            "Checkpoint saved: 4725 batches, 47260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4725\n",
            "Processed 47260 pairs so far\n",
            "\n",
            "        API Calls: 9452\n",
            "        Total Tokens: 3571081\n",
            "        Estimated Cost: $7.1422\n",
            "        \n",
            "Checkpoint saved: 4730 batches, 47310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4730\n",
            "Processed 47310 pairs so far\n",
            "\n",
            "        API Calls: 9462\n",
            "        Total Tokens: 3574888\n",
            "        Estimated Cost: $7.1498\n",
            "        \n",
            "Checkpoint saved: 4735 batches, 47360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4735\n",
            "Processed 47360 pairs so far\n",
            "\n",
            "        API Calls: 9472\n",
            "        Total Tokens: 3578449\n",
            "        Estimated Cost: $7.1569\n",
            "        \n",
            "Checkpoint saved: 4740 batches, 47410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4740\n",
            "Processed 47410 pairs so far\n",
            "\n",
            "        API Calls: 9482\n",
            "        Total Tokens: 3582021\n",
            "        Estimated Cost: $7.1640\n",
            "        \n",
            "Checkpoint saved: 4745 batches, 47460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4745\n",
            "Processed 47460 pairs so far\n",
            "\n",
            "        API Calls: 9492\n",
            "        Total Tokens: 3585707\n",
            "        Estimated Cost: $7.1714\n",
            "        \n",
            "Checkpoint saved: 4750 batches, 47510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4750\n",
            "Processed 47510 pairs so far\n",
            "\n",
            "        API Calls: 9502\n",
            "        Total Tokens: 3589576\n",
            "        Estimated Cost: $7.1792\n",
            "        \n",
            "Checkpoint saved: 4755 batches, 47560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4755\n",
            "Processed 47560 pairs so far\n",
            "\n",
            "        API Calls: 9512\n",
            "        Total Tokens: 3592947\n",
            "        Estimated Cost: $7.1859\n",
            "        \n",
            "Checkpoint saved: 4760 batches, 47610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4760\n",
            "Processed 47610 pairs so far\n",
            "\n",
            "        API Calls: 9522\n",
            "        Total Tokens: 3596645\n",
            "        Estimated Cost: $7.1933\n",
            "        \n",
            "Checkpoint saved: 4765 batches, 47660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4765\n",
            "Processed 47660 pairs so far\n",
            "\n",
            "        API Calls: 9532\n",
            "        Total Tokens: 3600047\n",
            "        Estimated Cost: $7.2001\n",
            "        \n",
            "Checkpoint saved: 4770 batches, 47710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4770\n",
            "Processed 47710 pairs so far\n",
            "\n",
            "        API Calls: 9542\n",
            "        Total Tokens: 3603895\n",
            "        Estimated Cost: $7.2078\n",
            "        \n",
            "Checkpoint saved: 4775 batches, 47760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4775\n",
            "Processed 47760 pairs so far\n",
            "\n",
            "        API Calls: 9552\n",
            "        Total Tokens: 3607704\n",
            "        Estimated Cost: $7.2154\n",
            "        \n",
            "Checkpoint saved: 4780 batches, 47810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4780\n",
            "Processed 47810 pairs so far\n",
            "\n",
            "        API Calls: 9562\n",
            "        Total Tokens: 3611572\n",
            "        Estimated Cost: $7.2231\n",
            "        \n",
            "Checkpoint saved: 4785 batches, 47860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4785\n",
            "Processed 47860 pairs so far\n",
            "\n",
            "        API Calls: 9572\n",
            "        Total Tokens: 3615530\n",
            "        Estimated Cost: $7.2311\n",
            "        \n",
            "Checkpoint saved: 4790 batches, 47910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4790\n",
            "Processed 47910 pairs so far\n",
            "\n",
            "        API Calls: 9582\n",
            "        Total Tokens: 3619498\n",
            "        Estimated Cost: $7.2390\n",
            "        \n",
            "Checkpoint saved: 4795 batches, 47960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4795\n",
            "Processed 47960 pairs so far\n",
            "\n",
            "        API Calls: 9592\n",
            "        Total Tokens: 3623670\n",
            "        Estimated Cost: $7.2473\n",
            "        \n",
            "Checkpoint saved: 4800 batches, 48010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4800\n",
            "Processed 48010 pairs so far\n",
            "\n",
            "        API Calls: 9602\n",
            "        Total Tokens: 3626878\n",
            "        Estimated Cost: $7.2538\n",
            "        \n",
            "Checkpoint saved: 4805 batches, 48060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4805\n",
            "Processed 48060 pairs so far\n",
            "\n",
            "        API Calls: 9612\n",
            "        Total Tokens: 3630575\n",
            "        Estimated Cost: $7.2611\n",
            "        \n",
            "Checkpoint saved: 4810 batches, 48110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4810\n",
            "Processed 48110 pairs so far\n",
            "\n",
            "        API Calls: 9622\n",
            "        Total Tokens: 3634612\n",
            "        Estimated Cost: $7.2692\n",
            "        \n",
            "Checkpoint saved: 4815 batches, 48160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4815\n",
            "Processed 48160 pairs so far\n",
            "\n",
            "        API Calls: 9632\n",
            "        Total Tokens: 3638614\n",
            "        Estimated Cost: $7.2772\n",
            "        \n",
            "Checkpoint saved: 4820 batches, 48210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4820\n",
            "Processed 48210 pairs so far\n",
            "\n",
            "        API Calls: 9642\n",
            "        Total Tokens: 3641961\n",
            "        Estimated Cost: $7.2839\n",
            "        \n",
            "Checkpoint saved: 4825 batches, 48260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4825\n",
            "Processed 48260 pairs so far\n",
            "\n",
            "        API Calls: 9652\n",
            "        Total Tokens: 3645471\n",
            "        Estimated Cost: $7.2909\n",
            "        \n",
            "Checkpoint saved: 4830 batches, 48310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4830\n",
            "Processed 48310 pairs so far\n",
            "\n",
            "        API Calls: 9662\n",
            "        Total Tokens: 3649162\n",
            "        Estimated Cost: $7.2983\n",
            "        \n",
            "Checkpoint saved: 4835 batches, 48360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4835\n",
            "Processed 48360 pairs so far\n",
            "\n",
            "        API Calls: 9672\n",
            "        Total Tokens: 3653424\n",
            "        Estimated Cost: $7.3068\n",
            "        \n",
            "Checkpoint saved: 4840 batches, 48410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4840\n",
            "Processed 48410 pairs so far\n",
            "\n",
            "        API Calls: 9682\n",
            "        Total Tokens: 3657306\n",
            "        Estimated Cost: $7.3146\n",
            "        \n",
            "Checkpoint saved: 4845 batches, 48460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4845\n",
            "Processed 48460 pairs so far\n",
            "\n",
            "        API Calls: 9692\n",
            "        Total Tokens: 3661298\n",
            "        Estimated Cost: $7.3226\n",
            "        \n",
            "Checkpoint saved: 4850 batches, 48510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4850\n",
            "Processed 48510 pairs so far\n",
            "\n",
            "        API Calls: 9702\n",
            "        Total Tokens: 3664962\n",
            "        Estimated Cost: $7.3299\n",
            "        \n",
            "Checkpoint saved: 4855 batches, 48560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4855\n",
            "Processed 48560 pairs so far\n",
            "\n",
            "        API Calls: 9712\n",
            "        Total Tokens: 3668277\n",
            "        Estimated Cost: $7.3366\n",
            "        \n",
            "Checkpoint saved: 4860 batches, 48610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4860\n",
            "Processed 48610 pairs so far\n",
            "\n",
            "        API Calls: 9722\n",
            "        Total Tokens: 3672156\n",
            "        Estimated Cost: $7.3443\n",
            "        \n",
            "Checkpoint saved: 4865 batches, 48660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4865\n",
            "Processed 48660 pairs so far\n",
            "\n",
            "        API Calls: 9732\n",
            "        Total Tokens: 3675623\n",
            "        Estimated Cost: $7.3512\n",
            "        \n",
            "Checkpoint saved: 4870 batches, 48710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4870\n",
            "Processed 48710 pairs so far\n",
            "\n",
            "        API Calls: 9742\n",
            "        Total Tokens: 3679092\n",
            "        Estimated Cost: $7.3582\n",
            "        \n",
            "Checkpoint saved: 4875 batches, 48760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4875\n",
            "Processed 48760 pairs so far\n",
            "\n",
            "        API Calls: 9752\n",
            "        Total Tokens: 3683097\n",
            "        Estimated Cost: $7.3662\n",
            "        \n",
            "Checkpoint saved: 4880 batches, 48810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4880\n",
            "Processed 48810 pairs so far\n",
            "\n",
            "        API Calls: 9762\n",
            "        Total Tokens: 3686767\n",
            "        Estimated Cost: $7.3735\n",
            "        \n",
            "Checkpoint saved: 4885 batches, 48860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4885\n",
            "Processed 48860 pairs so far\n",
            "\n",
            "        API Calls: 9772\n",
            "        Total Tokens: 3690070\n",
            "        Estimated Cost: $7.3801\n",
            "        \n",
            "Checkpoint saved: 4890 batches, 48910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4890\n",
            "Processed 48910 pairs so far\n",
            "\n",
            "        API Calls: 9782\n",
            "        Total Tokens: 3693737\n",
            "        Estimated Cost: $7.3875\n",
            "        \n",
            "Checkpoint saved: 4895 batches, 48960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4895\n",
            "Processed 48960 pairs so far\n",
            "\n",
            "        API Calls: 9792\n",
            "        Total Tokens: 3697577\n",
            "        Estimated Cost: $7.3952\n",
            "        \n",
            "Checkpoint saved: 4900 batches, 49010 pairs\n",
            "\n",
            "Checkpoint saved at batch 4900\n",
            "Processed 49010 pairs so far\n",
            "\n",
            "        API Calls: 9802\n",
            "        Total Tokens: 3701770\n",
            "        Estimated Cost: $7.4035\n",
            "        \n",
            "Checkpoint saved: 4905 batches, 49060 pairs\n",
            "\n",
            "Checkpoint saved at batch 4905\n",
            "Processed 49060 pairs so far\n",
            "\n",
            "        API Calls: 9812\n",
            "        Total Tokens: 3705479\n",
            "        Estimated Cost: $7.4110\n",
            "        \n",
            "Checkpoint saved: 4910 batches, 49110 pairs\n",
            "\n",
            "Checkpoint saved at batch 4910\n",
            "Processed 49110 pairs so far\n",
            "\n",
            "        API Calls: 9822\n",
            "        Total Tokens: 3709004\n",
            "        Estimated Cost: $7.4180\n",
            "        \n",
            "Checkpoint saved: 4915 batches, 49160 pairs\n",
            "\n",
            "Checkpoint saved at batch 4915\n",
            "Processed 49160 pairs so far\n",
            "\n",
            "        API Calls: 9832\n",
            "        Total Tokens: 3712713\n",
            "        Estimated Cost: $7.4254\n",
            "        \n",
            "Checkpoint saved: 4920 batches, 49210 pairs\n",
            "\n",
            "Checkpoint saved at batch 4920\n",
            "Processed 49210 pairs so far\n",
            "\n",
            "        API Calls: 9842\n",
            "        Total Tokens: 3716486\n",
            "        Estimated Cost: $7.4330\n",
            "        \n",
            "Checkpoint saved: 4925 batches, 49260 pairs\n",
            "\n",
            "Checkpoint saved at batch 4925\n",
            "Processed 49260 pairs so far\n",
            "\n",
            "        API Calls: 9852\n",
            "        Total Tokens: 3720086\n",
            "        Estimated Cost: $7.4402\n",
            "        \n",
            "Checkpoint saved: 4930 batches, 49310 pairs\n",
            "\n",
            "Checkpoint saved at batch 4930\n",
            "Processed 49310 pairs so far\n",
            "\n",
            "        API Calls: 9862\n",
            "        Total Tokens: 3724067\n",
            "        Estimated Cost: $7.4481\n",
            "        \n",
            "Checkpoint saved: 4935 batches, 49360 pairs\n",
            "\n",
            "Checkpoint saved at batch 4935\n",
            "Processed 49360 pairs so far\n",
            "\n",
            "        API Calls: 9872\n",
            "        Total Tokens: 3727999\n",
            "        Estimated Cost: $7.4560\n",
            "        \n",
            "Checkpoint saved: 4940 batches, 49410 pairs\n",
            "\n",
            "Checkpoint saved at batch 4940\n",
            "Processed 49410 pairs so far\n",
            "\n",
            "        API Calls: 9882\n",
            "        Total Tokens: 3731802\n",
            "        Estimated Cost: $7.4636\n",
            "        \n",
            "Checkpoint saved: 4945 batches, 49460 pairs\n",
            "\n",
            "Checkpoint saved at batch 4945\n",
            "Processed 49460 pairs so far\n",
            "\n",
            "        API Calls: 9892\n",
            "        Total Tokens: 3735252\n",
            "        Estimated Cost: $7.4705\n",
            "        \n",
            "Checkpoint saved: 4950 batches, 49510 pairs\n",
            "\n",
            "Checkpoint saved at batch 4950\n",
            "Processed 49510 pairs so far\n",
            "\n",
            "        API Calls: 9902\n",
            "        Total Tokens: 3739535\n",
            "        Estimated Cost: $7.4791\n",
            "        \n",
            "Checkpoint saved: 4955 batches, 49560 pairs\n",
            "\n",
            "Checkpoint saved at batch 4955\n",
            "Processed 49560 pairs so far\n",
            "\n",
            "        API Calls: 9912\n",
            "        Total Tokens: 3743002\n",
            "        Estimated Cost: $7.4860\n",
            "        \n",
            "Checkpoint saved: 4960 batches, 49610 pairs\n",
            "\n",
            "Checkpoint saved at batch 4960\n",
            "Processed 49610 pairs so far\n",
            "\n",
            "        API Calls: 9922\n",
            "        Total Tokens: 3746888\n",
            "        Estimated Cost: $7.4938\n",
            "        \n",
            "Checkpoint saved: 4965 batches, 49660 pairs\n",
            "\n",
            "Checkpoint saved at batch 4965\n",
            "Processed 49660 pairs so far\n",
            "\n",
            "        API Calls: 9932\n",
            "        Total Tokens: 3750516\n",
            "        Estimated Cost: $7.5010\n",
            "        \n",
            "Checkpoint saved: 4970 batches, 49710 pairs\n",
            "\n",
            "Checkpoint saved at batch 4970\n",
            "Processed 49710 pairs so far\n",
            "\n",
            "        API Calls: 9942\n",
            "        Total Tokens: 3754104\n",
            "        Estimated Cost: $7.5082\n",
            "        \n",
            "Checkpoint saved: 4975 batches, 49760 pairs\n",
            "\n",
            "Checkpoint saved at batch 4975\n",
            "Processed 49760 pairs so far\n",
            "\n",
            "        API Calls: 9952\n",
            "        Total Tokens: 3758023\n",
            "        Estimated Cost: $7.5160\n",
            "        \n",
            "Checkpoint saved: 4980 batches, 49810 pairs\n",
            "\n",
            "Checkpoint saved at batch 4980\n",
            "Processed 49810 pairs so far\n",
            "\n",
            "        API Calls: 9962\n",
            "        Total Tokens: 3761786\n",
            "        Estimated Cost: $7.5236\n",
            "        \n",
            "Checkpoint saved: 4985 batches, 49860 pairs\n",
            "\n",
            "Checkpoint saved at batch 4985\n",
            "Processed 49860 pairs so far\n",
            "\n",
            "        API Calls: 9972\n",
            "        Total Tokens: 3765845\n",
            "        Estimated Cost: $7.5317\n",
            "        \n",
            "Checkpoint saved: 4990 batches, 49910 pairs\n",
            "\n",
            "Checkpoint saved at batch 4990\n",
            "Processed 49910 pairs so far\n",
            "\n",
            "        API Calls: 9982\n",
            "        Total Tokens: 3769672\n",
            "        Estimated Cost: $7.5393\n",
            "        \n",
            "Checkpoint saved: 4995 batches, 49960 pairs\n",
            "\n",
            "Checkpoint saved at batch 4995\n",
            "Processed 49960 pairs so far\n",
            "\n",
            "        API Calls: 9992\n",
            "        Total Tokens: 3773461\n",
            "        Estimated Cost: $7.5469\n",
            "        \n",
            "Checkpoint saved: 5000 batches, 50010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5000\n",
            "Processed 50010 pairs so far\n",
            "\n",
            "        API Calls: 10002\n",
            "        Total Tokens: 3777306\n",
            "        Estimated Cost: $7.5546\n",
            "        \n",
            "Checkpoint saved: 5005 batches, 50060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5005\n",
            "Processed 50060 pairs so far\n",
            "\n",
            "        API Calls: 10012\n",
            "        Total Tokens: 3780633\n",
            "        Estimated Cost: $7.5613\n",
            "        \n",
            "Checkpoint saved: 5010 batches, 50110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5010\n",
            "Processed 50110 pairs so far\n",
            "\n",
            "        API Calls: 10022\n",
            "        Total Tokens: 3784194\n",
            "        Estimated Cost: $7.5684\n",
            "        \n",
            "Checkpoint saved: 5015 batches, 50160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5015\n",
            "Processed 50160 pairs so far\n",
            "\n",
            "        API Calls: 10032\n",
            "        Total Tokens: 3787739\n",
            "        Estimated Cost: $7.5755\n",
            "        \n",
            "Checkpoint saved: 5020 batches, 50210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5020\n",
            "Processed 50210 pairs so far\n",
            "\n",
            "        API Calls: 10042\n",
            "        Total Tokens: 3791398\n",
            "        Estimated Cost: $7.5828\n",
            "        \n",
            "Checkpoint saved: 5025 batches, 50260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5025\n",
            "Processed 50260 pairs so far\n",
            "\n",
            "        API Calls: 10052\n",
            "        Total Tokens: 3795376\n",
            "        Estimated Cost: $7.5908\n",
            "        \n",
            "Checkpoint saved: 5030 batches, 50310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5030\n",
            "Processed 50310 pairs so far\n",
            "\n",
            "        API Calls: 10062\n",
            "        Total Tokens: 3799529\n",
            "        Estimated Cost: $7.5991\n",
            "        \n",
            "Checkpoint saved: 5035 batches, 50360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5035\n",
            "Processed 50360 pairs so far\n",
            "\n",
            "        API Calls: 10072\n",
            "        Total Tokens: 3803503\n",
            "        Estimated Cost: $7.6070\n",
            "        \n",
            "Checkpoint saved: 5040 batches, 50410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5040\n",
            "Processed 50410 pairs so far\n",
            "\n",
            "        API Calls: 10082\n",
            "        Total Tokens: 3807721\n",
            "        Estimated Cost: $7.6154\n",
            "        \n",
            "Checkpoint saved: 5045 batches, 50460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5045\n",
            "Processed 50460 pairs so far\n",
            "\n",
            "        API Calls: 10092\n",
            "        Total Tokens: 3811593\n",
            "        Estimated Cost: $7.6232\n",
            "        \n",
            "Checkpoint saved: 5050 batches, 50510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5050\n",
            "Processed 50510 pairs so far\n",
            "\n",
            "        API Calls: 10102\n",
            "        Total Tokens: 3815499\n",
            "        Estimated Cost: $7.6310\n",
            "        \n",
            "Checkpoint saved: 5055 batches, 50560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5055\n",
            "Processed 50560 pairs so far\n",
            "\n",
            "        API Calls: 10112\n",
            "        Total Tokens: 3819245\n",
            "        Estimated Cost: $7.6385\n",
            "        \n",
            "Checkpoint saved: 5060 batches, 50610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5060\n",
            "Processed 50610 pairs so far\n",
            "\n",
            "        API Calls: 10122\n",
            "        Total Tokens: 3822764\n",
            "        Estimated Cost: $7.6455\n",
            "        \n",
            "Checkpoint saved: 5065 batches, 50660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5065\n",
            "Processed 50660 pairs so far\n",
            "\n",
            "        API Calls: 10132\n",
            "        Total Tokens: 3826400\n",
            "        Estimated Cost: $7.6528\n",
            "        \n",
            "Checkpoint saved: 5070 batches, 50710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5070\n",
            "Processed 50710 pairs so far\n",
            "\n",
            "        API Calls: 10142\n",
            "        Total Tokens: 3830457\n",
            "        Estimated Cost: $7.6609\n",
            "        \n",
            "Checkpoint saved: 5075 batches, 50760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5075\n",
            "Processed 50760 pairs so far\n",
            "\n",
            "        API Calls: 10152\n",
            "        Total Tokens: 3834565\n",
            "        Estimated Cost: $7.6691\n",
            "        \n",
            "Checkpoint saved: 5080 batches, 50810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5080\n",
            "Processed 50810 pairs so far\n",
            "\n",
            "        API Calls: 10162\n",
            "        Total Tokens: 3838931\n",
            "        Estimated Cost: $7.6779\n",
            "        \n",
            "Checkpoint saved: 5085 batches, 50860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5085\n",
            "Processed 50860 pairs so far\n",
            "\n",
            "        API Calls: 10172\n",
            "        Total Tokens: 3842765\n",
            "        Estimated Cost: $7.6855\n",
            "        \n",
            "Checkpoint saved: 5090 batches, 50910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5090\n",
            "Processed 50910 pairs so far\n",
            "\n",
            "        API Calls: 10182\n",
            "        Total Tokens: 3846586\n",
            "        Estimated Cost: $7.6932\n",
            "        \n",
            "Checkpoint saved: 5095 batches, 50960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5095\n",
            "Processed 50960 pairs so far\n",
            "\n",
            "        API Calls: 10192\n",
            "        Total Tokens: 3850531\n",
            "        Estimated Cost: $7.7011\n",
            "        \n",
            "Checkpoint saved: 5100 batches, 51010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5100\n",
            "Processed 51010 pairs so far\n",
            "\n",
            "        API Calls: 10202\n",
            "        Total Tokens: 3854608\n",
            "        Estimated Cost: $7.7092\n",
            "        \n",
            "Checkpoint saved: 5105 batches, 51060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5105\n",
            "Processed 51060 pairs so far\n",
            "\n",
            "        API Calls: 10212\n",
            "        Total Tokens: 3858364\n",
            "        Estimated Cost: $7.7167\n",
            "        \n",
            "Checkpoint saved: 5110 batches, 51110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5110\n",
            "Processed 51110 pairs so far\n",
            "\n",
            "        API Calls: 10222\n",
            "        Total Tokens: 3861734\n",
            "        Estimated Cost: $7.7235\n",
            "        \n",
            "Checkpoint saved: 5115 batches, 51160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5115\n",
            "Processed 51160 pairs so far\n",
            "\n",
            "        API Calls: 10232\n",
            "        Total Tokens: 3866042\n",
            "        Estimated Cost: $7.7321\n",
            "        \n",
            "Checkpoint saved: 5120 batches, 51210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5120\n",
            "Processed 51210 pairs so far\n",
            "\n",
            "        API Calls: 10242\n",
            "        Total Tokens: 3869801\n",
            "        Estimated Cost: $7.7396\n",
            "        \n",
            "Checkpoint saved: 5125 batches, 51260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5125\n",
            "Processed 51260 pairs so far\n",
            "\n",
            "        API Calls: 10252\n",
            "        Total Tokens: 3873741\n",
            "        Estimated Cost: $7.7475\n",
            "        \n",
            "Checkpoint saved: 5130 batches, 51310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5130\n",
            "Processed 51310 pairs so far\n",
            "\n",
            "        API Calls: 10262\n",
            "        Total Tokens: 3877353\n",
            "        Estimated Cost: $7.7547\n",
            "        \n",
            "Checkpoint saved: 5135 batches, 51360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5135\n",
            "Processed 51360 pairs so far\n",
            "\n",
            "        API Calls: 10272\n",
            "        Total Tokens: 3881369\n",
            "        Estimated Cost: $7.7627\n",
            "        \n",
            "Checkpoint saved: 5140 batches, 51410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5140\n",
            "Processed 51410 pairs so far\n",
            "\n",
            "        API Calls: 10282\n",
            "        Total Tokens: 3885176\n",
            "        Estimated Cost: $7.7704\n",
            "        \n",
            "Checkpoint saved: 5145 batches, 51460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5145\n",
            "Processed 51460 pairs so far\n",
            "\n",
            "        API Calls: 10292\n",
            "        Total Tokens: 3889643\n",
            "        Estimated Cost: $7.7793\n",
            "        \n",
            "Checkpoint saved: 5150 batches, 51510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5150\n",
            "Processed 51510 pairs so far\n",
            "\n",
            "        API Calls: 10302\n",
            "        Total Tokens: 3893551\n",
            "        Estimated Cost: $7.7871\n",
            "        \n",
            "Checkpoint saved: 5155 batches, 51560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5155\n",
            "Processed 51560 pairs so far\n",
            "\n",
            "        API Calls: 10312\n",
            "        Total Tokens: 3897345\n",
            "        Estimated Cost: $7.7947\n",
            "        \n",
            "Checkpoint saved: 5160 batches, 51610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5160\n",
            "Processed 51610 pairs so far\n",
            "\n",
            "        API Calls: 10322\n",
            "        Total Tokens: 3900978\n",
            "        Estimated Cost: $7.8020\n",
            "        \n",
            "Checkpoint saved: 5165 batches, 51660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5165\n",
            "Processed 51660 pairs so far\n",
            "\n",
            "        API Calls: 10332\n",
            "        Total Tokens: 3904719\n",
            "        Estimated Cost: $7.8094\n",
            "        \n",
            "Checkpoint saved: 5170 batches, 51710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5170\n",
            "Processed 51710 pairs so far\n",
            "\n",
            "        API Calls: 10342\n",
            "        Total Tokens: 3908409\n",
            "        Estimated Cost: $7.8168\n",
            "        \n",
            "Checkpoint saved: 5175 batches, 51760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5175\n",
            "Processed 51760 pairs so far\n",
            "\n",
            "        API Calls: 10352\n",
            "        Total Tokens: 3911980\n",
            "        Estimated Cost: $7.8240\n",
            "        \n",
            "Checkpoint saved: 5180 batches, 51810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5180\n",
            "Processed 51810 pairs so far\n",
            "\n",
            "        API Calls: 10362\n",
            "        Total Tokens: 3915783\n",
            "        Estimated Cost: $7.8316\n",
            "        \n",
            "Checkpoint saved: 5185 batches, 51860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5185\n",
            "Processed 51860 pairs so far\n",
            "\n",
            "        API Calls: 10372\n",
            "        Total Tokens: 3919459\n",
            "        Estimated Cost: $7.8389\n",
            "        \n",
            "Checkpoint saved: 5190 batches, 51910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5190\n",
            "Processed 51910 pairs so far\n",
            "\n",
            "        API Calls: 10382\n",
            "        Total Tokens: 3922877\n",
            "        Estimated Cost: $7.8458\n",
            "        \n",
            "Checkpoint saved: 5195 batches, 51960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5195\n",
            "Processed 51960 pairs so far\n",
            "\n",
            "        API Calls: 10392\n",
            "        Total Tokens: 3926900\n",
            "        Estimated Cost: $7.8538\n",
            "        \n",
            "Checkpoint saved: 5200 batches, 52010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5200\n",
            "Processed 52010 pairs so far\n",
            "\n",
            "        API Calls: 10402\n",
            "        Total Tokens: 3930736\n",
            "        Estimated Cost: $7.8615\n",
            "        \n",
            "Checkpoint saved: 5205 batches, 52060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5205\n",
            "Processed 52060 pairs so far\n",
            "\n",
            "        API Calls: 10412\n",
            "        Total Tokens: 3934720\n",
            "        Estimated Cost: $7.8694\n",
            "        \n",
            "Checkpoint saved: 5210 batches, 52110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5210\n",
            "Processed 52110 pairs so far\n",
            "\n",
            "        API Calls: 10422\n",
            "        Total Tokens: 3938321\n",
            "        Estimated Cost: $7.8766\n",
            "        \n",
            "Checkpoint saved: 5215 batches, 52160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5215\n",
            "Processed 52160 pairs so far\n",
            "\n",
            "        API Calls: 10432\n",
            "        Total Tokens: 3941949\n",
            "        Estimated Cost: $7.8839\n",
            "        \n",
            "Checkpoint saved: 5220 batches, 52210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5220\n",
            "Processed 52210 pairs so far\n",
            "\n",
            "        API Calls: 10442\n",
            "        Total Tokens: 3945744\n",
            "        Estimated Cost: $7.8915\n",
            "        \n",
            "Checkpoint saved: 5225 batches, 52260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5225\n",
            "Processed 52260 pairs so far\n",
            "\n",
            "        API Calls: 10452\n",
            "        Total Tokens: 3949656\n",
            "        Estimated Cost: $7.8993\n",
            "        \n",
            "Checkpoint saved: 5230 batches, 52310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5230\n",
            "Processed 52310 pairs so far\n",
            "\n",
            "        API Calls: 10462\n",
            "        Total Tokens: 3953445\n",
            "        Estimated Cost: $7.9069\n",
            "        \n",
            "Checkpoint saved: 5235 batches, 52360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5235\n",
            "Processed 52360 pairs so far\n",
            "\n",
            "        API Calls: 10472\n",
            "        Total Tokens: 3957244\n",
            "        Estimated Cost: $7.9145\n",
            "        \n",
            "Checkpoint saved: 5240 batches, 52410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5240\n",
            "Processed 52410 pairs so far\n",
            "\n",
            "        API Calls: 10482\n",
            "        Total Tokens: 3961549\n",
            "        Estimated Cost: $7.9231\n",
            "        \n",
            "Checkpoint saved: 5245 batches, 52460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5245\n",
            "Processed 52460 pairs so far\n",
            "\n",
            "        API Calls: 10492\n",
            "        Total Tokens: 3964992\n",
            "        Estimated Cost: $7.9300\n",
            "        \n",
            "Checkpoint saved: 5250 batches, 52510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5250\n",
            "Processed 52510 pairs so far\n",
            "\n",
            "        API Calls: 10502\n",
            "        Total Tokens: 3968232\n",
            "        Estimated Cost: $7.9365\n",
            "        \n",
            "Checkpoint saved: 5255 batches, 52560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5255\n",
            "Processed 52560 pairs so far\n",
            "\n",
            "        API Calls: 10512\n",
            "        Total Tokens: 3971893\n",
            "        Estimated Cost: $7.9438\n",
            "        \n",
            "Checkpoint saved: 5260 batches, 52610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5260\n",
            "Processed 52610 pairs so far\n",
            "\n",
            "        API Calls: 10522\n",
            "        Total Tokens: 3975772\n",
            "        Estimated Cost: $7.9515\n",
            "        \n",
            "Checkpoint saved: 5265 batches, 52660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5265\n",
            "Processed 52660 pairs so far\n",
            "\n",
            "        API Calls: 10532\n",
            "        Total Tokens: 3978991\n",
            "        Estimated Cost: $7.9580\n",
            "        \n",
            "Checkpoint saved: 5270 batches, 52710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5270\n",
            "Processed 52710 pairs so far\n",
            "\n",
            "        API Calls: 10542\n",
            "        Total Tokens: 3983089\n",
            "        Estimated Cost: $7.9662\n",
            "        \n",
            "Checkpoint saved: 5275 batches, 52760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5275\n",
            "Processed 52760 pairs so far\n",
            "\n",
            "        API Calls: 10552\n",
            "        Total Tokens: 3986667\n",
            "        Estimated Cost: $7.9733\n",
            "        \n",
            "Checkpoint saved: 5280 batches, 52810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5280\n",
            "Processed 52810 pairs so far\n",
            "\n",
            "        API Calls: 10562\n",
            "        Total Tokens: 3990355\n",
            "        Estimated Cost: $7.9807\n",
            "        \n",
            "Checkpoint saved: 5285 batches, 52860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5285\n",
            "Processed 52860 pairs so far\n",
            "\n",
            "        API Calls: 10572\n",
            "        Total Tokens: 3993919\n",
            "        Estimated Cost: $7.9878\n",
            "        \n",
            "Checkpoint saved: 5290 batches, 52910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5290\n",
            "Processed 52910 pairs so far\n",
            "\n",
            "        API Calls: 10582\n",
            "        Total Tokens: 3997604\n",
            "        Estimated Cost: $7.9952\n",
            "        \n",
            "Checkpoint saved: 5295 batches, 52960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5295\n",
            "Processed 52960 pairs so far\n",
            "\n",
            "        API Calls: 10592\n",
            "        Total Tokens: 4001628\n",
            "        Estimated Cost: $8.0033\n",
            "        \n",
            "Checkpoint saved: 5300 batches, 53010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5300\n",
            "Processed 53010 pairs so far\n",
            "\n",
            "        API Calls: 10602\n",
            "        Total Tokens: 4005406\n",
            "        Estimated Cost: $8.0108\n",
            "        \n",
            "Checkpoint saved: 5305 batches, 53060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5305\n",
            "Processed 53060 pairs so far\n",
            "\n",
            "        API Calls: 10612\n",
            "        Total Tokens: 4009402\n",
            "        Estimated Cost: $8.0188\n",
            "        \n",
            "Checkpoint saved: 5310 batches, 53110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5310\n",
            "Processed 53110 pairs so far\n",
            "\n",
            "        API Calls: 10622\n",
            "        Total Tokens: 4013332\n",
            "        Estimated Cost: $8.0267\n",
            "        \n",
            "Checkpoint saved: 5315 batches, 53160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5315\n",
            "Processed 53160 pairs so far\n",
            "\n",
            "        API Calls: 10632\n",
            "        Total Tokens: 4017231\n",
            "        Estimated Cost: $8.0345\n",
            "        \n",
            "Checkpoint saved: 5320 batches, 53210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5320\n",
            "Processed 53210 pairs so far\n",
            "\n",
            "        API Calls: 10642\n",
            "        Total Tokens: 4021387\n",
            "        Estimated Cost: $8.0428\n",
            "        \n",
            "Checkpoint saved: 5325 batches, 53260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5325\n",
            "Processed 53260 pairs so far\n",
            "\n",
            "        API Calls: 10652\n",
            "        Total Tokens: 4024951\n",
            "        Estimated Cost: $8.0499\n",
            "        \n",
            "Checkpoint saved: 5330 batches, 53310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5330\n",
            "Processed 53310 pairs so far\n",
            "\n",
            "        API Calls: 10662\n",
            "        Total Tokens: 4028819\n",
            "        Estimated Cost: $8.0576\n",
            "        \n",
            "Checkpoint saved: 5335 batches, 53360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5335\n",
            "Processed 53360 pairs so far\n",
            "\n",
            "        API Calls: 10672\n",
            "        Total Tokens: 4032633\n",
            "        Estimated Cost: $8.0653\n",
            "        \n",
            "Checkpoint saved: 5340 batches, 53410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5340\n",
            "Processed 53410 pairs so far\n",
            "\n",
            "        API Calls: 10682\n",
            "        Total Tokens: 4036469\n",
            "        Estimated Cost: $8.0729\n",
            "        \n",
            "Checkpoint saved: 5345 batches, 53460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5345\n",
            "Processed 53460 pairs so far\n",
            "\n",
            "        API Calls: 10692\n",
            "        Total Tokens: 4040450\n",
            "        Estimated Cost: $8.0809\n",
            "        \n",
            "Checkpoint saved: 5350 batches, 53510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5350\n",
            "Processed 53510 pairs so far\n",
            "\n",
            "        API Calls: 10702\n",
            "        Total Tokens: 4044726\n",
            "        Estimated Cost: $8.0895\n",
            "        \n",
            "Checkpoint saved: 5355 batches, 53560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5355\n",
            "Processed 53560 pairs so far\n",
            "\n",
            "        API Calls: 10712\n",
            "        Total Tokens: 4048427\n",
            "        Estimated Cost: $8.0969\n",
            "        \n",
            "Checkpoint saved: 5360 batches, 53610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5360\n",
            "Processed 53610 pairs so far\n",
            "\n",
            "        API Calls: 10722\n",
            "        Total Tokens: 4052002\n",
            "        Estimated Cost: $8.1040\n",
            "        \n",
            "Checkpoint saved: 5365 batches, 53660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5365\n",
            "Processed 53660 pairs so far\n",
            "\n",
            "        API Calls: 10732\n",
            "        Total Tokens: 4055774\n",
            "        Estimated Cost: $8.1115\n",
            "        \n",
            "Checkpoint saved: 5370 batches, 53710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5370\n",
            "Processed 53710 pairs so far\n",
            "\n",
            "        API Calls: 10742\n",
            "        Total Tokens: 4059694\n",
            "        Estimated Cost: $8.1194\n",
            "        \n",
            "Checkpoint saved: 5375 batches, 53760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5375\n",
            "Processed 53760 pairs so far\n",
            "\n",
            "        API Calls: 10752\n",
            "        Total Tokens: 4063372\n",
            "        Estimated Cost: $8.1267\n",
            "        \n",
            "Checkpoint saved: 5380 batches, 53810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5380\n",
            "Processed 53810 pairs so far\n",
            "\n",
            "        API Calls: 10762\n",
            "        Total Tokens: 4067519\n",
            "        Estimated Cost: $8.1350\n",
            "        \n",
            "Checkpoint saved: 5385 batches, 53860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5385\n",
            "Processed 53860 pairs so far\n",
            "\n",
            "        API Calls: 10772\n",
            "        Total Tokens: 4070747\n",
            "        Estimated Cost: $8.1415\n",
            "        \n",
            "Checkpoint saved: 5390 batches, 53910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5390\n",
            "Processed 53910 pairs so far\n",
            "\n",
            "        API Calls: 10782\n",
            "        Total Tokens: 4074365\n",
            "        Estimated Cost: $8.1487\n",
            "        \n",
            "Checkpoint saved: 5395 batches, 53960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5395\n",
            "Processed 53960 pairs so far\n",
            "\n",
            "        API Calls: 10792\n",
            "        Total Tokens: 4077969\n",
            "        Estimated Cost: $8.1559\n",
            "        \n",
            "Checkpoint saved: 5400 batches, 54010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5400\n",
            "Processed 54010 pairs so far\n",
            "\n",
            "        API Calls: 10802\n",
            "        Total Tokens: 4081487\n",
            "        Estimated Cost: $8.1630\n",
            "        \n",
            "Checkpoint saved: 5405 batches, 54060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5405\n",
            "Processed 54060 pairs so far\n",
            "\n",
            "        API Calls: 10812\n",
            "        Total Tokens: 4085434\n",
            "        Estimated Cost: $8.1709\n",
            "        \n",
            "Checkpoint saved: 5410 batches, 54110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5410\n",
            "Processed 54110 pairs so far\n",
            "\n",
            "        API Calls: 10822\n",
            "        Total Tokens: 4088952\n",
            "        Estimated Cost: $8.1779\n",
            "        \n",
            "Checkpoint saved: 5415 batches, 54160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5415\n",
            "Processed 54160 pairs so far\n",
            "\n",
            "        API Calls: 10832\n",
            "        Total Tokens: 4092537\n",
            "        Estimated Cost: $8.1851\n",
            "        \n",
            "Checkpoint saved: 5420 batches, 54210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5420\n",
            "Processed 54210 pairs so far\n",
            "\n",
            "        API Calls: 10842\n",
            "        Total Tokens: 4096400\n",
            "        Estimated Cost: $8.1928\n",
            "        \n",
            "Checkpoint saved: 5425 batches, 54260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5425\n",
            "Processed 54260 pairs so far\n",
            "\n",
            "        API Calls: 10852\n",
            "        Total Tokens: 4100343\n",
            "        Estimated Cost: $8.2007\n",
            "        \n",
            "Checkpoint saved: 5430 batches, 54310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5430\n",
            "Processed 54310 pairs so far\n",
            "\n",
            "        API Calls: 10862\n",
            "        Total Tokens: 4104356\n",
            "        Estimated Cost: $8.2087\n",
            "        \n",
            "Checkpoint saved: 5435 batches, 54360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5435\n",
            "Processed 54360 pairs so far\n",
            "\n",
            "        API Calls: 10872\n",
            "        Total Tokens: 4108429\n",
            "        Estimated Cost: $8.2169\n",
            "        \n",
            "Checkpoint saved: 5440 batches, 54410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5440\n",
            "Processed 54410 pairs so far\n",
            "\n",
            "        API Calls: 10882\n",
            "        Total Tokens: 4112004\n",
            "        Estimated Cost: $8.2240\n",
            "        \n",
            "Checkpoint saved: 5445 batches, 54460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5445\n",
            "Processed 54460 pairs so far\n",
            "\n",
            "        API Calls: 10892\n",
            "        Total Tokens: 4116128\n",
            "        Estimated Cost: $8.2323\n",
            "        \n",
            "Checkpoint saved: 5450 batches, 54510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5450\n",
            "Processed 54510 pairs so far\n",
            "\n",
            "        API Calls: 10902\n",
            "        Total Tokens: 4120173\n",
            "        Estimated Cost: $8.2403\n",
            "        \n",
            "Checkpoint saved: 5455 batches, 54560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5455\n",
            "Processed 54560 pairs so far\n",
            "\n",
            "        API Calls: 10912\n",
            "        Total Tokens: 4124309\n",
            "        Estimated Cost: $8.2486\n",
            "        \n",
            "Checkpoint saved: 5460 batches, 54610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5460\n",
            "Processed 54610 pairs so far\n",
            "\n",
            "        API Calls: 10922\n",
            "        Total Tokens: 4128667\n",
            "        Estimated Cost: $8.2573\n",
            "        \n",
            "Checkpoint saved: 5465 batches, 54660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5465\n",
            "Processed 54660 pairs so far\n",
            "\n",
            "        API Calls: 10932\n",
            "        Total Tokens: 4132465\n",
            "        Estimated Cost: $8.2649\n",
            "        \n",
            "Checkpoint saved: 5470 batches, 54710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5470\n",
            "Processed 54710 pairs so far\n",
            "\n",
            "        API Calls: 10942\n",
            "        Total Tokens: 4136433\n",
            "        Estimated Cost: $8.2729\n",
            "        \n",
            "Checkpoint saved: 5475 batches, 54760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5475\n",
            "Processed 54760 pairs so far\n",
            "\n",
            "        API Calls: 10952\n",
            "        Total Tokens: 4140249\n",
            "        Estimated Cost: $8.2805\n",
            "        \n",
            "Checkpoint saved: 5480 batches, 54810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5480\n",
            "Processed 54810 pairs so far\n",
            "\n",
            "        API Calls: 10962\n",
            "        Total Tokens: 4143894\n",
            "        Estimated Cost: $8.2878\n",
            "        \n",
            "Checkpoint saved: 5485 batches, 54860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5485\n",
            "Processed 54860 pairs so far\n",
            "\n",
            "        API Calls: 10972\n",
            "        Total Tokens: 4148000\n",
            "        Estimated Cost: $8.2960\n",
            "        \n",
            "Checkpoint saved: 5490 batches, 54910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5490\n",
            "Processed 54910 pairs so far\n",
            "\n",
            "        API Calls: 10982\n",
            "        Total Tokens: 4151904\n",
            "        Estimated Cost: $8.3038\n",
            "        \n",
            "Checkpoint saved: 5495 batches, 54960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5495\n",
            "Processed 54960 pairs so far\n",
            "\n",
            "        API Calls: 10992\n",
            "        Total Tokens: 4155607\n",
            "        Estimated Cost: $8.3112\n",
            "        \n",
            "Checkpoint saved: 5500 batches, 55010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5500\n",
            "Processed 55010 pairs so far\n",
            "\n",
            "        API Calls: 11002\n",
            "        Total Tokens: 4159622\n",
            "        Estimated Cost: $8.3192\n",
            "        \n",
            "Checkpoint saved: 5505 batches, 55060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5505\n",
            "Processed 55060 pairs so far\n",
            "\n",
            "        API Calls: 11012\n",
            "        Total Tokens: 4163212\n",
            "        Estimated Cost: $8.3264\n",
            "        \n",
            "Checkpoint saved: 5510 batches, 55110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5510\n",
            "Processed 55110 pairs so far\n",
            "\n",
            "        API Calls: 11022\n",
            "        Total Tokens: 4166662\n",
            "        Estimated Cost: $8.3333\n",
            "        \n",
            "Checkpoint saved: 5515 batches, 55160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5515\n",
            "Processed 55160 pairs so far\n",
            "\n",
            "        API Calls: 11032\n",
            "        Total Tokens: 4170273\n",
            "        Estimated Cost: $8.3405\n",
            "        \n",
            "Checkpoint saved: 5520 batches, 55210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5520\n",
            "Processed 55210 pairs so far\n",
            "\n",
            "        API Calls: 11042\n",
            "        Total Tokens: 4173992\n",
            "        Estimated Cost: $8.3480\n",
            "        \n",
            "Checkpoint saved: 5525 batches, 55260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5525\n",
            "Processed 55260 pairs so far\n",
            "\n",
            "        API Calls: 11052\n",
            "        Total Tokens: 4177957\n",
            "        Estimated Cost: $8.3559\n",
            "        \n",
            "Checkpoint saved: 5530 batches, 55310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5530\n",
            "Processed 55310 pairs so far\n",
            "\n",
            "        API Calls: 11062\n",
            "        Total Tokens: 4182261\n",
            "        Estimated Cost: $8.3645\n",
            "        \n",
            "Checkpoint saved: 5535 batches, 55360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5535\n",
            "Processed 55360 pairs so far\n",
            "\n",
            "        API Calls: 11072\n",
            "        Total Tokens: 4185943\n",
            "        Estimated Cost: $8.3719\n",
            "        \n",
            "Checkpoint saved: 5540 batches, 55410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5540\n",
            "Processed 55410 pairs so far\n",
            "\n",
            "        API Calls: 11082\n",
            "        Total Tokens: 4190064\n",
            "        Estimated Cost: $8.3801\n",
            "        \n",
            "Checkpoint saved: 5545 batches, 55460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5545\n",
            "Processed 55460 pairs so far\n",
            "\n",
            "        API Calls: 11092\n",
            "        Total Tokens: 4193672\n",
            "        Estimated Cost: $8.3873\n",
            "        \n",
            "Checkpoint saved: 5550 batches, 55510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5550\n",
            "Processed 55510 pairs so far\n",
            "\n",
            "        API Calls: 11102\n",
            "        Total Tokens: 4197285\n",
            "        Estimated Cost: $8.3946\n",
            "        \n",
            "Checkpoint saved: 5555 batches, 55560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5555\n",
            "Processed 55560 pairs so far\n",
            "\n",
            "        API Calls: 11112\n",
            "        Total Tokens: 4200839\n",
            "        Estimated Cost: $8.4017\n",
            "        \n",
            "Checkpoint saved: 5560 batches, 55610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5560\n",
            "Processed 55610 pairs so far\n",
            "\n",
            "        API Calls: 11122\n",
            "        Total Tokens: 4204527\n",
            "        Estimated Cost: $8.4091\n",
            "        \n",
            "Checkpoint saved: 5565 batches, 55660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5565\n",
            "Processed 55660 pairs so far\n",
            "\n",
            "        API Calls: 11132\n",
            "        Total Tokens: 4208239\n",
            "        Estimated Cost: $8.4165\n",
            "        \n",
            "Checkpoint saved: 5570 batches, 55710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5570\n",
            "Processed 55710 pairs so far\n",
            "\n",
            "        API Calls: 11142\n",
            "        Total Tokens: 4211953\n",
            "        Estimated Cost: $8.4239\n",
            "        \n",
            "Checkpoint saved: 5575 batches, 55760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5575\n",
            "Processed 55760 pairs so far\n",
            "\n",
            "        API Calls: 11152\n",
            "        Total Tokens: 4216063\n",
            "        Estimated Cost: $8.4321\n",
            "        \n",
            "Checkpoint saved: 5580 batches, 55810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5580\n",
            "Processed 55810 pairs so far\n",
            "\n",
            "        API Calls: 11162\n",
            "        Total Tokens: 4219653\n",
            "        Estimated Cost: $8.4393\n",
            "        \n",
            "Checkpoint saved: 5585 batches, 55860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5585\n",
            "Processed 55860 pairs so far\n",
            "\n",
            "        API Calls: 11172\n",
            "        Total Tokens: 4223588\n",
            "        Estimated Cost: $8.4472\n",
            "        \n",
            "Checkpoint saved: 5590 batches, 55910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5590\n",
            "Processed 55910 pairs so far\n",
            "\n",
            "        API Calls: 11182\n",
            "        Total Tokens: 4227123\n",
            "        Estimated Cost: $8.4542\n",
            "        \n",
            "Checkpoint saved: 5595 batches, 55960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5595\n",
            "Processed 55960 pairs so far\n",
            "\n",
            "        API Calls: 11192\n",
            "        Total Tokens: 4230997\n",
            "        Estimated Cost: $8.4620\n",
            "        \n",
            "Checkpoint saved: 5600 batches, 56010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5600\n",
            "Processed 56010 pairs so far\n",
            "\n",
            "        API Calls: 11202\n",
            "        Total Tokens: 4234543\n",
            "        Estimated Cost: $8.4691\n",
            "        \n",
            "Checkpoint saved: 5605 batches, 56060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5605\n",
            "Processed 56060 pairs so far\n",
            "\n",
            "        API Calls: 11212\n",
            "        Total Tokens: 4238427\n",
            "        Estimated Cost: $8.4769\n",
            "        \n",
            "Checkpoint saved: 5610 batches, 56110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5610\n",
            "Processed 56110 pairs so far\n",
            "\n",
            "        API Calls: 11222\n",
            "        Total Tokens: 4241766\n",
            "        Estimated Cost: $8.4835\n",
            "        \n",
            "Checkpoint saved: 5615 batches, 56160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5615\n",
            "Processed 56160 pairs so far\n",
            "\n",
            "        API Calls: 11232\n",
            "        Total Tokens: 4245761\n",
            "        Estimated Cost: $8.4915\n",
            "        \n",
            "Checkpoint saved: 5620 batches, 56210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5620\n",
            "Processed 56210 pairs so far\n",
            "\n",
            "        API Calls: 11242\n",
            "        Total Tokens: 4249480\n",
            "        Estimated Cost: $8.4990\n",
            "        \n",
            "Checkpoint saved: 5625 batches, 56260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5625\n",
            "Processed 56260 pairs so far\n",
            "\n",
            "        API Calls: 11252\n",
            "        Total Tokens: 4253348\n",
            "        Estimated Cost: $8.5067\n",
            "        \n",
            "Checkpoint saved: 5630 batches, 56310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5630\n",
            "Processed 56310 pairs so far\n",
            "\n",
            "        API Calls: 11262\n",
            "        Total Tokens: 4257132\n",
            "        Estimated Cost: $8.5143\n",
            "        \n",
            "Checkpoint saved: 5635 batches, 56360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5635\n",
            "Processed 56360 pairs so far\n",
            "\n",
            "        API Calls: 11272\n",
            "        Total Tokens: 4260818\n",
            "        Estimated Cost: $8.5216\n",
            "        \n",
            "Checkpoint saved: 5640 batches, 56410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5640\n",
            "Processed 56410 pairs so far\n",
            "\n",
            "        API Calls: 11282\n",
            "        Total Tokens: 4264614\n",
            "        Estimated Cost: $8.5292\n",
            "        \n",
            "Checkpoint saved: 5645 batches, 56460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5645\n",
            "Processed 56460 pairs so far\n",
            "\n",
            "        API Calls: 11292\n",
            "        Total Tokens: 4268363\n",
            "        Estimated Cost: $8.5367\n",
            "        \n",
            "Checkpoint saved: 5650 batches, 56510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5650\n",
            "Processed 56510 pairs so far\n",
            "\n",
            "        API Calls: 11302\n",
            "        Total Tokens: 4272383\n",
            "        Estimated Cost: $8.5448\n",
            "        \n",
            "Checkpoint saved: 5655 batches, 56560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5655\n",
            "Processed 56560 pairs so far\n",
            "\n",
            "        API Calls: 11312\n",
            "        Total Tokens: 4276130\n",
            "        Estimated Cost: $8.5523\n",
            "        \n",
            "Checkpoint saved: 5660 batches, 56610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5660\n",
            "Processed 56610 pairs so far\n",
            "\n",
            "        API Calls: 11322\n",
            "        Total Tokens: 4279707\n",
            "        Estimated Cost: $8.5594\n",
            "        \n",
            "Checkpoint saved: 5665 batches, 56660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5665\n",
            "Processed 56660 pairs so far\n",
            "\n",
            "        API Calls: 11332\n",
            "        Total Tokens: 4283727\n",
            "        Estimated Cost: $8.5675\n",
            "        \n",
            "Checkpoint saved: 5670 batches, 56710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5670\n",
            "Processed 56710 pairs so far\n",
            "\n",
            "        API Calls: 11342\n",
            "        Total Tokens: 4287385\n",
            "        Estimated Cost: $8.5748\n",
            "        \n",
            "Checkpoint saved: 5675 batches, 56760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5675\n",
            "Processed 56760 pairs so far\n",
            "\n",
            "        API Calls: 11352\n",
            "        Total Tokens: 4291098\n",
            "        Estimated Cost: $8.5822\n",
            "        \n",
            "Checkpoint saved: 5680 batches, 56810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5680\n",
            "Processed 56810 pairs so far\n",
            "\n",
            "        API Calls: 11362\n",
            "        Total Tokens: 4295011\n",
            "        Estimated Cost: $8.5900\n",
            "        \n",
            "Checkpoint saved: 5685 batches, 56860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5685\n",
            "Processed 56860 pairs so far\n",
            "\n",
            "        API Calls: 11372\n",
            "        Total Tokens: 4299024\n",
            "        Estimated Cost: $8.5980\n",
            "        \n",
            "Checkpoint saved: 5690 batches, 56910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5690\n",
            "Processed 56910 pairs so far\n",
            "\n",
            "        API Calls: 11382\n",
            "        Total Tokens: 4303032\n",
            "        Estimated Cost: $8.6061\n",
            "        \n",
            "Checkpoint saved: 5695 batches, 56960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5695\n",
            "Processed 56960 pairs so far\n",
            "\n",
            "        API Calls: 11392\n",
            "        Total Tokens: 4306502\n",
            "        Estimated Cost: $8.6130\n",
            "        \n",
            "Checkpoint saved: 5700 batches, 57010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5700\n",
            "Processed 57010 pairs so far\n",
            "\n",
            "        API Calls: 11402\n",
            "        Total Tokens: 4309889\n",
            "        Estimated Cost: $8.6198\n",
            "        \n",
            "Checkpoint saved: 5705 batches, 57060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5705\n",
            "Processed 57060 pairs so far\n",
            "\n",
            "        API Calls: 11412\n",
            "        Total Tokens: 4313418\n",
            "        Estimated Cost: $8.6268\n",
            "        \n",
            "Checkpoint saved: 5710 batches, 57110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5710\n",
            "Processed 57110 pairs so far\n",
            "\n",
            "        API Calls: 11422\n",
            "        Total Tokens: 4316951\n",
            "        Estimated Cost: $8.6339\n",
            "        \n",
            "Checkpoint saved: 5715 batches, 57160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5715\n",
            "Processed 57160 pairs so far\n",
            "\n",
            "        API Calls: 11432\n",
            "        Total Tokens: 4320818\n",
            "        Estimated Cost: $8.6416\n",
            "        \n",
            "Checkpoint saved: 5720 batches, 57210 pairs\n",
            "\n",
            "Checkpoint saved at batch 5720\n",
            "Processed 57210 pairs so far\n",
            "\n",
            "        API Calls: 11442\n",
            "        Total Tokens: 4324311\n",
            "        Estimated Cost: $8.6486\n",
            "        \n",
            "Checkpoint saved: 5725 batches, 57260 pairs\n",
            "\n",
            "Checkpoint saved at batch 5725\n",
            "Processed 57260 pairs so far\n",
            "\n",
            "        API Calls: 11452\n",
            "        Total Tokens: 4327771\n",
            "        Estimated Cost: $8.6555\n",
            "        \n",
            "Checkpoint saved: 5730 batches, 57310 pairs\n",
            "\n",
            "Checkpoint saved at batch 5730\n",
            "Processed 57310 pairs so far\n",
            "\n",
            "        API Calls: 11462\n",
            "        Total Tokens: 4331620\n",
            "        Estimated Cost: $8.6632\n",
            "        \n",
            "Checkpoint saved: 5735 batches, 57360 pairs\n",
            "\n",
            "Checkpoint saved at batch 5735\n",
            "Processed 57360 pairs so far\n",
            "\n",
            "        API Calls: 11472\n",
            "        Total Tokens: 4335209\n",
            "        Estimated Cost: $8.6704\n",
            "        \n",
            "Checkpoint saved: 5740 batches, 57410 pairs\n",
            "\n",
            "Checkpoint saved at batch 5740\n",
            "Processed 57410 pairs so far\n",
            "\n",
            "        API Calls: 11482\n",
            "        Total Tokens: 4339120\n",
            "        Estimated Cost: $8.6782\n",
            "        \n",
            "Checkpoint saved: 5745 batches, 57460 pairs\n",
            "\n",
            "Checkpoint saved at batch 5745\n",
            "Processed 57460 pairs so far\n",
            "\n",
            "        API Calls: 11492\n",
            "        Total Tokens: 4343258\n",
            "        Estimated Cost: $8.6865\n",
            "        \n",
            "Checkpoint saved: 5750 batches, 57510 pairs\n",
            "\n",
            "Checkpoint saved at batch 5750\n",
            "Processed 57510 pairs so far\n",
            "\n",
            "        API Calls: 11502\n",
            "        Total Tokens: 4346741\n",
            "        Estimated Cost: $8.6935\n",
            "        \n",
            "Checkpoint saved: 5755 batches, 57560 pairs\n",
            "\n",
            "Checkpoint saved at batch 5755\n",
            "Processed 57560 pairs so far\n",
            "\n",
            "        API Calls: 11512\n",
            "        Total Tokens: 4350509\n",
            "        Estimated Cost: $8.7010\n",
            "        \n",
            "Checkpoint saved: 5760 batches, 57610 pairs\n",
            "\n",
            "Checkpoint saved at batch 5760\n",
            "Processed 57610 pairs so far\n",
            "\n",
            "        API Calls: 11522\n",
            "        Total Tokens: 4354256\n",
            "        Estimated Cost: $8.7085\n",
            "        \n",
            "Checkpoint saved: 5765 batches, 57660 pairs\n",
            "\n",
            "Checkpoint saved at batch 5765\n",
            "Processed 57660 pairs so far\n",
            "\n",
            "        API Calls: 11532\n",
            "        Total Tokens: 4357826\n",
            "        Estimated Cost: $8.7157\n",
            "        \n",
            "Checkpoint saved: 5770 batches, 57710 pairs\n",
            "\n",
            "Checkpoint saved at batch 5770\n",
            "Processed 57710 pairs so far\n",
            "\n",
            "        API Calls: 11542\n",
            "        Total Tokens: 4361855\n",
            "        Estimated Cost: $8.7237\n",
            "        \n",
            "Checkpoint saved: 5775 batches, 57760 pairs\n",
            "\n",
            "Checkpoint saved at batch 5775\n",
            "Processed 57760 pairs so far\n",
            "\n",
            "        API Calls: 11552\n",
            "        Total Tokens: 4365368\n",
            "        Estimated Cost: $8.7307\n",
            "        \n",
            "Checkpoint saved: 5780 batches, 57810 pairs\n",
            "\n",
            "Checkpoint saved at batch 5780\n",
            "Processed 57810 pairs so far\n",
            "\n",
            "        API Calls: 11562\n",
            "        Total Tokens: 4369042\n",
            "        Estimated Cost: $8.7381\n",
            "        \n",
            "Checkpoint saved: 5785 batches, 57860 pairs\n",
            "\n",
            "Checkpoint saved at batch 5785\n",
            "Processed 57860 pairs so far\n",
            "\n",
            "        API Calls: 11572\n",
            "        Total Tokens: 4372490\n",
            "        Estimated Cost: $8.7450\n",
            "        \n",
            "Checkpoint saved: 5790 batches, 57910 pairs\n",
            "\n",
            "Checkpoint saved at batch 5790\n",
            "Processed 57910 pairs so far\n",
            "\n",
            "        API Calls: 11582\n",
            "        Total Tokens: 4375693\n",
            "        Estimated Cost: $8.7514\n",
            "        \n",
            "Checkpoint saved: 5795 batches, 57960 pairs\n",
            "\n",
            "Checkpoint saved at batch 5795\n",
            "Processed 57960 pairs so far\n",
            "\n",
            "        API Calls: 11592\n",
            "        Total Tokens: 4379831\n",
            "        Estimated Cost: $8.7597\n",
            "        \n",
            "Checkpoint saved: 5800 batches, 58010 pairs\n",
            "\n",
            "Checkpoint saved at batch 5800\n",
            "Processed 58010 pairs so far\n",
            "\n",
            "        API Calls: 11602\n",
            "        Total Tokens: 4383615\n",
            "        Estimated Cost: $8.7672\n",
            "        \n",
            "Checkpoint saved: 5805 batches, 58060 pairs\n",
            "\n",
            "Checkpoint saved at batch 5805\n",
            "Processed 58060 pairs so far\n",
            "\n",
            "        API Calls: 11612\n",
            "        Total Tokens: 4387700\n",
            "        Estimated Cost: $8.7754\n",
            "        \n",
            "Checkpoint saved: 5810 batches, 58110 pairs\n",
            "\n",
            "Checkpoint saved at batch 5810\n",
            "Processed 58110 pairs so far\n",
            "\n",
            "        API Calls: 11622\n",
            "        Total Tokens: 4391515\n",
            "        Estimated Cost: $8.7830\n",
            "        \n",
            "Checkpoint saved: 5815 batches, 58160 pairs\n",
            "\n",
            "Checkpoint saved at batch 5815\n",
            "Processed 58160 pairs so far\n",
            "\n",
            "        API Calls: 11632\n",
            "        Total Tokens: 4395559\n",
            "        Estimated Cost: $8.7911\n",
            "        \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpY_aoPleMop",
        "outputId": "e025e7f0-9b00-476b-aecc-bc5a9af77f74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['sentence_translation', 'paraphrase_translation'], dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load your Excel file\n",
        "df = pd.read_csv('/content/drive/My Drive/Colab Notebooks/paranmt_small_full_translated_pairs.csv')  # Replace with your file path\n",
        "\n",
        "# Remove rows where the specific column contains '---'\n",
        "df = df[(df['sentence_translation'] != '---') &(df['paraphrase_translation'] != '---')]  # Replace 'YourColumn' with your column name\n",
        "\n",
        "# Save the cleaned data back to Excel\n",
        "df.to_csv('/content/drive/My Drive/Colab Notebooks/paranmt_small_full_translated_pairs_cleaned.csv', index=False)\n"
      ],
      "metadata": {
        "id": "LN67ui1WSDhO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iZetRXUWBkXq"
      },
      "source": [
        "## **ppdb-1.0-s-m2o**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4M10q7Ite_fu"
      },
      "source": [
        "#### Processed batch 50/1666 (32 items)\n",
        "\n",
        "        API Calls: 51\n",
        "        Total Tokens: 537225\n",
        "        Estimated Cost: $1.07\n",
        "        \n",
        "Processed batch 51/1666 (32 items)\n",
        "Processed batch 52/1666 (32 items)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_NetKpqc929"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, List, Dict, Any, Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [\"\"] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if text in cache:\n",
        "                    translations[i] = cache[text]\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for text, trans in zip(uncached_texts, new_translations):\n",
        "                            self.batch_cache[text] = trans\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = trans\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2)  # Wait before retrying\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = f\"ERROR: {str(e)}\"\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 10949):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"rb\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[str]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"rb\") as f:\n",
        "                # Skip to start line\n",
        "                for _ in range(self.start_line):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        text = line.strip().decode('utf-8')\n",
        "                        if text:  # Only add non-empty texts\n",
        "                            current_batch.append(text)\n",
        "                            processed_lines += 1\n",
        "\n",
        "                            if len(current_batch) == self.batch_size:\n",
        "                                yield current_batch\n",
        "                                current_batch = []\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing line: {line.strip()}\")\n",
        "                        print(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 53315\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translations_count = checkpoint[\"translations_count\"]\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    # Create iterator with sentence limit\n",
        "    dataset_iterator = DatasetIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_line=start_line,\n",
        "        max_sentences=max_sentences\n",
        "    )\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    print(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    print(f\"Will process up to {max_sentences} sentences\")\n",
        "    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                print(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                print(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    print(\"\\nFinal Statistics:\")\n",
        "    print(cost_tracker.report())\n",
        "    print(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "yeoIw578nCt3",
        "outputId": "8bfcd8e6-d071-46a7-e669-e3e92417696a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Starting from batch 0, line 0\n",
            "Will process up to 53315 sentences\n",
            "Total lines to process: 53315\n",
            "Checkpoint saved at batch 0\n",
            "Processed batch 0/1666 (32 items)\n",
            "\n",
            "        API Calls: 1\n",
            "        Total Tokens: 7785\n",
            "        Estimated Cost: $0.02\n",
            "        \n",
            "Processed batch 1/1666 (32 items)\n",
            "Processed batch 2/1666 (32 items)\n",
            "Processed batch 3/1666 (32 items)\n",
            "Processed batch 4/1666 (32 items)\n",
            "Processed batch 5/1666 (32 items)\n",
            "Processed batch 6/1666 (32 items)\n",
            "Processed batch 7/1666 (32 items)\n",
            "Processed batch 8/1666 (32 items)\n",
            "Processed batch 9/1666 (32 items)\n",
            "Checkpoint saved at batch 10\n",
            "Processed batch 10/1666 (32 items)\n",
            "\n",
            "        API Calls: 11\n",
            "        Total Tokens: 113148\n",
            "        Estimated Cost: $0.23\n",
            "        \n",
            "Processed batch 11/1666 (32 items)\n",
            "Processed batch 12/1666 (32 items)\n",
            "Processed batch 13/1666 (32 items)\n",
            "Processed batch 14/1666 (32 items)\n",
            "Processed batch 15/1666 (32 items)\n",
            "Processed batch 16/1666 (32 items)\n",
            "Processed batch 17/1666 (32 items)\n",
            "Processed batch 18/1666 (32 items)\n",
            "Processed batch 19/1666 (32 items)\n",
            "Checkpoint saved at batch 20\n",
            "Processed batch 20/1666 (32 items)\n",
            "\n",
            "        API Calls: 21\n",
            "        Total Tokens: 218383\n",
            "        Estimated Cost: $0.44\n",
            "        \n",
            "Processed batch 21/1666 (32 items)\n",
            "Processed batch 22/1666 (32 items)\n",
            "Processed batch 23/1666 (32 items)\n",
            "Processed batch 24/1666 (32 items)\n",
            "Processed batch 25/1666 (32 items)\n",
            "Processed batch 26/1666 (32 items)\n",
            "Processed batch 27/1666 (32 items)\n",
            "Processed batch 28/1666 (32 items)\n",
            "Processed batch 29/1666 (32 items)\n",
            "Checkpoint saved at batch 30\n",
            "Processed batch 30/1666 (32 items)\n",
            "\n",
            "        API Calls: 31\n",
            "        Total Tokens: 319473\n",
            "        Estimated Cost: $0.64\n",
            "        \n",
            "Processed batch 31/1666 (32 items)\n",
            "Processed batch 32/1666 (32 items)\n",
            "Processed batch 33/1666 (32 items)\n",
            "Processed batch 34/1666 (32 items)\n",
            "Processed batch 35/1666 (32 items)\n",
            "Processed batch 36/1666 (32 items)\n",
            "Processed batch 37/1666 (32 items)\n",
            "Processed batch 38/1666 (32 items)\n",
            "Processed batch 39/1666 (32 items)\n",
            "Checkpoint saved at batch 40\n",
            "Processed batch 40/1666 (32 items)\n",
            "\n",
            "        API Calls: 41\n",
            "        Total Tokens: 424584\n",
            "        Estimated Cost: $0.85\n",
            "        \n",
            "Processed batch 41/1666 (32 items)\n",
            "Processed batch 42/1666 (32 items)\n",
            "Processed batch 43/1666 (32 items)\n",
            "Processed batch 44/1666 (32 items)\n",
            "Processed batch 45/1666 (32 items)\n",
            "Processed batch 46/1666 (32 items)\n",
            "Processed batch 47/1666 (32 items)\n",
            "Processed batch 48/1666 (32 items)\n",
            "Processed batch 49/1666 (32 items)\n",
            "Checkpoint saved at batch 50\n",
            "Processed batch 50/1666 (32 items)\n",
            "\n",
            "        API Calls: 51\n",
            "        Total Tokens: 537225\n",
            "        Estimated Cost: $1.07\n",
            "        \n",
            "Processed batch 51/1666 (32 items)\n",
            "Processed batch 52/1666 (32 items)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-13-2735f72bc999>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     process_dataset(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-933115c55510>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(input_file, dataset_name, api_key, batch_size, checkpoint_interval, max_sentences)\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m             \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset_name}_{batch_num}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 254\u001b[0;31m             \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    255\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-933115c55510>\u001b[0m in \u001b[0;36mtranslate_batch\u001b[0;34m(self, texts, batch_id)\u001b[0m\n\u001b[1;32m     78\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_attempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m                         response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     81\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                             messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    997\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/ppdb-1.0-s-m2o\"\n",
        "    dataset_name = \"ppdb-1.0-s-m2o_test\"\n",
        "    max_sentences = 53315  # Total number of sentences to process\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,\n",
        "        checkpoint_interval=10,  # Increased for larger dataset\n",
        "        max_sentences=max_sentences,\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2uD21LepBkXu"
      },
      "source": [
        "## **paws_train.tsv**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zLqJihJifL3b"
      },
      "source": [
        "##### Two tries of code - unsuccessful pairing of paraphrase sentences 1 and 2 from .tsv file\n",
        "\n",
        "Final Statistics of Try 1:\n",
        "\n",
        "        API Calls: 1544\n",
        "        Total Tokens: 3364654\n",
        "        Estimated Cost: $6.73\n",
        "        \n",
        "Total translations: 49401"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fo_M6FGuBgLR"
      },
      "outputs": [],
      "source": [
        "# Try 1\n",
        "# Code for translating TSV dataset with batching and checkpoints\n",
        "\n",
        "from typing import Iterator, List, Dict, Any, Tuple, Optional\n",
        "import numpy as np\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [\"\"] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if text in cache:\n",
        "                    translations[i] = cache[text]\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for text, trans in zip(uncached_texts, new_translations):\n",
        "                            self.batch_cache[text] = trans\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = trans\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2)  # Wait before retrying\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = f\"ERROR: {str(e)}\"\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, text_column_index: int = 1, max_sentences: int = 10949):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.text_column_index = text_column_index  # For TSV files, specify which column contains the text\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        # Add 1 to account for header in TSV\n",
        "        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            print(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[str]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip header\n",
        "                header = next(f, None)\n",
        "                if not header:\n",
        "                    raise ValueError(\"Empty file or no header found\")\n",
        "\n",
        "                # Skip to start line (accounting for already skipped header)\n",
        "                for _ in range(self.start_line - 1):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        columns = line.strip().split('\\t')\n",
        "                        if len(columns) > self.text_column_index:\n",
        "                            text = columns[self.text_column_index].strip()\n",
        "                            if text:  # Only add non-empty texts\n",
        "                                current_batch.append(text)\n",
        "                                processed_lines += 1\n",
        "\n",
        "                                if len(current_batch) == self.batch_size:\n",
        "                                    yield current_batch\n",
        "                                    current_batch = []\n",
        "                    except Exception as e:\n",
        "                        print(f\"Error processing line: {line.strip()}\")\n",
        "                        print(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 49402,\n",
        "    text_column_index: int = 1\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translations_count = checkpoint[\"translations_count\"]\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    # Create iterator with sentence limit and TSV column index\n",
        "    dataset_iterator = DatasetIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_line=start_line,\n",
        "        text_column_index=text_column_index,\n",
        "        max_sentences=max_sentences\n",
        "    )\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    print(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    print(f\"Will process up to {max_sentences} sentences\")\n",
        "    print(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "    print(f\"Reading text from column index: {text_column_index}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                print(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            print(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                print(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    print(\"\\nFinal Statistics:\")\n",
        "    print(cost_tracker.report())\n",
        "    print(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "Pvsa3wqq6LwP",
        "outputId": "c08a0791-56a7-443d-b664-59c61209dfbb"
      },
      "outputs": [
        {
          "ename": "TypeError",
          "evalue": "process_dataset() got an unexpected keyword argument 'text_column_index'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-0a7f0607f683>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     process_dataset(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: process_dataset() got an unexpected keyword argument 'text_column_index'"
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n",
        "    dataset_name = \"paws_paraphrase_test\"\n",
        "    max_sentences = 49402  # Total number of sentences to process\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,\n",
        "        checkpoint_interval=10,  # Increased for larger dataset\n",
        "        max_sentences=max_sentences,\n",
        "        text_column_index=1  # Specify which column contains the text to translate\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "afJjJjPuWmdU"
      },
      "outputs": [],
      "source": [
        "# Try 2\n",
        "# corrected code with data integrity (to make matched pairs of original sentences), and with additional logging, checkpointing and caching\n",
        "\n",
        "import logging\n",
        "from typing import Iterator, List, Dict, Tuple, Optional\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [\"\"] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if text in cache:\n",
        "                    translations[i] = cache[text]\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for text, trans in zip(uncached_texts, new_translations):\n",
        "                            self.batch_cache[text] = trans\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = trans\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2)  # Wait before retrying\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = f\"ERROR: {str(e)}\"\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 10949):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[Tuple[str, str]]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip header\n",
        "                header = next(f, None)\n",
        "                if not header:\n",
        "                    raise ValueError(\"Empty file or no header found\")\n",
        "\n",
        "                # Skip to start line (accounting for already skipped header)\n",
        "                for _ in range(self.start_line - 1):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        columns = line.strip().split('\\t')\n",
        "                        if len(columns) >= 3:  # Ensure there are enough columns\n",
        "                            sentence1 = columns[1].strip()\n",
        "                            sentence2 = columns[2].strip()\n",
        "                            if sentence1 and sentence2:  # Only add non-empty pairs\n",
        "                                current_batch.append((sentence1, sentence2))\n",
        "                                processed_lines += 1\n",
        "\n",
        "                                if len(current_batch) == self.batch_size:\n",
        "                                    yield current_batch\n",
        "                                    current_batch = []\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error processing line: {line.strip()}\")\n",
        "                        logging.error(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 49402\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        # Load checkpoint if exists\n",
        "        checkpoint = translator.load_checkpoint(dataset_name)\n",
        "        start_batch = checkpoint[\"last_batch\"] + 1\n",
        "        translations_count = checkpoint[\"translations_count\"]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading checkpoint: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    try:\n",
        "        # Create iterator with sentence limit\n",
        "        dataset_iterator = DatasetIterator(\n",
        "            file_path=input_file,\n",
        "            batch_size=batch_size,\n",
        "            start_line=start_line,\n",
        "            max_sentences=max_sentences\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error initializing dataset iterator: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    logging.info(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    logging.info(f\"Will process up to {max_sentences} sentences\")\n",
        "    logging.info(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            sentences1, sentences2 = zip(*batch) if batch else ([], [])\n",
        "            translations, response = translator.translate_batch(sentences1, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(sentences1, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                logging.info(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            logging.info(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                logging.info(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    logging.info(\"\\nFinal Statistics:\")\n",
        "    logging.info(cost_tracker.report())\n",
        "    logging.info(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "_P1YE9sHXtLU",
        "outputId": "3d92f6b0-5d00-498a-da6d-957f463a7fd5"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-acee5a8ccf88>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     process_dataset(\n\u001b[0m\u001b[1;32m     16\u001b[0m         \u001b[0minput_file\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_file\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mdataset_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-9a7ace776464>\u001b[0m in \u001b[0;36mprocess_dataset\u001b[0;34m(input_file, dataset_name, api_key, batch_size, checkpoint_interval, max_sentences)\u001b[0m\n\u001b[1;32m    274\u001b[0m             \u001b[0mbatch_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"{dataset_name}_{batch_num}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    275\u001b[0m             \u001b[0msentences1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msentences2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 276\u001b[0;31m             \u001b[0mtranslations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtranslator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-9a7ace776464>\u001b[0m in \u001b[0;36mtranslate_batch\u001b[0;34m(self, texts, batch_id)\u001b[0m\n\u001b[1;32m     83\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mattempt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mretry_attempts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m                         response = self.client.chat.completions.create(\n\u001b[0m\u001b[1;32m     86\u001b[0m                             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"gpt-3.5-turbo\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                             messages=[\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_utils/_utils.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m                         \u001b[0mmsg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34mf\"Missing required argument: {quote(missing[0])}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmsg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m  \u001b[0;31m# type: ignore\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/resources/chat/completions.py\u001b[0m in \u001b[0;36mcreate\u001b[0;34m(self, messages, model, audio, frequency_penalty, function_call, functions, logit_bias, logprobs, max_completion_tokens, max_tokens, metadata, modalities, n, parallel_tool_calls, prediction, presence_penalty, reasoning_effort, response_format, seed, service_tier, stop, store, stream, stream_options, temperature, tool_choice, tools, top_logprobs, top_p, user, extra_headers, extra_query, extra_body, timeout)\u001b[0m\n\u001b[1;32m    861\u001b[0m     ) -> ChatCompletion | Stream[ChatCompletionChunk]:\n\u001b[1;32m    862\u001b[0m         \u001b[0mvalidate_response_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m         return self._post(\n\u001b[0m\u001b[1;32m    864\u001b[0m             \u001b[0;34m\"/chat/completions\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m             body=maybe_transform(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mpost\u001b[0;34m(self, path, cast_to, body, options, files, stream, stream_cls)\u001b[0m\n\u001b[1;32m   1281\u001b[0m             \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"post\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0murl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mjson_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbody\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mto_httpx_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1282\u001b[0m         )\n\u001b[0;32m-> 1283\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mResponseT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstream_cls\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream_cls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1284\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1285\u001b[0m     def patch(\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36mrequest\u001b[0;34m(self, cast_to, options, remaining_retries, stream, stream_cls)\u001b[0m\n\u001b[1;32m    958\u001b[0m             \u001b[0mretries_taken\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    959\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 960\u001b[0;31m         return self._request(\n\u001b[0m\u001b[1;32m    961\u001b[0m             \u001b[0mcast_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcast_to\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m             \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/openai/_base_client.py\u001b[0m in \u001b[0;36m_request\u001b[0;34m(self, cast_to, options, retries_taken, stream, stream_cls)\u001b[0m\n\u001b[1;32m    994\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    995\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 996\u001b[0;31m             response = self._client.send(\n\u001b[0m\u001b[1;32m    997\u001b[0m                 \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    998\u001b[0m                 \u001b[0mstream\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstream\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_should_stream_response_body\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36msend\u001b[0;34m(self, request, stream, auth, follow_redirects)\u001b[0m\n\u001b[1;32m    912\u001b[0m         \u001b[0mauth\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_build_request_auth\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 914\u001b[0;31m         response = self._send_handling_auth(\n\u001b[0m\u001b[1;32m    915\u001b[0m             \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    916\u001b[0m             \u001b[0mauth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauth\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_auth\u001b[0;34m(self, request, auth, follow_redirects, history)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m                 response = self._send_handling_redirects(\n\u001b[0m\u001b[1;32m    943\u001b[0m                     \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m                     \u001b[0mfollow_redirects\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfollow_redirects\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_handling_redirects\u001b[0;34m(self, request, follow_redirects, history)\u001b[0m\n\u001b[1;32m    977\u001b[0m                 \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    978\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 979\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_send_single_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    980\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    981\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event_hooks\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"response\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_client.py\u001b[0m in \u001b[0;36m_send_single_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m   1012\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1013\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrequest_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1014\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtransport\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1015\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1016\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSyncByteStream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpx/_transports/default.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    248\u001b[0m         )\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_httpcore_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 250\u001b[0;31m             \u001b[0mresp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pool\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    251\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtyping\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_close_connections\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosing\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 256\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    258\u001b[0m         \u001b[0;31m# Return the response. Note that in this case we still have to manage\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection_pool.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    234\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    235\u001b[0m                     \u001b[0;31m# Send the request on the assigned connection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 236\u001b[0;31m                     response = connection.handle_request(\n\u001b[0m\u001b[1;32m    237\u001b[0m                         \u001b[0mpool_request\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m                     )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/connection.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    101\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_connection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandle_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_connect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mRequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mNetworkStream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    134\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"response_closed\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_response_closed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 136\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mexc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m     \u001b[0;31m# Sending the request...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36mhandle_request\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    104\u001b[0m                     \u001b[0mheaders\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m                     \u001b[0mtrailing_data\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 106\u001b[0;31m                 ) = self._receive_response_headers(**kwargs)\n\u001b[0m\u001b[1;32m    107\u001b[0m                 trace.return_value = (\n\u001b[1;32m    108\u001b[0m                     \u001b[0mhttp_version\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_response_headers\u001b[0;34m(self, request)\u001b[0m\n\u001b[1;32m    175\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    176\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 177\u001b[0;31m             \u001b[0mevent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_receive_event\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    178\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mResponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_sync/http11.py\u001b[0m in \u001b[0;36m_receive_event\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mevent\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mh11\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNEED_DATA\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m                 data = self._network_stream.read(\n\u001b[0m\u001b[1;32m    218\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mREAD_NUM_BYTES\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m                 )\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/httpcore/_backends/sync.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, max_bytes, timeout)\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmap_exceptions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexc_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msettimeout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_bytes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbytes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mfloat\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mrecv\u001b[0;34m(self, buflen, flags)\u001b[0m\n\u001b[1;32m   1293\u001b[0m                     \u001b[0;34m\"non-zero flags not allowed in calls to recv() on %s\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1294\u001b[0m                     self.__class__)\n\u001b[0;32m-> 1295\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1296\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1297\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuflen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/ssl.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(self, len, buffer)\u001b[0m\n\u001b[1;32m   1166\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1167\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1168\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sslobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1169\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mSSLError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1170\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mSSL_ERROR_EOF\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msuppress_ragged_eofs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n",
        "    dataset_name = \"paws_paraphrase_test_again\"\n",
        "    max_sentences = 49402  # Total number of sentences to process\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,\n",
        "        checkpoint_interval=10,  # Increased for larger dataset\n",
        "        max_sentences=max_sentences\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPMWLbqcvROM"
      },
      "source": [
        "#### Final code - execution for around 6h (20.000 pairs of paraphrases)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fyzbbA6-oatm"
      },
      "outputs": [],
      "source": [
        "# Try 3 - code more similar to the GaMS model translation (similar simplify batching, added cache keys, checkpointing)\n",
        "\n",
        "import logging\n",
        "from typing import Iterator, List, Dict, Tuple, Optional\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[Tuple[str, str]], batch_id: str) -> Tuple[List[Tuple[str, str]], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [(\"\", \"\")] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, (text1, text2) in enumerate(texts):\n",
        "                text1, text2 = text1.strip(), text2.strip()\n",
        "                cache_key = f\"{text1}_{text2}\"\n",
        "                if cache_key in cache:\n",
        "                    translations[i] = cache[cache_key]\n",
        "                else:\n",
        "                    uncached_texts.append((text1, text2))\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join([f\"{t1} - {t2}\" for t1, t2 in uncached_texts])}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for (text1, text2), trans in zip(uncached_texts, new_translations):\n",
        "                            cache_key = f\"{text1}_{text2}\"\n",
        "                            self.batch_cache[cache_key] = (text1, trans)\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = (texts[idx][0], trans)\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2)  # Wait before retrying\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = (texts[idx][0], f\"ERROR: {str(e)}\")\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in batch processing: {str(e)}\")\n",
        "            return [(text[0], f\"ERROR: {str(e)}\") for text in texts], None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 10949):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[Tuple[str, str]]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip header\n",
        "                header = next(f, None)\n",
        "                if not header:\n",
        "                    raise ValueError(\"Empty file or no header found\")\n",
        "\n",
        "                # Skip to start line (accounting for already skipped header)\n",
        "                for _ in range(self.start_line - 1):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        columns = line.strip().split('\\t')\n",
        "                        if len(columns) >= 3:  # Ensure there are enough columns\n",
        "                            sentence1 = columns[1].strip()\n",
        "                            sentence2 = columns[2].strip()\n",
        "                            if sentence1 and sentence2:  # Only add non-empty pairs\n",
        "                                current_batch.append((sentence1, sentence2))\n",
        "                                processed_lines += 1\n",
        "\n",
        "                                if len(current_batch) == self.batch_size:\n",
        "                                    yield current_batch\n",
        "                                    current_batch = []\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error processing line: {line.strip()}\")\n",
        "                        logging.error(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[Tuple[str, str]], translations: List[Tuple[str, str]], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", [orig[0] for orig in originals]),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", [trans[1] for trans in translations]),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig[0]}\\nTranslation: {trans[1]}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 20000\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        # Load checkpoint if exists\n",
        "        checkpoint = translator.load_checkpoint(dataset_name)\n",
        "        start_batch = checkpoint[\"last_batch\"] + 1\n",
        "        translations_count = checkpoint[\"translations_count\"]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading checkpoint: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    try:\n",
        "        # Create iterator with sentence limit\n",
        "        dataset_iterator = DatasetIterator(\n",
        "            file_path=input_file,\n",
        "            batch_size=batch_size,\n",
        "            start_line=start_line,\n",
        "            max_sentences=max_sentences\n",
        "        )\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error initializing dataset iterator: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    logging.info(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    logging.info(f\"Will process up to {max_sentences} sentences\")\n",
        "    logging.info(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                logging.info(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            logging.info(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                logging.info(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    logging.info(\"\\nFinal Statistics:\")\n",
        "    logging.info(cost_tracker.report())\n",
        "    logging.info(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mx1HJPsUHjbo"
      },
      "outputs": [],
      "source": [
        "# Variation of code for the sentences from 20.001 on\n",
        "\n",
        "# Try 3 - code more similar to the GaMS model translation (similar simplify batching, added cache keys, checkpointing)\n",
        "\n",
        "import logging\n",
        "from typing import Iterator, List, Dict, Tuple, Optional\n",
        "import json\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[Tuple[str, str]], batch_id: str) -> Tuple[List[Tuple[str, str]], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = [(\"\", \"\")] * len(texts)  # Initialize with empty strings\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, (text1, text2) in enumerate(texts):\n",
        "                text1, text2 = text1.strip(), text2.strip()\n",
        "                cache_key = f\"{text1}_{text2}\"\n",
        "                if cache_key in cache:\n",
        "                    translations[i] = cache[cache_key]\n",
        "                else:\n",
        "                    uncached_texts.append((text1, text2))\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join([f\"{t1} - {t2}\" for t1, t2 in uncached_texts])}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for (text1, text2), trans in zip(uncached_texts, new_translations):\n",
        "                            cache_key = f\"{text1}_{text2}\"\n",
        "                            self.batch_cache[cache_key] = (text1, trans)\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations[idx] = (texts[idx][0], trans)\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2)  # Wait before retrying\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations[idx] = (texts[idx][0], f\"ERROR: {str(e)}\")\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in batch processing: {str(e)}\")\n",
        "            return [(text[0], f\"ERROR: {str(e)}\") for text in texts], None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 10949):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line + 1, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[Tuple[str, str]]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip header\n",
        "                header = next(f, None)\n",
        "                if not header:\n",
        "                    raise ValueError(\"Empty file or no header found\")\n",
        "\n",
        "                # Skip to start line (accounting for already skipped header)\n",
        "                for _ in range(self.start_line - 1):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    try:\n",
        "                        columns = line.strip().split('\\t')\n",
        "                        if len(columns) >= 3:  # Ensure there are enough columns\n",
        "                            sentence1 = columns[1].strip()\n",
        "                            sentence2 = columns[2].strip()\n",
        "                            if sentence1 and sentence2:  # Only add non-empty pairs\n",
        "                                current_batch.append((sentence1, sentence2))\n",
        "                                processed_lines += 1\n",
        "\n",
        "                                if len(current_batch) == self.batch_size:\n",
        "                                    yield current_batch\n",
        "                                    current_batch = []\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error processing line: {line.strip()}\")\n",
        "                        logging.error(f\"Error details: {str(e)}\")\n",
        "                        continue\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[Tuple[str, str]], translations: List[Tuple[str, str]], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", [orig[0] for orig in originals]),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", [trans[1] for trans in translations]),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig[0]}\\nTranslation: {trans[1]}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 5,\n",
        "    max_sentences: int = 20000,\n",
        "    start_line: int = 0  # Add start_line parameter with default value\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    try:\n",
        "        # Load checkpoint if exists\n",
        "        checkpoint = translator.load_checkpoint(dataset_name)\n",
        "        start_batch = checkpoint[\"last_batch\"] + 1\n",
        "        translations_count = checkpoint[\"translations_count\"]\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error loading checkpoint: {str(e)}\")\n",
        "        return\n",
        "\n",
        "    # Create iterator with sentence limit and starting line\n",
        "    dataset_iterator = DatasetIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_line=start_line,  # Use the start_line parameter\n",
        "        max_sentences=max_sentences\n",
        "    )\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    logging.info(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    logging.info(f\"Will process up to {max_sentences} sentences\")\n",
        "    logging.info(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                logging.info(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            logging.info(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                logging.info(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    logging.info(\"\\nFinal Statistics:\")\n",
        "    logging.info(cost_tracker.report())\n",
        "    logging.info(f\"Total translations: {translations_count}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                logging.info(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            logging.info(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                logging.info(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    logging.info(\"\\nFinal Statistics:\")\n",
        "    logging.info(cost_tracker.report())\n",
        "    logging.info(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true
        },
        "id": "lbyJVemMtsmd"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n",
        "    dataset_name = \"paws_paraphrase_test_3.1\"\n",
        "    max_sentences = 20000  # Total number of sentences to process\n",
        "    start_line = 20001  # Start from line 20,001 and this line can be commented for the first half of the data set\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,\n",
        "        checkpoint_interval=10,  # Increased for larger dataset\n",
        "        max_sentences=max_sentences,\n",
        "        start_line=start_line  # Pass the start_line parameter and this line can be commented for the first half of the data set\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "faVNT2NCBkXu"
      },
      "source": [
        "#### Check the number of lines in the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFJknMiJBkXu",
        "outputId": "315e9965-8e56-4cac-cc9c-e3f330c4f8c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found!\n",
            "The file has 88537 rows.\n"
          ]
        }
      ],
      "source": [
        "# Chech the size of the file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/paws_paraphrase_test_originals_GPT3.5.txt\"\n",
        "\n",
        "# Check if file exists\n",
        "import os\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found!\")\n",
        "else:\n",
        "    print(\"File not found. Check the path!\")\n",
        "\n",
        "#file_path = \"/content/drive/My Drive/Colab Notebooks/paws_train.tsv\"\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/paws_paraphrase_test_originals_GPT3.5.txt\"\n",
        "\n",
        "# Count lines in the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n",
        "\n",
        "print(f\"The file has {line_count} rows.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E3utXiV_BkXv"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iQO0l3V8lR6"
      },
      "source": [
        "####"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QX4Hvgo88jtl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fdUr6j_0BkXz"
      },
      "source": [
        "## **Quora Duplicate Questions**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVsiYu9eBkXz"
      },
      "source": [
        "#### Checking the size of the file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w1RGOCsPBkXz",
        "outputId": "acbcff30-d8fc-4059-ed9a-cfd50af946f7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File found!\n",
            "The file has 200032 rows.\n"
          ]
        }
      ],
      "source": [
        "# Check the size of the file\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "#file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n",
        "#file_path = \"/content/drive/My Drive/Colab Notebooks/quora_test_translations_GPT3.5.txt\"\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/quora_test_originals_GPT3.5.txt\"\n",
        "\n",
        "# Check if file exists\n",
        "import os\n",
        "if os.path.exists(file_path):\n",
        "    print(\"File found!\")\n",
        "else:\n",
        "    print(\"File not found. Check the path!\")\n",
        "\n",
        "#file_path = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n",
        "#file_path = \"/content/drive/My Drive/Colab Notebooks/quora_test_translations_GPT3.5.txt\"\n",
        "file_path = \"/content/drive/My Drive/Colab Notebooks/quora_test_originals_GPT3.5.txt\"\n",
        "\n",
        "# Count lines in the file\n",
        "with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
        "    line_count = sum(1 for line in file if line.strip())  # Excludes empty lines\n",
        "\n",
        "print(f\"The file has {line_count} rows.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_qBaj9Tq3wH"
      },
      "source": [
        "#### Translate with in-between checkpoints (because it is the biggest file of all)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tlKxaHfcq8-I"
      },
      "outputs": [],
      "source": [
        "from typing import Iterator, List, Dict, Any, Tuple, Optional\n",
        "import json\n",
        "import os\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from openai import OpenAI\n",
        "from openai.types.chat import ChatCompletion\n",
        "import time\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
        "\n",
        "# Configure logging\n",
        "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
        "\n",
        "class TranslationManager:\n",
        "    def __init__(self, api_key: str, cache_dir: str = \"translation_cache\", checkpoint_dir: str = \"checkpoints\"):\n",
        "        self.client = OpenAI(api_key=api_key)\n",
        "        self.cache_dir = Path(cache_dir)\n",
        "        self.checkpoint_dir = Path(checkpoint_dir)\n",
        "        self.cache_dir.mkdir(exist_ok=True)\n",
        "        self.checkpoint_dir.mkdir(exist_ok=True)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "        self.max_cache_size = 1000\n",
        "\n",
        "    def _get_cache_file(self, batch_id: str) -> Path:\n",
        "        return self.cache_dir / f\"cache_{batch_id}.json\"\n",
        "\n",
        "    def get_checkpoint_file(self, dataset_name: str) -> Path:\n",
        "        return self.checkpoint_dir / f\"{dataset_name}_checkpoint.json\"\n",
        "\n",
        "    def _save_batch_cache(self, batch_id: str):\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        with open(cache_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(self.batch_cache, f, ensure_ascii=False, indent=2)\n",
        "        self.batch_cache = {}\n",
        "        self.current_cache_size = 0\n",
        "\n",
        "    def _load_cache(self, batch_id: str) -> Dict:\n",
        "        cache_file = self._get_cache_file(batch_id)\n",
        "        if cache_file.exists():\n",
        "            with open(cache_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {}\n",
        "\n",
        "    def save_checkpoint(self, dataset_name: str, batch_num: int, translations_count: int):\n",
        "        checkpoint_data = {\n",
        "            \"last_batch\": batch_num,\n",
        "            \"translations_count\": translations_count,\n",
        "            \"timestamp\": datetime.now().isoformat()\n",
        "        }\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        with open(checkpoint_file, 'w', encoding='utf-8') as f:\n",
        "            json.dump(checkpoint_data, f, indent=2)\n",
        "        logging.info(f\"Checkpoint saved to {checkpoint_file}\")\n",
        "\n",
        "    def load_checkpoint(self, dataset_name: str) -> Dict:\n",
        "        checkpoint_file = self.get_checkpoint_file(dataset_name)\n",
        "        if checkpoint_file.exists():\n",
        "            with open(checkpoint_file, 'r', encoding='utf-8') as f:\n",
        "                return json.load(f)\n",
        "        return {\"last_batch\": -1, \"translations_count\": 0}\n",
        "\n",
        "    def translate_batch(self, texts: List[str], batch_id: str) -> Tuple[List[str], Optional[ChatCompletion]]:\n",
        "        if not texts:\n",
        "            return [], None\n",
        "\n",
        "        try:\n",
        "            cache = self._load_cache(batch_id)\n",
        "            translations = []\n",
        "            uncached_texts = []\n",
        "            uncached_indices = []\n",
        "\n",
        "            for i, text in enumerate(texts):\n",
        "                text = text.strip()\n",
        "                if not text:  # Skip empty strings\n",
        "                    translations.append(\"\")\n",
        "                    continue\n",
        "                if text in cache:\n",
        "                    translations.append(cache[text])\n",
        "                else:\n",
        "                    uncached_texts.append(text)\n",
        "                    uncached_indices.append(i)\n",
        "\n",
        "            if uncached_texts:\n",
        "                retry_attempts = 3\n",
        "                for attempt in range(retry_attempts):\n",
        "                    try:\n",
        "                        response = self.client.chat.completions.create(\n",
        "                            model=\"gpt-3.5-turbo\",\n",
        "                            messages=[\n",
        "                                {\"role\": \"system\", \"content\": \"Translate the following English texts to Slovenian:\"},\n",
        "                                {\"role\": \"user\", \"content\": \"\\n-\\n\".join(uncached_texts)}\n",
        "                            ],\n",
        "                            temperature=0.3\n",
        "                        )\n",
        "\n",
        "                        new_translations = [choice.message.content for choice in response.choices]\n",
        "\n",
        "                        # Ensure we have the same number of translations as input texts\n",
        "                        if len(new_translations) != len(uncached_texts):\n",
        "                            new_translations = new_translations[:len(uncached_texts)]\n",
        "                            if len(new_translations) < len(uncached_texts):\n",
        "                                new_translations.extend([\"\"] * (len(uncached_texts) - len(new_translations)))\n",
        "\n",
        "                        for text, trans in zip(uncached_texts, new_translations):\n",
        "                            self.batch_cache[text] = trans\n",
        "                            self.current_cache_size += 1\n",
        "\n",
        "                        for idx, trans in zip(uncached_indices, new_translations):\n",
        "                            translations.insert(idx, trans)\n",
        "\n",
        "                        if self.current_cache_size >= self.max_cache_size:\n",
        "                            self._save_batch_cache(batch_id)\n",
        "\n",
        "                        return translations, response\n",
        "\n",
        "                    except Exception as e:\n",
        "                        logging.error(f\"Error in API call (attempt {attempt + 1}/{retry_attempts}): {str(e)}\")\n",
        "                        if attempt < retry_attempts - 1:\n",
        "                            time.sleep(2 ** attempt)  # Exponential backoff\n",
        "                        else:\n",
        "                            # If all attempts fail, log the error and return partial results\n",
        "                            for idx in uncached_indices:\n",
        "                                translations.insert(idx, f\"ERROR: {str(e)}\")\n",
        "                            return translations, None\n",
        "\n",
        "            return translations, None\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error in batch processing: {str(e)}\")\n",
        "            return [f\"ERROR: {str(e)}\"] * len(texts), None\n",
        "\n",
        "class DatasetIterator:\n",
        "    def __init__(self, file_path: str, batch_size: int, start_line: int = 0, max_sentences: int = 400000):\n",
        "        self.file_path = file_path\n",
        "        self.batch_size = batch_size\n",
        "        self.max_sentences = max_sentences\n",
        "        self.total_lines = min(self._count_lines(), max_sentences)  # Limit total lines\n",
        "        self.start_line = min(start_line, max(0, self.total_lines - 1))\n",
        "\n",
        "    def _count_lines(self) -> int:\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                return sum(1 for _ in f)\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error counting lines: {str(e)}\")\n",
        "            return 0\n",
        "\n",
        "    def __iter__(self) -> Iterator[List[str]]:\n",
        "        current_batch = []\n",
        "        processed_lines = 0\n",
        "\n",
        "        try:\n",
        "            with open(self.file_path, \"r\", encoding=\"utf-8\") as f:\n",
        "                # Skip to start line\n",
        "                for _ in range(self.start_line):\n",
        "                    next(f, None)\n",
        "\n",
        "                for line in f:\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                    line = line.strip()\n",
        "                    if line:  # Only add non-empty lines\n",
        "                        current_batch.append(line)\n",
        "                        processed_lines += 1\n",
        "\n",
        "                        if len(current_batch) == self.batch_size:\n",
        "                            yield current_batch\n",
        "                            current_batch = []\n",
        "\n",
        "                    if processed_lines >= self.max_sentences:\n",
        "                        break\n",
        "\n",
        "                if current_batch:  # Don't forget last partial batch\n",
        "                    yield current_batch\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error reading file: {str(e)}\")\n",
        "            if current_batch:  # Yield any remaining batch on error\n",
        "                yield current_batch\n",
        "\n",
        "class CostTracker:\n",
        "    def __init__(self):\n",
        "        self.requests = 0\n",
        "        self.total_tokens = 0\n",
        "        self.price_per_1k_tokens = 0.002\n",
        "\n",
        "    def update(self, response: ChatCompletion):\n",
        "        self.requests += 1\n",
        "        self.total_tokens += response.usage.completion_tokens + response.usage.prompt_tokens\n",
        "\n",
        "    def get_cost(self) -> float:\n",
        "        return (self.total_tokens / 1000) * self.price_per_1k_tokens\n",
        "\n",
        "    def report(self) -> str:\n",
        "        return f\"\"\"\n",
        "        API Calls: {self.requests}\n",
        "        Total Tokens: {self.total_tokens}\n",
        "        Estimated Cost: ${self.get_cost():.2f}\n",
        "        \"\"\"\n",
        "\n",
        "def save_pairs(originals: List[str], translations: List[str], dataset_name: str, mode: str = \"a\"):\n",
        "    base_path = Path(\"/content/drive/My Drive/Colab Notebooks\")\n",
        "\n",
        "    for filename, data in [\n",
        "        (f\"{dataset_name}_originals_GPT3.5.txt\", originals),\n",
        "        (f\"{dataset_name}_translations_GPT3.5.txt\", translations),\n",
        "    ]:\n",
        "        try:\n",
        "            with open(base_path / filename, mode, encoding=\"utf-8\") as f:\n",
        "                for item in data:\n",
        "                    f.write(f\"{item}\\n\")\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error saving to {filename}: {str(e)}\")\n",
        "\n",
        "    try:\n",
        "        with open(base_path / f\"{dataset_name}_aligned_pairs_GPT3.5.txt\", mode, encoding=\"utf-8\") as f:\n",
        "            for orig, trans in zip(originals, translations):\n",
        "                f.write(f\"Original: {orig}\\nTranslation: {trans}\\n---\\n\")\n",
        "    except Exception as e:\n",
        "        logging.error(f\"Error saving aligned pairs: {str(e)}\")\n",
        "\n",
        "def process_dataset(\n",
        "    input_file: str,\n",
        "    dataset_name: str,\n",
        "    api_key: str,\n",
        "    batch_size: int = 32,\n",
        "    checkpoint_interval: int = 10,\n",
        "    max_sentences: int = 404302\n",
        "):\n",
        "    translator = TranslationManager(api_key=api_key)\n",
        "\n",
        "    # Load checkpoint if exists\n",
        "    checkpoint = translator.load_checkpoint(dataset_name)\n",
        "    start_batch = checkpoint[\"last_batch\"] + 1\n",
        "    translations_count = checkpoint[\"translations_count\"]\n",
        "\n",
        "    # Calculate starting line\n",
        "    start_line = start_batch * batch_size\n",
        "\n",
        "    # Create iterator with sentence limit\n",
        "    dataset_iterator = DatasetIterator(\n",
        "        file_path=input_file,\n",
        "        batch_size=batch_size,\n",
        "        start_line=start_line,\n",
        "        max_sentences=max_sentences\n",
        "    )\n",
        "\n",
        "    cost_tracker = CostTracker()\n",
        "    total_batches = max(1, min(dataset_iterator.total_lines, max_sentences) // batch_size)\n",
        "\n",
        "    logging.info(f\"Starting from batch {start_batch}, line {start_line}\")\n",
        "    logging.info(f\"Will process up to {max_sentences} sentences\")\n",
        "    logging.info(f\"Total lines to process: {min(dataset_iterator.total_lines, max_sentences)}\")\n",
        "\n",
        "    for batch_num, batch in enumerate(dataset_iterator, start=start_batch):\n",
        "        try:\n",
        "            batch_id = f\"{dataset_name}_{batch_num}\"\n",
        "            translations, response = translator.translate_batch(batch, batch_id)\n",
        "\n",
        "            if response is not None:\n",
        "                cost_tracker.update(response)\n",
        "\n",
        "            save_pairs(batch, translations, dataset_name, mode=\"a\")\n",
        "            translations_count += len(translations)\n",
        "\n",
        "            if batch_num % checkpoint_interval == 0:\n",
        "                translator.save_checkpoint(dataset_name, batch_num, translations_count)\n",
        "                logging.info(f\"Checkpoint saved at batch {batch_num}\")\n",
        "\n",
        "            logging.info(f\"Processed batch {batch_num}/{total_batches} ({len(batch)} items)\")\n",
        "            if batch_num % 10 == 0:\n",
        "                logging.info(cost_tracker.report())\n",
        "\n",
        "            # Add a small delay to avoid rate limiting\n",
        "            time.sleep(0.5)\n",
        "\n",
        "        except Exception as e:\n",
        "            logging.error(f\"Error processing batch {batch_num}: {str(e)}\")\n",
        "            translator.save_checkpoint(dataset_name, batch_num-1, translations_count)\n",
        "            time.sleep(1)  # Longer delay on error\n",
        "            continue\n",
        "\n",
        "    # Final checkpoint and report\n",
        "    translator.save_checkpoint(dataset_name, total_batches-1, translations_count)\n",
        "    logging.info(\"\\nFinal Statistics:\")\n",
        "    logging.info(cost_tracker.report())\n",
        "    logging.info(f\"Total translations: {translations_count}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZRABuXWseyA"
      },
      "outputs": [],
      "source": [
        "if __name__ == \"__main__\":\n",
        "    input_file = \"/content/drive/My Drive/Colab Notebooks/quora_duplicate_questions.tsv\"\n",
        "    dataset_name = \"quora_test\"\n",
        "    max_sentences = 404302  # Total number of sentences to process\n",
        "\n",
        "    try:\n",
        "        from google.colab import userdata\n",
        "        api_key = userdata.get('OPENAI_API_KEY')\n",
        "    except ImportError:\n",
        "        api_key = os.getenv('OPENAI_API_KEY')\n",
        "\n",
        "    if not api_key:\n",
        "        raise ValueError(\"API key not found. Please set the OPENAI_API_KEY in Colab secrets or environment variables.\")\n",
        "\n",
        "    process_dataset(\n",
        "        input_file=input_file,\n",
        "        dataset_name=dataset_name,\n",
        "        api_key=api_key,\n",
        "        batch_size=32,\n",
        "        checkpoint_interval=10,  # Save checkpoints frequently\n",
        "        max_sentences=max_sentences,\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Create a subset file, including only sentences labeled as paraphrases (label = 1)\n",
        "\n",
        "* it is not needed since the dataset is not fully marked from the start"
      ],
      "metadata": {
        "id": "O7DUsSYZH3b0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Load the original dataset\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/quora_test_translations_GPT3.5.txt'\n",
        "df = pd.read_csv(file_path, delimiter='\\t', on_bad_lines='skip', header=None)\n",
        "df.columns = ['id', 'qid1', 'qid2', 'vprašanje1', 'vprašanje2', 'ali_podvojeno']\n",
        "\n",
        "# Converting 'ali_podvojeno' to numeric\n",
        "df['ali_podvojeno'] = pd.to_numeric(df['ali_podvojeno'], errors='coerce')\n",
        "\n",
        "# Filter rows\n",
        "subset_df = df[df['ali_podvojeno'] == 1]\n",
        "\n",
        "# Check if subset is empty\n",
        "if subset_df.empty:\n",
        "    print(\"Subset is empty. No rows found with 'ali_podvojeno' equal to 1.\")\n",
        "else:\n",
        "    # Specify the full output path within your Google Drive\n",
        "    output_file_path = '/content/drive/My Drive/Colab Notebooks/quora_subset_translations_GPT3.5.txt'\n",
        "\n",
        "    # Create the directory if it doesn't exist\n",
        "    os.makedirs(os.path.dirname(output_file_path), exist_ok=True)\n",
        "\n",
        "    subset_df.to_csv(output_file_path, index=False, sep='\\t', header=True)\n",
        "    print(f\"Subset saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eLyrpEU1IC3r",
        "outputId": "668bf9da-512f-47ac-e0d1-ba22201741e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Subset saved to /content/drive/My Drive/Colab Notebooks/quora_subset_translations_GPT3.5.txt\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-4b993485ad45>:6: DtypeWarning: Columns (3,4,5) have mixed types. Specify dtype option on import or set low_memory=False.\n",
            "  df = pd.read_csv(file_path, delimiter='\\t', on_bad_lines='skip', header=None)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "TmWaic8Nn2M6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Preprocess the dataset with separating sentences from their paraphrases in separate columns, and cleaning possible numbers at the start of the sentence."
      ],
      "metadata": {
        "id": "3tUCfiean4lQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/predprocesiranje_quora_clean.xlsx'\n",
        "#df = pd.DataFrame(pd.read_excel(file_path))\n",
        "\n",
        "print(df.columns)\n",
        "\n",
        "#df = df.rename(columns={'id\\tqid1\\tqid2\\tvprašanje1\\tvprašanje2\\tali_podvojeno': 'translated_sentence'})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CDrs6Ple7m-R",
        "outputId": "f11224b9-f163-4a8c-c09e-48f2bd82f0cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['id\\tqid1\\tqid2\\tvprašanje1\\tvprašanje2\\tali_podvojeno', 'Unnamed: 1',\n",
            "       'Unnamed: 2', 'Unnamed: 3', 'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6',\n",
            "       'Unnamed: 7'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/predprocesiranje_quora_clean.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Print the column names to see what's actually available\n",
        "print(df.columns)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qfJdZcH-BXte",
        "outputId": "7818aae7-7cdb-461c-f84d-e0bf2942d3f7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Index(['translated_sentence', 'Unnamed: 1', 'Unnamed: 2', 'Unnamed: 3',\n",
            "       'Unnamed: 4', 'Unnamed: 5', 'Unnamed: 6', 'Unnamed: 7'],\n",
            "      dtype='object')\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/predprocesiranje_quora_clean.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Check the data types of each column\n",
        "print(\"Data types:\")\n",
        "print(df.dtypes)\n",
        "\n",
        "# Look at the first few rows\n",
        "print(\"\\nFirst 5 rows:\")\n",
        "print(df.head())\n",
        "\n",
        "# Check if there are any non-string values in the 'translated_sentence' column\n",
        "non_string_values = df['translated_sentence'].apply(lambda x: not isinstance(x, str))\n",
        "if non_string_values.any():\n",
        "    print(\"\\nNon-string values found:\")\n",
        "    print(df.loc[non_string_values, 'translated_sentence'].head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4LFAo9IaQyKw",
        "outputId": "1b7a225e-b71c-474e-c6a8-cdf33e99e2f1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data types:\n",
            "translated_sentence    object\n",
            "Unnamed: 1             object\n",
            "Unnamed: 2             object\n",
            "Unnamed: 3             object\n",
            "Unnamed: 4             object\n",
            "Unnamed: 5             object\n",
            "Unnamed: 6             object\n",
            "Unnamed: 7             object\n",
            "dtype: object\n",
            "\n",
            "First 5 rows:\n",
            "                                 translated_sentence Unnamed: 1 Unnamed: 2  \\\n",
            "0  0\\t1\\t2\\tKakšen je korak za korakom vodnik za ...        NaN        NaN   \n",
            "1  1\\t3\\t4\\tKakšna je zgodba o diamantu Kohinoor ...        NaN        NaN   \n",
            "2  2\\t5\\t6\\tKako lahko povečam hitrost svoje inte...        NaN        NaN   \n",
            "3  3\\t7\\t8\\tZakaj sem mentalno zelo osamljen? Kak...        NaN        NaN   \n",
            "4  4\\t9\\t10\\tKateri se hitro raztopi v vodi: slad...        NaN        NaN   \n",
            "\n",
            "  Unnamed: 3 Unnamed: 4 Unnamed: 5 Unnamed: 6 Unnamed: 7  \n",
            "0        NaN        NaN        NaN        NaN        NaN  \n",
            "1        NaN        NaN        NaN        NaN        NaN  \n",
            "2        NaN        NaN        NaN        NaN        NaN  \n",
            "3        NaN        NaN        NaN        NaN        NaN  \n",
            "4        NaN        NaN        NaN        NaN        NaN  \n",
            "\n",
            "Non-string values found:\n",
            "5541    0\n",
            "5544    0\n",
            "5547    0\n",
            "5550    1\n",
            "5553    0\n",
            "Name: translated_sentence, dtype: object\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/predprocesiranje_quora_clean.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Define the function to clean and split text\n",
        "def clean_and_split(text):\n",
        "    # Convert to string if not already\n",
        "    if not isinstance(text, str):\n",
        "        text = str(text)\n",
        "\n",
        "    # Remove numbers and tabs at the beginning of the string\n",
        "    cleaned_text = re.sub(r'^\\d+(\\t\\d+)*\\t', '', text)\n",
        "\n",
        "    # Split the text at the first occurrence of '?'\n",
        "    parts = cleaned_text.split('?', 1)\n",
        "    if len(parts) == 2:\n",
        "        sentence = parts[0].strip() + '?'  # Add the question mark back to the sentence\n",
        "        paraphrase = parts[1].strip()\n",
        "        # Remove numbers at the end of the paraphrase\n",
        "        paraphrase = re.sub(r'\\s*\\d+\\s*$', '', paraphrase)\n",
        "        return pd.Series([sentence, paraphrase])\n",
        "    else:\n",
        "        return pd.Series([cleaned_text, None])  # Handle cases where splitting doesn't work\n",
        "\n",
        "# Apply the function to the 'translated_sentence' column\n",
        "result = df['translated_sentence'].apply(clean_and_split)\n",
        "\n",
        "# Convert the result to a DataFrame and rename the columns\n",
        "result = result.rename({0: 'sentence_translation', 1: 'paraphrase_translation'}, axis=1)\n",
        "\n",
        "# Display the DataFrame\n",
        "print(result)\n",
        "\n",
        "# Save the result to a new Excel file\n",
        "output_file_path = '/content/drive/My Drive/Colab Notebooks/processed_quora_clean.xlsx'\n",
        "result.to_excel(output_file_path, index=False)\n",
        "print(f\"Processed data saved to {output_file_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VWL5MxWdoEa7",
        "outputId": "0927c533-f347-430f-88ee-2a9bcc27b362"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                     sentence_translation  \\\n",
            "0       Kakšen je korak za korakom vodnik za vlaganje ...   \n",
            "1       Kakšna je zgodba o diamantu Kohinoor (Koh-i-No...   \n",
            "2       Kako lahko povečam hitrost svoje internetne po...   \n",
            "3                       Zakaj sem mentalno zelo osamljen?   \n",
            "4       Kateri se hitro raztopi v vodi: sladkor, sol, ...   \n",
            "...                                                   ...   \n",
            "184442                Kaj je črna stvar v sredini banane?   \n",
            "184443  Kakšni so nekateri nasveti za uspešno opravlja...   \n",
            "184444  Kakšna so vaša mnenja o demonetizaciji bankovc...   \n",
            "184445  Od kod je prišla energija iz velikega poka/sin...   \n",
            "184446  Kako dobim svoj denar nazaj od prevare prek We...   \n",
            "\n",
            "                                   paraphrase_translation  \n",
            "0       Kakšen je korak za korakom vodnik za vlaganje ...  \n",
            "1       Kaj bi se zgodilo, če bi indijska vlada ukradl...  \n",
            "2       Kako se lahko poveča hitrost interneta s hekan...  \n",
            "3       Kako lahko to rešim?\\tNajdi ostanek, ko se [ma...  \n",
            "4                  Katera riba bi preživela v slani vodi?  \n",
            "...                                                   ...  \n",
            "184442                         Kako narediti bananin sok?  \n",
            "184443  Kakšni so nekateri nasveti za uspešno opravlja...  \n",
            "184444  Zakaj je Narendra Modi prepovedal 500 in 1000 ...  \n",
            "184445   Od kod je prišla energija za vžig Velikega poka?  \n",
            "184446       Ali bom dobil/a svoj denar nazaj od prevare?  \n",
            "\n",
            "[184447 rows x 2 columns]\n",
            "Processed data saved to /content/drive/My Drive/Colab Notebooks/processed_quora_clean.xlsx\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clean the whole row where the sentence in the 'paraphrase_translation' is missing\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "# Load the Excel file into a DataFrame\n",
        "file_path = '/content/drive/My Drive/Colab Notebooks/processed_quora_clean.xlsx'\n",
        "df = pd.read_excel(file_path)\n",
        "\n",
        "# Display the first few rows to understand the structure\n",
        "print(\"Before cleaning:\")\n",
        "print(df.head())\n",
        "\n",
        "# Remove rows where the 'paraphrase_translation' column is empty\n",
        "df_cleaned = df.dropna(subset=['paraphrase_translation'])\n",
        "\n",
        "# Display the first few rows after cleaning\n",
        "print(\"\\nAfter cleaning:\")\n",
        "print(df_cleaned.head())\n",
        "\n",
        "# Save the cleaned DataFrame back to an Excel file\n",
        "cleaned_file_path = '/content/drive/My Drive/Colab Notebooks/processed_quora_clean_00.xlsx'\n",
        "df_cleaned.to_excel(cleaned_file_path, index=False)\n",
        "\n",
        "print(f\"Cleaned file saved to: {cleaned_file_path}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8HXe7J_4R2Vi",
        "outputId": "b28d9372-4689-4401-a2f6-6856438aac67"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Before cleaning:\n",
            "                                sentence_translation  \\\n",
            "0  Kakšen je korak za korakom vodnik za vlaganje ...   \n",
            "1  Kakšna je zgodba o diamantu Kohinoor (Koh-i-No...   \n",
            "2  Kako lahko povečam hitrost svoje internetne po...   \n",
            "3                  Zakaj sem mentalno zelo osamljen?   \n",
            "4  Kateri se hitro raztopi v vodi: sladkor, sol, ...   \n",
            "\n",
            "                              paraphrase_translation  \n",
            "0  Kakšen je korak za korakom vodnik za vlaganje ...  \n",
            "1  Kaj bi se zgodilo, če bi indijska vlada ukradl...  \n",
            "2  Kako se lahko poveča hitrost interneta s hekan...  \n",
            "3  Kako lahko to rešim?\\tNajdi ostanek, ko se [ma...  \n",
            "4             Katera riba bi preživela v slani vodi?  \n",
            "\n",
            "After cleaning:\n",
            "                                sentence_translation  \\\n",
            "0  Kakšen je korak za korakom vodnik za vlaganje ...   \n",
            "1  Kakšna je zgodba o diamantu Kohinoor (Koh-i-No...   \n",
            "2  Kako lahko povečam hitrost svoje internetne po...   \n",
            "3                  Zakaj sem mentalno zelo osamljen?   \n",
            "4  Kateri se hitro raztopi v vodi: sladkor, sol, ...   \n",
            "\n",
            "                              paraphrase_translation  \n",
            "0  Kakšen je korak za korakom vodnik za vlaganje ...  \n",
            "1  Kaj bi se zgodilo, če bi indijska vlada ukradl...  \n",
            "2  Kako se lahko poveča hitrost interneta s hekan...  \n",
            "3  Kako lahko to rešim?\\tNajdi ostanek, ko se [ma...  \n",
            "4             Katera riba bi preživela v slani vodi?  \n",
            "Cleaned file saved to: /content/drive/My Drive/Colab Notebooks/processed_quora_clean_00.xlsx\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "8rUN7eCZBkXU",
        "-BPcvwxZBkXY",
        "sX-obQ9KBkXZ",
        "4FhHiV9ewezJ",
        "jCSpfR0I_0zc",
        "BCcetVRwBkXc",
        "X7Li0568BkXc",
        "GOjH7N941z9z",
        "tnJIJ9oNBkXf",
        "7teb07y3BkXj",
        "xVkgw0v4d1wc",
        "0NYeXsGabw-l",
        "FioThshNM5l1",
        "iZetRXUWBkXq",
        "2uD21LepBkXu",
        "zLqJihJifL3b",
        "faVNT2NCBkXu",
        "xVsiYu9eBkXz",
        "4_qBaj9Tq3wH",
        "O7DUsSYZH3b0"
      ],
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "93892b5a169342e38910618d132a0b6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1fc0462a6a0244baa92dbdbacdb9773d",
              "IPY_MODEL_d5890d6518184bca8811c36d9042ee01",
              "IPY_MODEL_5bfbf06eab5c46f6b1429f419e09038f"
            ],
            "layout": "IPY_MODEL_ccc8022c90384c60942d0053e97a9720"
          }
        },
        "1fc0462a6a0244baa92dbdbacdb9773d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d816d48d2f5147a5b9c3a05fe525e466",
            "placeholder": "​",
            "style": "IPY_MODEL_b54007a9878647b9b9fe9c177a247c6f",
            "value": "Processing batches:  55%"
          }
        },
        "d5890d6518184bca8811c36d9042ee01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d626f686a2f4ac586910e7c5653b431",
            "max": 10000,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_994460ab9a4948ecbc1759ff9bd07aa5",
            "value": 5816
          }
        },
        "5bfbf06eab5c46f6b1429f419e09038f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d8ec78ef09494eeba03f389df4081bbf",
            "placeholder": "​",
            "style": "IPY_MODEL_8b363c624da34378b372fbacf1013442",
            "value": " 5816/10000 [9:32:24&lt;6:53:15,  5.93s/it, cost=$8.7911, pairs=58160]"
          }
        },
        "ccc8022c90384c60942d0053e97a9720": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d816d48d2f5147a5b9c3a05fe525e466": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b54007a9878647b9b9fe9c177a247c6f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d626f686a2f4ac586910e7c5653b431": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "994460ab9a4948ecbc1759ff9bd07aa5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d8ec78ef09494eeba03f389df4081bbf": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8b363c624da34378b372fbacf1013442": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}